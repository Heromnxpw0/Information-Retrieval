{"Artificial Intelligence": {"Artificial intelligence": {"title": "Artificial intelligence", "content": "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence\u2014the ability to complete any task performable by a human on an at least equal level\u2014is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the \"AI boom\"). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\n\n== Goals ==\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\n\n=== Reasoning and problem-solving ===\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\n\n=== Knowledge representation ===\n\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\n\n\n=== Planning and decision-making ===\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\n\n\n=== Learning ===\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\n\n=== Natural language processing ===\nNatural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\n\n\n=== Perception ===\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition, image classification, facial recognition, object recognition,object tracking, and robotic perception.\n\n\n=== Social intelligence ===\n\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction.\nHowever, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.\n\n\n=== General intelligence ===\nA machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\n\n== Techniques ==\nAI research uses a wide variety of techniques to accomplish the goals above.\n\n\n=== Search and optimization ===\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\n\n\n==== State space search ====\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.\n\n\n==== Local search ====\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\n\n=== Logic ===\nFormal logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.\n\n\n=== Probabilistic methods for uncertain reasoning ===\n\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation\u2013maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\n\n=== Classifiers and statistical learning methods ===\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers.\n\n\n=== Artificial neural networks ===\n\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\nIn feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks. Perceptrons use only a single layer of neurons; deep learning uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are \"close\" to each other\u2014this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object.\n\n\n=== Deep learning ===\n\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2023. The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\n\n\n=== GPT ===\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.\nCurrent models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\n\n\n=== Hardware and software ===\n\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\nThe transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster.\n\n\n== Applications ==\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).\n\n\n=== Health and medicine ===\n\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\n\n\n=== Games ===\n\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\n\n\n=== Mathematics ===\nIn mathematics, special forms of formal step-by-step reasoning are used. In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.\nAlternatively, dedicated models for mathematic problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind, Llemma from eleuther or Julius.\nWhen natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematic tasks.\nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\n\n\n=== Finance ===\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\nWorld Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"\n\n\n=== Military ===\n\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.\nIn November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.\n\n\n=== Generative AI ===\n\nIn the early 2020s, generative AI gained widespread prominence. GenAI is AI capable of generating text, images, videos, or other data using generative models, often in response to prompts.\nIn March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.\n\n\n=== Agents ===\nArtificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.\n\n\n=== Other industry-specific tasks ===\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n\n\n== Ethics ==\n\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\n\n\n=== Risks and harm ===\n\n\n==== Privacy and copyright ====\n\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\nAI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\n\n\n==== Dominance by tech giants ====\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\n\n==== Substantial power needs and other environmental impacts ====\n\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\nProdigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.\n\n\n==== Misinformation ====\n\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem .\nIn 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.\n\n\n==== Algorithmic bias and fairness ====\n\nMachine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\n\n==== Lack of transparency ====\n\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\n\n\n==== Bad actors and weaponized AI ====\n\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China.\nThere many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\n\n\n==== Technological unemployment ====\n\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\n\n\n==== Existential risk ====\n\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-like \"sentience\" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google.\" He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\nIn 2023, many leading AI experts issued the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\nOther researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\n\n\n=== Ethical machines and alignment ===\n\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.\nOther approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines.\n\n\n=== Open source ===\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\n\n\n=== Frameworks ===\nArtificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values\u2014developed by the Alan Turing Institute tests projects in four main areas:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\n\n\n=== Regulation ===\n\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\n\n\n== History ==\n\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCullouch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022.\n\n\n== Philosophy ==\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy can relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.\n\n\n=== Defining artificial intelligence ===\n\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine\u2014and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\n\n\n=== Evaluating approaches to AI ===\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\n\n==== Symbolic AI and its limits ====\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\n\n==== Neat vs. scruffy ====\n\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\n\n\n==== Soft vs. hard computing ====\n\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\n\n==== Narrow vs. general AI ====\n\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n\n\n=== Machine consciousness, sentience, and mind ===\n\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\n\n==== Consciousness ====\n\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\n\n==== Computationalism and functionalism ====\n\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\n\n\n==== AI welfare and rights ====\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\n\n\n== Future ==\n\n\n=== Superintelligence and the singularity ===\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\n\n\n=== Transhumanism ===\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\n\n\n== In fiction ==\n\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\n\n== See also ==\nArtificial intelligence content detection \u2013 Software to detect AI-generated content\nBehavior selection algorithm \u2013 Algorithm that selects actions for intelligent agents\nBusiness process automation \u2013 Automation of business processes\nCase-based reasoning \u2013 Process of solving new problems based on the solutions of similar past problems\nComputational intelligence \u2013 Ability of a computer to learn a specific task from data or experimental observation\nDigital immortality \u2013 Hypothetical concept of storing a personality in digital form\nEmergent algorithm \u2013 Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies \u2013 Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence \u2013 List of definitions of terms and concepts commonly used in the study of artificial intelligence\nIntelligence amplification \u2013 Use of information technology to augment human intelligence\nMind uploading \u2013 Hypothetical process of digitally emulating a brain\nMoravec's paradox \u2013 Observation that perception requires more computation than reasoning\nOrganoid intelligence \u2013 Use of brain cells and brain organoids for intelligent computing\nRobotic process automation \u2013 Form of business process automation technology\nWeak artificial intelligence \u2013 Form of artificial intelligence\nWetware computer \u2013 Computer composed of organic material\nHallucination (artificial intelligence) \u2013 Erroneous material generated by AI\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n=== AI textbooks ===\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\n\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\nThese were the four of the most widely used AI textbooks in 2008:\n\nOther textbooks:\n\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.\n\n\n=== History of AI ===\n\n\n=== Other sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).", "link": "https://en.wikipedia.org/wiki/Artificial_intelligence"}, "Artificial general intelligence": {"title": "Artificial general intelligence", "content": "Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. AGI is considered one of the definitions of strong AI.\nCreating AGI is a primary goal of AI research and of companies such as OpenAI and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\nThe timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; and a minority believe it may never be achieved. Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid progress towards AGI, suggesting it could be achieved sooner than many expect.\nThere is debate on the exact definition of AGI, and regarding whether modern large language models (LLMs) such as GPT-4 are early forms of AGI. AGI is a common topic in science fiction and futures studies.\nContention exists over whether AGI represents an existential risk. Many experts on AI have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be too remote to present such a risk.\n\n\n== Terminology ==\nAGI is also known as strong AI, full AI, human-level AI, or general intelligent action. However, some academic sources reserve the term \"strong AI\" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.\nA framework for classifying AGI in levels was proposed in 2023 by Google DeepMind researchers. They define five levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI.\n\n\n== Characteristics ==\n\nVarious popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches. \n\n\n=== Intelligence traits ===\nHowever, researchers generally hold that intelligence is required to do all of the following:\n\nreason, use strategy, solve puzzles, and make judgments under uncertainty\nrepresent knowledge, including common sense knowledge\nplan\nlearn\ncommunicate in natural language\nif necessary, integrate these skills in completion of any given goal\nMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.\nComputer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.\n\n\n=== Physical traits ===\nOther capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:\n\nthe ability to sense (e.g. see, hear, etc.), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.)\nThis includes the ability to detect and respond to hazard.\n\n\n=== Tests for human-level AGI ===\nSeveral tests meant to confirm human-level AGI have been considered, including:\n\nThe Turing Test (Turing)\nProposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence,\" this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine.\nTuring described the test as follows:\nThe idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.\nIn 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI.\nThe Robot College Student Test (Goertzel)\nA machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.\nThe Employment Test (Nilsson)\nA machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.\nThe Ikea test (Marcus)\nAlso known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak)\nA machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. This has not yet been completed.\nThe Modern Turing Test (Suleyman)\nAn AI model is given $100,000 and has to obtain $1 million.\n\n\n=== AI-complete problems ===\n\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.\nThere are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\nHowever, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.\n\n\n== History ==\n\n\n=== Classical AI ===\n\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"\nTheir predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".\nSeveral classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".\n\n\n=== Narrow AI research ===\n\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.\n\nAt the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\nHowever, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) \u2013 nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\n\n\n=== Modern artificial general intelligence research ===\nThe term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\". This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, was also called universal artificial intelligence.\nThe term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.\nAs of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do.\n\n\n=== Feasibility ===\nAs of 2023, the development and potential achievement of Artificial General Intelligence (AGI) remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.\nA further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?\nMost AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.\nA report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.\nIn 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4\u2019s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.\nBlaise Ag\u00fcera y Arcas and Peter Norvig wrote in 2023 that a significant level of general intelligence has already been achieved with frontier models. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".\n2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images).\nIn 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.\n\n\n=== Timescales ===\n\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.\nIn the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16\u201326 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.\nIn 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.\nIn 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.\nIn the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.\nIn 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.\nIn 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.\nIn 2023, the AI researcher Geoffrey Hinton stated that:\n\nThe idea that this stuff could actually get smarter than people \u2013 a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.In May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years. In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans. In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".\n\n\n== Whole brain emulation ==\n\nWhile the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain. Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\n\n\n=== Early estimates ===\n For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5\u00d71014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).\nIn 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" \u2013 a measure used to rate current supercomputers \u2013 then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\n\n=== Current research ===\nThe Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain. In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain. A supercomputer with similar computing capability as the human brain is expected in April 2024. Called \"DeepSouth\", it could perform 228 trillions of synaptic operations per second.\n\n\n=== Criticisms of simulation-based approaches ===\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.\nA fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.\n\n\n== Philosophical perspective ==\n\n\n=== \"Strong AI\" as defined in philosophy ===\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He proposed a distinction between two hypotheses about artificial intelligence:\n\nStrong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\".\nWeak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.\nThe first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.\nIn contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\". This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.\nMainstream AI is most interested in how a program behaves. According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\" If the program can behave as if it has a mind, then there is no need to know if it actually has mind \u2013 indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.\n\n\n=== Consciousness ===\n\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts.\nSelf-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"\u2014an operating system or debugger is able to be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)\u2014but this is not what people typically mean when they use the term \"self-awareness\".\nThese traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals. Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights. Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.\n\n\n== Benefits ==\nAGI could have a wide variety of applications. If oriented towards such goals, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.\nAGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.\nAGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.\n\n\n== Risks ==\n\n\n=== Existential risks ===\n\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".\n\n\n==== Risk of loss of control and human extinction ====\nThe thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.\nIn 2014, Stephen Hawking criticized widespread indifference:\n\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here\u2014we'll leave the lights on?' Probably not\u2014but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.\nThe skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.\nMany scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors), and the use of AI in weapon systems.\nThe thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI. Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.\nSkeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.\nIn 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"\n\n\n=== Mass unemployment ===\n\nResearchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.\nAccording to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:\n\nEveryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk considers that the automation of society will require governments to adopt a universal basic income.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Sources ==\n\n\n== Further reading ==\n\n\n== External links ==\nThe AGI portal maintained by Pei Wang", "link": "https://en.wikipedia.org/wiki/Artificial_general_intelligence"}, "A.I. Artificial Intelligence": {"title": "A.I. Artificial Intelligence", "content": "A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt star in supporting roles.\nDevelopment of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss's story in the early 1970s. Kubrick hired a series of writers, including Brian Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson's treatment for the screenplay and dedicated the film to Kubrick.\nA.I. Artificial Intelligence was released on June 29, 2001, by Warner Bros. Pictures in North America. It received generally positive reviews from critics and grossed $235.9 million against a budget of $90\u2013100 million. It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards. In a 2016 BBC poll of 177 critics around the world, A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg's best works and one of the greatest films of the 21st century, and of all time.\n\n\n== Plot ==\nIn the 22nd century, rising sea levels from global warming have wiped out 99% of existing cities, reducing the world's population. Mecha humanoid robots have been created as replacements.\nIn Madison, New Jersey, David, a prototype Mecha child capable of experiencing love, is given to Henry Swinton and his wife Monica, whose son Martin is in suspended animation after contracting a rare disease. Initially uncomfortable with David, Monica eventually warms to him and activates his imprinting protocol. Wanting her to love him in return, he befriends Teddy, Martin's old robotic teddy bear.\nAfter Martin is unexpectedly cured of his disease and brought home, he jealously goads David into cutting off a piece of Monica's hair. David enters his adoptive parents' room that night, but Monica turns over and is poked in the eye by the scissors. While Henry attends to her wounds, Teddy picks up the lock of hair from the floor and places it in his pocket. During a pool party, one of Martin's friends pokes David with a knife, triggering his self-protection programming. He grabs  Martin, causing them to fall into the pool. While Martin is rescued, David is accused of endangering living people.\nHenry convinces Monica to return David to his creators for destruction. En route, she instead spares David by abandoning him in the woods full of scrap metal and obsolete Mecha. Now accompanied solely by Teddy, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy to become human, which he believes will regain Monica's love.\nDavid and Teddy are captured by the \"Flesh Fair\", a traveling circus-like event at which obsolete Mecha are destroyed in front of jeering crowds. About to be destroyed himself, David pleads for his life, and the audience revolts and allows David to escape with Gigolo Joe, a prostitute Mecha on the run after being framed for murder. David, Teddy and Joe go to the decadent resort town of Rouge City, where \"Dr. Know\", a holographic answer engine, directs them to the top of Rockefeller Center in the flooded ruins of New York City and provides fairy tale information that David interprets as suggesting that a Blue Fairy can help him.\nAbove the ruins of New York, David meets Professor Hobby, his creator, who tells him that their meeting demonstrates David's ability to love and desire. David finds copies of himself, including female variants called \"Darlene\", ready to be shipped. Disheartened by his lost sense of individuality, David attempts suicide by falling from a skyscraper into the ocean. While underwater, David notices a figure resembling the Blue Fairy before Joe rescues him in an amphibious aircraft. Before David can explain, authorities capture Joe with an electromagnet. David and Teddy take control of the aircraft to see the Blue Fairy, which turns out to be a statue from an attraction on Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. Believing that the Blue Fairy is real, David repeatedly asks the statue to turn him into a real boy until his power source is depleted.\nTwo thousand years later, humanity is extinct and Manhattan is buried under glacial ice. Mecha have evolved into an advanced form, and a group known as the Specialists, interested in humanity, find and resurrect David and Teddy. They reconstruct the Swinton family home from David's memories before explaining, via an interactive version of the Blue Fairy, that he cannot become human. However, they recreate Monica through genetic material from the strand of hair that Teddy kept. This version of Monica can live for only one day and cannot be revived. David spends his happiest day with Monica, and as she falls asleep in the evening, Monica tells David that she has always loved him. David lies down next to her and closes his eyes.\n\n\n== Cast ==\n\n\n== Production ==\n\n\n=== Development ===\nStanley Kubrick began development on an adaptation of \"Super-Toys Last All Summer Long\" in the late 1970s, hiring the story's author, Brian Aldiss, to write a film treatment. In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing. Warner Bros. agreed to co-finance A.I. and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw briefly served as writer, leaving after six weeks due to Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, \"Not only did the bastard fire me, he hired my enemy [Watson] instead.\" Kubrick handed Watson The Adventures of Pinocchio for inspiration, calling A.I. \"a picaresque robot version of Pinocchio\".\nThree weeks later, Watson gave Kubrick his first story treatment, and concluded his work on A.I. in May 1991 with another treatment of 90 pages. Gigolo Joe was originally conceived as a G.I. Mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, \"I guess we lost the kiddie market.\" Meanwhile, Kubrick dropped A.I. to work on a film adaptation of Wartime Lies, feeling computer animation was not advanced enough to create the David character. After the release of Spielberg's Jurassic Park, with its innovative computer-generated imagery, it was announced in November 1993 that production of A.I. would begin in 1994. Dennis Muren and Ned Gorman, who worked on Jurassic Park, became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic.\n\n\n=== Pre-production ===\nIn early 1994, the film was in pre-production with Christopher \"Fangorn\" Baker as concept artist and Sara Maitland assisting on the story, which gave it \"a feminist fairy-tale focus\". Maitland said that Kubrick never referred to the film as A.I., but as Pinocchio. Chris Cunningham became the new visual effects supervisor. Some of his unproduced work for A.I. can be seen on the DVD The Work of Director Chris Cunningham.\nAside from considering computer animation, Kubrick also had Joseph Mazzello do a screen test for the lead role. Cunningham helped assemble a series of \"little robot-type humans\" for the David character. \"We tried to construct a little boy with a movable rubber face to see whether we could make it look appealing,\" producer Jan Harlan reflected. \"But it was a total failure, it looked awful.\" Hans Moravec was brought in as a technical consultant. Meanwhile, Kubrick and Harlan thought that A.I. would be closer to Steven Spielberg's sensibilities as director. Kubrick handed the position to Spielberg in 1995, but Spielberg chose to direct other projects and convinced Kubrick to remain as director. The film was put on hold due to Kubrick's commitment to Eyes Wide Shut (1999).\nAfter Kubrick's death in March 1999, Harlan and Christiane Kubrick approached Spielberg to take over the director's position. By November 1999, Spielberg was writing the screenplay based on Watson's 90-page story treatment. It was his first solo screenplay credit since Close Encounters of the Third Kind (1977). Pre-production was briefly halted during February 2000 because Spielberg pondered directing other projects, which were Harry Potter and the Philosopher's Stone, Minority Report and Memoirs of a Geisha. The following month, Spielberg announced that A.I. would be his next project, with Minority Report as a follow-up. When he decided to fast track A.I., Spielberg brought back Chris Baker as concept artist. Ian Watson reported that the final script was very faithful to Kubrick's vision; even the ending, which is often attributed to Spielberg, saying, \"The final 20 minutes are pretty close to what I wrote for Stanley, and what Stanley wanted, faithfully filmed by Spielberg without added schmaltz\".\n\n\n=== Filming ===\nThe original start date was July 10, 2000, but filming was delayed until August. Aside from a couple of weeks of shooting on location in Oxbow Regional Park in Oregon, A.I. was shot entirely using sound stages at Warner Bros. Studios and the Spruce Goose Dome in Long Beach, California.\nSpielberg copied Kubrick's obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew, banning press from the set, and making actors sign confidentiality agreements. For instance, Jack Angel, who voiced Teddy, recorded his lines entirely out of context, only receiving direction to sound like Eeyore from Winnie the Pooh, except \"very wise and old and stoic\". However, Spielberg asked Angel to be on the set every day to make line alterations wherever he felt necessary. Social robotics expert Cynthia Breazeal served as technical consultant during production. Costume designer Bob Ringwood studied pedestrians on the Las Vegas Strip for his influence on the Rouge City extras.\n\n\n=== Visual effects ===\nVisual effects, such as removing the visible rods controlling Teddy and removing Haley Joel Osment's breath, were provided in-houses by Industrial Light & Magic (ILM) and PDI/DreamWorks.\n\n\n=== Casting ===\nJulianne Moore and Gwyneth Paltrow were considered for the role of Monica Swinton before Frances O'Connor was cast. Jerry Seinfeld was originally considered to voice and play the Comedian Robot before Chris Rock was cast.\n\n\n== Soundtrack ==\n\nThe film's soundtrack album was released by Warner Sunset Records in 2001. The original score was composed and conducted by John Williams and features singers Lara Fabian on two songs and Josh Groban on one. The film's score also had a limited release as an official \"For your consideration Academy Promo\", as well as a complete score issued by La-La Land Records in 2015. The band Ministry appears in the film playing the song \"What About Us?\", but the song does not appear on the official soundtrack album.\n\n\n== Release ==\n\n\n=== Marketing ===\nThe teaser trailer debuted on December 8, 2000 with the theatrical release of Proof of Life. Warner Bros. used an alternate reality game titled The Beast to promote the film. Over forty websites were created by Atomic Pictures in New York City (kept online at Cloudmakers.org), including the website for Cybertronics Corp. There were to be a series of video games for the Xbox video game console that followed the storyline of The Beast, but they went undeveloped. To avoid audiences mistaking A.I. for a family film, no action figures were created, although Hasbro released a talking Teddy following the film's release in June 2001.\nA.I. premiered at the Venice Film Festival in 2001.\n\n\n=== Home media ===\nA.I. Artificial Intelligence was released on VHS and DVD in the United States by DreamWorks Home Entertainment on March 5, 2002 in widescreen and fullscreen two-disc special editions featuring an extensive sixteen-part documentary detailing the film's development, production, music and visual effects. The bonuses also include interviews with Haley Joel Osment, Jude Law, Frances O'Connor, Steven Spielberg and John Williams, two teaser trailers for the film's original theatrical release, and an extensive photo gallery featuring production stills and Stanley Kubrick's original storyboards. It was released overseas by Warner Home Video.\nThe film was released on Blu-ray in Japan by Warner Home Video on December 22, 2010, followed shortly by a United States release by Paramount Home Entertainment (owners of the pre-2010 DreamWorks catalog) on April 5, 2011. This Blu-ray features the film remastered in high-definition and incorporates all the bonus features previously included on the two-disc special-edition DVD.\n\n\n== Reception ==\n\n\n=== Box office ===\nThe film opened in 3,242 theaters in the United States and Canada on June 29, 2001, earning $29.35 million at #1 during its opening weekend. A.I went on to gross $78.62 million in the United States and Canada. Opening on 524 screens in Japan, A.I. grossed almost two billion Yen in its first five days, the biggest June opening in Japan at the time, and sold more tickets in its opening weekend than Star Wars: Episode I \u2013 The Phantom Menace, although it grossed slightly less. It went on to gross $78 million in Japan. It grossed $79 million in other countries, for a worldwide total of $235.93 million.\n\n\n=== Critical response ===\nOn Rotten Tomatoes, A.I. Artificial Intelligence holds an approval rating of 76% based on reviews from 203 critics, with an average rating of 6.60/10. The website's critical consensus reads: \"A curious, not always seamless, amalgamation of Kubrick's chilly bleakness and Spielberg's warm-hearted optimism. A.I. is, in a word, fascinating.\" On Metacritic, it has a weighted average score of 65 out of 100 based on reviews from 32 critics, which indicates \"generally favorable reviews\". Audiences surveyed by CinemaScore gave the film an average grade of \"C+\" on a scale of A+ to F.\nProducer Jan Harlan stated that Kubrick \"would have applauded\" the final film, while Kubrick's widow Christiane also enjoyed A.I. Brian Aldiss admired the film as well: \"I thought what an inventive, intriguing, ingenious, involving film this was. There are flaws in it and I suppose I might have a personal quibble but it's so long since I wrote it.\" Of the film's ending, he wondered how it might have been had Kubrick directed the film: \"That is one of the 'ifs' of film history\u2014at least the ending indicates Spielberg adding some sugar to Kubrick's wine. The actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence. But it's a brilliant piece of film and of course it's a phenomenon because it contains the energies and talents of two brilliant filmmakers.\"\nRichard Corliss of Time magazine heavily praised Spielberg's direction, as well as the cast and visual effects.\nRoger Ebert of the Chicago Sun-Times gave the film three stars out of a possible four, saying that it is \"wonderful and maddening\". Ebert later gave the film a full four stars and added it to his \"Great Movies\" list in 2011.\nLeonard Maltin, on the other hand, gives the film two stars out of four in his Movie Guide, writing, \"[The] intriguing story draws us in, thanks in part to Osment's exceptional performance, but takes several wrong turns; ultimately, it just doesn't work. Spielberg rewrote the adaptation Stanley Kubrick commissioned of the Brian Aldiss short story Super Toys Last All Summer Long; [the] result is a curious and uncomfortable hybrid of Kubrick and Spielberg sensibilities.\" However, Maltin called John Williams's music score \"striking\".\nJonathan Rosenbaum of the Chicago Reader compared A.I. to Solaris (1972), and praised both \"Kubrick for proposing that Spielberg direct the project and Spielberg for doing his utmost to respect Kubrick's intentions while making it a profoundly personal work\". In 2009, he described A.I. as \"a very great and deeply misunderstood film\", noting that Andrew Sarris, Stan Brakhage and James Naremore \"more or less\" agreed with this assessment.\nFilm critic Armond White of the New York Press praised the film, noting that \"each part of David's journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema's most thoughtful, speculative artists \u2013 Borzage, Ozu, Demy, Tarkovsky.\"\nFilmmaker Billy Wilder hailed A.I. as \"the most underrated film of the past few years\". When British filmmaker Ken Russell saw the film, he wept during the ending.\nScreenwriter Ian Watson has speculated, \"Worldwide, A.I. was very successful (and the 4th-highest earner of the year) but it didn't do quite so well in America, because the film, so I'm told, was too poetical and intellectual in general for American tastes. Plus, quite a few critics in America misunderstood the film, thinking for instance that the Giacometti-style beings in the final 20 minutes were aliens (whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film) and also thinking that the final 20 minutes were a sentimental addition by Spielberg, whereas those scenes were exactly what I wrote for Stanley and exactly what he wanted, filmed faithfully by Spielberg.\"\nMick LaSalle of the San Francisco Chronicle gave a largely negative review. \"A.I. exhibits all its creators' bad traits and none of the good. So we end up with the structureless, meandering, slow-motion endlessness of Kubrick combined with the fuzzy, cuddly mindlessness of Spielberg.\" Dubbing it Spielberg's \"first boring movie\", LaSalle also believed that the robots at the end of the film were aliens, and compared Gigolo Joe to the \"useless\" Jar Jar Binks, yet praised Robin Williams for his portrayal of a futuristic Albert Einstein.\nPeter Travers of Rolling Stone magazine gave a mixed review, concluding, \"Spielberg cannot live up to Kubrick's darker side of the future\", but still put the film on his top ten list that year.\nDavid Denby in The New Yorker criticized A.I. for not adhering closely to his concept of the Pinocchio character.\nSpielberg responded to some of the criticisms of the film, stating that many of the \"so called sentimental\" elements of A.I., including the ending, were in fact Kubrick's, and the darker elements were his own. However, Sara Maitland, who worked on the project with Kubrick in the 1990s, said that Kubrick never started production on A.I. because he had a hard time making the ending work.\nJames Berardinelli found the film \"consistently involving, with moments of near-brilliance, but far from a masterpiece. In fact, as the long-awaited 'collaboration' of Kubrick and Spielberg, it ranks as something of a disappointment.\" Of the film's highly debated finale, he claimed, \"There is no doubt that the concluding 30 minutes are all Spielberg; the outstanding question is where Kubrick's vision left off and Spielberg's began.\"\nJohn Simon of the National Review described A.I. \"as an uneasy mix of trauma and treacle\".\nIn 2002, Spielberg told film critic Joe Leydon, \"People pretend to think they know Stanley Kubrick, and think they know me, when most of them don't know either of us... And what's really funny about that is, all the parts of A.I. that people assume were Stanley's were mine. And all the parts of A.I. that people accuse me of sweetening and softening and sentimentalizing were all Stanley's. The teddy bear was Stanley's. The whole last 20 minutes of the movie was completely Stanley's. The whole first 35, 40 minutes of the film\u2014all the stuff in the house\u2014was word for word, from Stanley's screenplay. This was Stanley's vision... Eighty percent of the critics got it all mixed up. But I could see why. Because, obviously, I've done a lot of movies where people have cried and have been sentimental. And I've been accused of sentimentalizing hard-core material. But in fact it was Stanley who did the sweetest parts of A.I., not me. I'm the guy who did the dark center of the movie, with the Flesh Fair and everything else. That's why he wanted me to make the movie in the first place. He said, 'This is much closer to your sensibilities than my own.'\" Spielberg said, \"While there was divisiveness when A.I. came out, I felt that I had achieved Stanley's wishes, or goals.\"\nOn re-watching the film many years after its release, BBC film critic Mark Kermode apologized to Spielberg in a January 2013 interview for \"getting it wrong\" on the film when he first viewed it in 2001. He came to believe that the film is Spielberg's \"enduring masterpiece\".\n\n\n=== Accolades ===\nVisual effects supervisors Dennis Muren, Stan Winston, Michael Lantieri and Scott Farrar were nominated for the Academy Award for Best Visual Effects, and John Williams was nominated for Best Original Music Score. Steven Spielberg, Jude Law and Williams received nominations at the 59th Golden Globe Awards. A.I. was successful at the Saturn Awards, winning five awards, including Best Science Fiction Film along with Best Writing for Spielberg and Best Performance by a Younger Actor for Osment.\n\nAmerican Film Institute nominated the film in AFI's 100 Years of Film Scores.\n\n\n== See also ==\nList of underwater science fiction works\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nHarlan, Jan; Struthers, Jane M. (2009). A.I. Artificial Intelligence: From Stanley Kubrick to Steven Spielberg: The Vision Behind the Film. London: Thames & Hudson. ISBN 978-0-500514894.\nRice, Julian (2017). Kubrick's Story: Spielberg's Film: A.I. Artificial Intelligence. Rowman & Littlefield. ISBN 978-1-442278189.\n\n\n== External links ==\n\nOfficial website at the Wayback Machine (archived 2008-05-26)\nOfficial Warner Bros. Site\nA.I. Artificial Intelligence at IMDb\nA.I. Artificial Intelligence at AllMovie\nA.I. Artificial Intelligence at Rotten Tomatoes\nA.I. Artificial Intelligence at Box Office Mojo", "link": "https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence"}, "Applications of artificial intelligence": {"title": "Applications of artificial intelligence", "content": "Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology that has numerous applications, including language translation, image recognition, decision-making, credit scoring and e-commerce. AI includes the development of machines which can perceive, understand, act and learn a scientific discipline.\n\n\n== Internet and e-commerce ==\n\n\n=== Recommendation systems ===\n\nA recommendation system predicts the rating or preference a user would give to an item. Artificial intelligence recommendation systems are designed to offer suggestions based on previous behavior. These systems have been used by companies such as Netflix, Amazon, Instagram and YouTube, where they generate personalized playlists, product suggestions, and video recommendations.\n\n\n=== Web feeds and posts ===\nMachine learning is also used in web feeds such as for determining which posts should show up in social media feeds. Various types of social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.\n\n\n=== Targeted advertising and increasing internet engagement ===\n\nAI is used to target web advertisements to those most likely to click or engage in them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints. Both AdSense and Facebook use AI for advertising. Online gambling companies use AI to improve customer targeting.\nPersonality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting. AI has been used to customize shopping options and personalize offers.\n\n\n=== Virtual assistants ===\n\nIntelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.\n\n\n=== Search engines ===\nBing Chat has used artificial intelligence as part of its search engine.\n\n\n=== Spam filtering ===\n\nMachine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements. Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails. These models can be refined from new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types.\n\n\n=== Language translation ===\n\nSpeech translation technology attempts to convert one language's spoken words into another. This potentially reduces language barriers in global commerce and cross-cultural exchange by allowing speakers of various languages to communicate with one another. \nAI has been used to automatically translate spoken language and textual content, in products such as Microsoft Translator, Google Translate and DeepL Translator. Additionally, research and development are in progress to decode and conduct animal communication.\nMeaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.\n\n\n=== Facial recognition and image labeling ===\n\nAI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.\nImage labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.  Facebook's DeepFace identifies human faces in digital images.\n\n\n== Games ==\n\nGames have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero). AI has replaced hand-coded algorithms in most chess programs. Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.\n\n\n== Economic and social challenges ==\n\nAI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.\n\n\n== Agriculture ==\n\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n\n\n=== Precision Farming ===\nAI helps in achieving precise farming, which calls for the use of algorithims to analyze data retrieved from satellite imagery and on-site field sensors. It allows for optimization of resource usage and helps to make the right decisions regarding the kind of nutrients, water, and pesticides required to maximize yield. \n\n\n=== Crop and soil monitoring ===\nUsing machine learning models to monitor the health of crops and the soil. The models will be able to detect and predict diseases and pests in crops ahead of time to allow timely interventions. \n\n\n=== Automated Machinery ===\nThere are automated machinery such as tractors and harvesters, which can operate autonomously with minimal human labor. With the use of AI many duties in the area are possible to be done with precision. \n\n\n== Cyber security ==\nCyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.\nApplications of AI in cyber security include:\n\nNetwork protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.\nEndpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors.\nAI-related cyber security application cases vary in both benefit and complexity. Security features such as Security Orchestration, Automation, and Response (SOAR) and Extended Endpoint Detection and Response (XDR) offer significant benefits for businesses, but require significant integration and adaptation efforts.\nApplication security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service.\nAI technology can also be utilized to improve system security and safeguard our privacy. Randrianasolo (2012) suggested a security system based on artificial intelligence that can recognize intrusions and adapt to perform better. In order to improve cloud computing security, Sahil (2015) created a user profile system for the cloud environment with AI techniques.\nSuspect user behavior: Machine learning can identify fraud or compromised applications as they occur.\nGoogle fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.\n\n\n== Education ==\n\nAI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.\u201d \nThe World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.\nPersonalized Learning\nAI driven tutoring systems, such as Khan Academy, Duolingo and Carnegie Learning are the forefoot of delivering personalized education.\nThese platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content to suit each student's pace and style of learning.\nAdministrative Efficiency\nIn educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.\nFurthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind. \nEthical and Privacy Concerns\nDespite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data. \nIt is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.\nMuch regulation will be influenced by the AI Act, the world\u2019s first comprehensive AI law. \n\n\n== Finance ==\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards. Kasisto and Moneystream use AI.\nBanks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.\nThe use of AI in applications such as online trading and decision-making has changed major economic theories. For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient. The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises. Especially for smaller and more innovative enterprises.\n\n\n=== Trading and investment ===\nAlgorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.\nLarge financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.\n\n\n=== Underwriting ===\nOnline lender Upstart uses machine learning for underwriting.\nZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.\n\n\n=== Audit ===\nAI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.\nContinuous auditing with AI allows a real-time monitoring and reporting of financial activities and providing businesses with timely insights that can lead to quick decision making. \n\n\n=== Anti-money laundering ===\nAI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML). AI can be used to \"develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability\". A study about deep learning for AML identified \"key challenges for researchers\" to have \"access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced\" and suggests future research should bring-out \"explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data\".\nBanks use machine learning (ML) to upgrade process monitoring and demonstrating the ability of  responding efficiently to evolving techniques.\nThrough ML and other methods, financial organizations can detect laundering operations and run compliance in an automated and very fast mode.\n\n\n=== History ===\nIn the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year. One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"\nOne of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.\nIn the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion. These expert systems were later replaced by machine learning systems.\nAI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.\n\n\n== Government ==\n\nAI facial recognition systems are used for mass surveillance, notably in China. In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.\n\n\n=== Military ===\n\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.\nIn 2023, the United States Department of Defense tested generative AI based on large language models to digitize and integrate data across the military.\nIn the 2023 Israel\u2013Hamas war, Israel used two AI systems to generate targets to strike: Habsora (translated: \"the gospel\") was used to compile a list of buildings to target, while \"Lavender\" produced a list of people. \"Lavender\" produced a list of 37,000 people to target. The list of buildings to target included Gazan private homes of people that were suspected of affiliation to Hamas operatives. The combination of AI targeting technology with policy shift away from avoiding civilian targets resulted in unprecedented numbers of civilian deaths. IDF officials say the program addresses the previous issue of the air force running out of targets. Using Habsora, officials say that suspected and junior Hamas members homes significantly expand the \"AI target bank.\" An internal source describes the process as a \u201cmass assassination factory\u201d.\nIn 2024, the U.S. military trained artificial intelligence to identify airstrike targets during its operations in Iraq and Syria.\nIn 2024 a Chinese laboratory at the Joint Operations College of the National Defense University in Shijiazhuang has created an AI military commander, for use in large-scale war simulations in the role of the commander-in-chief.\nWorldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are in wide use. The Ukrainian Army has developed 2024 autonomous Kamikazedrones in oder to make Russian interference during flight ineffective.  Many researchers avoid military applications.\n\n\n== Health ==\n\n\n=== Healthcare ===\n\nAI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients. Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.\nThe early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem. Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines. Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers. Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions. In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.\nAnother study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.\nArtificial neural networks are used as clinical decision support systems for medical diagnosis, such as in concept processing technology in EMR software.\nOther healthcare tasks thought suitable for an AI that are in development include:\n\nScreening\nHeart sound analysis\nCompanion robots for elder care\nMedical record analysis\nTreatment plan design\nMedication management\nAssisting blind people\nConsultations\nDrug creation (e.g. by identifying candidate drugs and by using existing drug screening data such as in life extension research)\nClinical training\nOutcome prediction for surgical procedures\nHIV prognosis\nIdentifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics-based fingerprints (including pandemic pathogens)\nHelping link genes to their functions, otherwise analyzing genes and identification of novel biological targets\nHelp development of biomarkers\nHelp tailor therapies to individuals in personalized medicine/precision medicine\n\n\n=== Workplace health and safety ===\n\nAI-enabled chatbots decrease the need for humans to perform basic call center tasks.\nMachine learning in sentiment analysis can spot fatigue in order to prevent overwork. Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient. For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury. Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.\nAI can auto-code workers' compensation claims. AI-enabled virtual reality systems can enhance safety training for hazard recognition. AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.\n\n\n=== Biochemistry ===\nAlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).\n\n\n== Chemistry and biology ==\n\nMachine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces. Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\", have been used to explore the origins of life on Earth, drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design). There is research about which types of computer-aided chemistry would benefit from machine learning. It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\". It has been used for the design of proteins with prespecified functional sites.\nIt has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.\nThere are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs. It is widely used in genetic research.\nThere also is some use of machine learning in synthetic biology, disease biology, nanotechnology (e.g. nanostructured materials and bionanotechnology), and materials science.\n\n\n=== Novel types of machine learning ===\n\nThere are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.\nSimilarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics). Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.\nMoreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain \u2013 as premised in the form of digital replication in The Age of Em, possibly using physical neural networks \u2013 that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence. An alternative or additive approach to scanning are types of reverse engineering of the brain.\nA subcategory of artificial intelligence is embodied, some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\n\n\n==== Digital ghosts ====\n\n\n==== Biological computing in AI and as AI ====\nHowever, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers \u2013 they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed \u2013 such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop). A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\". Technologies that integrate biology and are often AI-based include biorobotics.\n\n\n== Astronomy, space activities and ufology ==\n\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nIn the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data \u2013 such as real-time observations \u2013 and other technosignatures, e.g. via anomaly detection. In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs. The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.\n\n\n=== Future or non-human applications ===\n\nLoeb has speculated that one type of technological equipment the project may detect could be \"AI astronauts\" and in 2021 \u2013 in an opinion piece \u2013 that AI \"will\" \"supersede natural intelligence\", while Martin Rees stated that there \"may\" be more civilizations than thought with the \"majority of them\" being artificial. In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as \"safety of encounters with an alien AI\", suffering risks (or inverse goals), moral license/responsibility in respect to colonization-effects, or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of \"AI astronauts\" that engage in \"supervised evolution\" (see also: directed evolution, uplift, directed panspermia and space colonization).\n\n\n=== Astrochemistry ===\nIt can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals \u2013 such as phosphine possibly detected on Venus \u2013 which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.\n\n\n== Other fields of research ==\n\n\n=== Evidence of general impacts ===\nIn April 2024, the Scientific Advice Mechanism to the European Commission published advice including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\nAs benefits, the evidence review highlighted:\n\nits role in accelerating research and innovation\nits capacity to automate workflows\nenhancing dissemination of scientific work\nAs challenges:\n\nlimitations and risks around transparency, reproducibility and interpretability\npoor performance (inaccuracy)\nrisk of harm through misuse or unintended use\nsocietal concerns including the spread of misinformation and increasing inequalities\n\n\n=== Archaeology, history and imaging of sites ===\n\nMachine learning can help to restore and attribute ancient texts. It can help to index texts for example to enable better and easier searching and classification of fragments.\n\nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred. \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\". \n\n\n=== Physics ===\n\nA deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.\n\n\n=== Materials science ===\nAI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.\n\n\n=== Reverse engineering ===\nMachine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts, and for quickly understanding the behavior of malware. It can be used to reverse engineer artificial intelligence models. It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites. Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.\n\n\n== Law ==\n\n\n=== Legal analysis ===\nAI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers. While its use is common, it is not expected to replace most work done by lawyers in the near future.\nThe electronic discovery industry uses machine learning to reduce manual searching.\n\n\n=== Law enforcement and legal proceedings ===\nLaw enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants. \nCOMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.\nOne concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.\nIn 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.:\u200a124\u200a Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.:\u200a124\u200a\n\n\n== Services ==\n\n\n=== Human resources ===\n\nAnother application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.\n\n\n=== Job search ===\nAI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows.\n\n\n=== Online and telephone customer service ===\n\nAI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers.\nA Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative. Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.\n\n\n=== Hospitality ===\nIn the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots.\n\n\n== Media ==\n\nAI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\nTypical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\n\nMotion interpolation\nPixel-art scaling algorithms\nImage scaling\nImage restoration\nPhoto colorization\nFilm restoration and video upscaling\nPhoto tagging\nAutomated species identification (such as identifying plants, fungi and animals with an app)\nText-to-image models such as DALL-E, Midjourney and Stable Diffusion\nImage to video\nText to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google\nText to music with AI models such as MusicLM\nText to speech such as ElevenLabs and 15.ai\nMotion capture\nMake image transparent\n\n\n=== Deep-fakes ===\nDeep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\nIn January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.\nIn June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\nIn September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.\nIn 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames. DARPA gave 68 million dollars to work on deep-fake detection.\nAudio deepfakes and AI software capable of detecting deep-fakes and cloning human voices have been developed.\nRespeecher is a program that enables one person to speak with the voice of another.\n\n\n=== Video content analysis, surveillance and manipulated media detection ===\n\nAI algorithms have been used to detect deepfake videos.\n\n\n=== Video production ===\nArtificial Intelligence is also starting to be used in video production, with tools and softwares being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023. Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.\n\n\n=== Music ===\n\nAI has been used to compose music of various genres.\nDavid Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music. The algorithm behind Emily Howell is registered as a US patent.\nIn 2012, AI Iamus created the first complete classical album.\nAIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.\nMelomics creates computer-generated music for stress and pain relief.\nAt Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\nThe Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced and musicians such as Taryn Southern collaborated with the project to create music.\nSouth Korean singer Hayeon's debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.\n\n\n=== Writing and reporting ===\n\nNarrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.\nYseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.\nTALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".\nWhile AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.\nSouth Korean company Hanteo Global uses a journalism bot to write articles.\nLiterary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\n\n\n==== Sports writing ====\nIn 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using a software from Narrative Science.\nAfter being unable to cover every Minor League Baseball game with a large team of people, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.\nUOL in Brazil expanded the use of AI in their writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.\nEl Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter will be forced to change their comment in order to publish it.\nA local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been able to be done before without an extremely large team.\nLede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local news paper. This was met with a lot of criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.\n\n\n=== Wikipedia ===\n Millions of its articles have been edited by bots which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data, mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences, detecting covert vandalism or recommending articles and tasks to new editors.\nMachine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.\n\n\n=== Video games ===\n\nIn video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010). AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.\nKinect, which provides a 3D body\u2013motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.\n\n\n=== Art ===\n\nAI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968 with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.\nAI platforms such as \"DALL-E\", Stable Diffusion, Imagen, and Midjourney have been used for generating visual images from inputs such as text or other images. Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\nSince their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators. Examples of GAN programs that generate art include Artbreeder and DeepDream.\n\n\n==== Art analysis ====\nIn addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\n\n\n=== Computer animation ===\nAI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\". It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur. AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.\n\n\n== Utilities ==\n\n\n=== Energy system ===\nPower electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.\nThe U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability. \nMachine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).\n\n\n=== Telecommunications ===\nMany telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.\n\n\n== Manufacturing ==\n\n\n=== Sensors ===\nArtificial intelligence has been combined with digital spectrometry by IdeaCuria Inc., enable applications such as at-home water quality monitoring.\n\n\n=== Toys and games ===\nIn the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\nMattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.\n\n\n=== Oil and gas ===\nOil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.\n\n\n== Transport ==\n\n\n=== Automotive ===\n\nAI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.\nAI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg  and VW Caravell feature the DSP transmission. A number of \u0160koda variants (\u0160koda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\nThere are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses as well as autonomous rail transport in operation.\nThere also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.\nTransportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.\nAI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.\nAutonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018. A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.\nAutonomous vehicles require accurate maps to be able to navigate between destinations. Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).\n\n\n==== Traffic management ====\nAI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.\n\nSmart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.\n\n\n=== Military ===\nThe Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.\nAircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\nAI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.\nAOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\nSpeech recognition allows traffic controllers to give verbal directions to drones.\nArtificial intelligence supported design of aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n\n\n=== NASA ===\nIn 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved. The software compensated for damaged components by relying on the remaining undamaged components.\nThe 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.\n\n\n=== Maritime ===\nNeural networks are used by situational awareness systems in ships and boats. There also are autonomous boats.\n\n\n== Environmental monitoring ==\n\nAutonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning.\nFor example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution \u2013 primarily ocean pollution \u2013 by helping identify who and where mismanages plastic waste, dumping it into oceans.\n\n\n=== Early-warning systems ===\nMachine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.\n\n\n== Computer science ==\n\n\n=== Programming assistance ===\n\n\n==== AI-powered code assisting tools ====\nAI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy. Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\nGitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages. Price for individuals: $10/mo or $100/yr, with one free month trial.\nTabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota. Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.\nCodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.\nGhostwriter by Replit offers code completion and chat. They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\nCodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing. Individual plan is free, professional plan is $19/user/month.\nOther tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby\n\n\n==== Neural network design ====\nAI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.\n\n\n==== Quantum computing ====\n\nMachine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing).\n\n\n=== Historical contributions ===\nAI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:\n\nTime sharing\nInteractive interpreters\nGraphical user interfaces and the computer mouse\nRapid application development environments\nThe linked list data structure\nAutomatic storage management\nSymbolic programming\nFunctional programming\nDynamic programming\nObject-oriented programming\nOptical character recognition\nConstraint satisfaction\n\n\n== Business ==\n\n\n=== Content extraction ===\nAn optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.\n\n\n== Architecture ==\nAI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex. \nAI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.\n\n\n== List of applications ==\n\n\n== See also ==\nApplications of artificial intelligence to legal informatics\nApplications of deep learning\nApplications of machine learning\nCollective intelligence \u00a7 Applications\nList of artificial intelligence projects\nList of datasets for machine-learning research\nOpen data\nProgress in artificial intelligence\nTimeline of computing 2020\u2013present\n\n\n== Footnotes ==\n\n\n== Further reading ==\nKaplan, A.M.; Haenlein, M. (2018). \"Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence\". Business Horizons. 62 (1): 15\u201325. doi:10.1016/j.bushor.2018.08.004. S2CID 158433736.\nKurzweil, Ray (2005). The Singularity is Near: When Humans Transcend Biology. New York: Viking. ISBN 978-0-670-03384-3.\nNational Research Council (1999). \"Developments in Artificial Intelligence\". Funding a Revolution: Government Support for Computing Research. National Academy Press. ISBN 978-0-309-06278-7. OCLC 246584055.\nMoghaddam, M. J.; Soleymani, M. R.; Farsi, M. A. (2015). \"Sequence planning for stamping operations in progressive dies\". Journal of Intelligent Manufacturing. 26 (2): 347\u2013357. doi:10.1007/s10845-013-0788-0. S2CID 7843287.\nFelten, Ed (3 May 2016). \"Preparing for the Future of Artificial Intelligence\".", "link": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence"}, "Artificial intelligence in healthcare": {"title": "Artificial intelligence in healthcare", "content": "Artificial intelligence in healthcare is the application of artificial intelligence (AI) to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data. It can also augment and exceed human capabilities by providing faster or new ways to diagnose, treat, or prevent disease. Using AI in healthcare has the potential improve predicting, diagnosing and treating diseases. Through machine learning algorithms and deep learning, AI can analyse large sets of clinical data and electronic health records and can help to diagnose the disease more quickly and precisely. In addition, AI is becoming more relevant in bringing culturally competent healthcare practices to the industry. \nAI programs are applied to practices such as diagnostics, treatment protocol development, drug development, personalized medicine, and patient monitoring and care.\nBecause radiographs are the most common imaging tests conducted in most radiology departments, the potential for AI to help with triage and interpretation of traditional radiographs (X-ray pictures) is particularly noteworthy.\nAs widespread use of AI in healthcare is relatively new, research is ongoing into its application in various fields of medicine and related industries. \nUsing AI also presents unprecedented ethical concerns related to issues such as data privacy, automation of jobs, and amplifying already existing biases. Furthermore, new technologies brought about by AI in healthcare are often resisted by healthcare leaders, leading to slow and erratic adoption.\n\n\n== Applications in healthcare systems ==\n\n\n=== Disease diagnosis ===\nAccurate and early diagnosis of diseases is still a challenge in healthcare. Recognising medical conditions and their symptoms is a complex problem. AI can assist clinicians with its data processing capabilities to save time and improve accuracy. Through the use of machine learning, artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analyis of mass electronic health records (EHRs). AI can help early prediction, for example, of Alzheimer's disease and dementias, by looking through large numbers of similar cases and possible treatments. \nDoctors' decision making could also be supported by AI in urgent situations, for example in the emergency department. Here AI algorithms can help prioritise more serious cases and reduce waiting time. Decision support systems augmented with AI can offer real-time suggestions and faster data interpretation to aid the decisions made by healthcare professionals.\nIn 2023 a study reported higher satisfaction rates with ChatGPT-generated responses compared with those from physicians for medical questions posted on Reddit\u2019s r/AskDocs. Evaluators preferred ChatGPT's responses to physician responses in 78.6% of 585 evaluations, noting better quality and empathy. The authors noted that these were isolated questions, not in the context of an established patient-physician relationship.\nRecent developments in statistical physics, machine learning, and inference algorithms are also being explored for their potential in improving medical diagnostic approaches.\n\n\n=== Electronic health records ===\nElectronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, the next step is to use artificial intelligence to interpret the records and provide new information to physicians.\nOne application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms. For example, the term heart attack and myocardial infarction mean the same things, but physicians may use one over the over based on personal preferences. NLP algorithms consolidate these differences so that larger datasets can be analyzed. Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read. Other applications use concept processing to analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.\nBeyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record and predict a risk for a disease based on their previous information and family history. One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts. This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses. Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease. Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time. One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 70\u201372% accuracy in predicting individualized treatment response. These methods are helpful due to the fact that the amount of online health records doubles every five years. Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.\n\n\n=== Drug interactions ===\nImprovements in natural language processing led to the development of algorithms to identify drug-drug interactions in medical literature. Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken. To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers at Carlos III University assembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms. Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.  Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.\nOther algorithms identify drug-drug interactions from patterns in user-generated content, especially electronic health records and/or adverse event reports. Organizations such as the FDA Adverse Event Reporting System (FAERS) and the World Health Organization's VigiBase allow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.\n\n\n=== Telemedicine ===\n\nThe increase of telemedicine, the treatment of patients remotely, has shown the rise of possible AI applications. AI can assist in caring for patients remotely by monitoring their information through sensors. A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.\nAnother application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance on chatbots for mental healthcare does not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider (be it a chat-bot or psychologist), though.\nSince the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations. Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal. Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.\n\n\n=== Workload Management ===\nAI has the potential to streamline care coordination and reduce the workload. AI algorithms can automate administrative tasks, prioritize patient needs and facilitate seamless communication in a healthcare team. This enables healthcare providers to focus more on direct patient care and ensures the efficient and coordinated delivery of healthcare services.\n\n\n== Clinical applications ==\n\n\n=== Cardiovascular ===\nArtificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool. Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment for acute coronary syndrome. Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital. Another growing area of research is the utility of AI in classifying heart sounds and diagnosing valvular disease. Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data on social determinants of health as they pertain to cardiovascular disease.\nA key limitation in early studies evaluating AI were omissions of data comparing algorithmic performance to humans. Examples of studies which assess AI performance relative to physicians includes how AI is noninferior to humans in interpretation of cardiac echocardiograms and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.\nIn cardiovascular tissue engineering and organoid studies, AI is increasingly used to analyze microscopy images, and integrate electrophysiological read outs.\n\n\n=== Dermatology ===\nMedical imaging (such as X-ray and photography) is a commonly used tool in dermatology and the development of deep learning has been strongly tied to image processing. Therefore, there is a natural fit between the dermatology and deep learning. Machine learning learning holds great potential to process these images for better diagnoses. Han et al. showed keratinocytic skin cancer detection from face photographs. Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images. Noyan et al. demonstrated a convolutional neural network that achieved 94% accuracy at identifying skin cells from microscopic Tzanck smear images. A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.\nAccording to some researchers, AI algorithms have been shown to be more effective than dermatologists at identifying cancer. However, a 2021 review article found that a majority of papers analyzing the performance of AI algorithms designed for skin cancer classification failed to use external test sets. Only four research studies were found in which the AI algorithms were tested on clinics, regions, or populations distinct from those it was trained on, and in each of those four studies, the performance of dermatologists was found to be on par with that of the algorithm. Moreover, only one study was set in the context of a full clinical examination; others were based on interaction through web-apps or online questionnaires, with most based entirely on context-free images of lesions. In this study, it was found that dermatologists significantly outperformed the algorithms. Many articles claiming superior performance of AI algorithms also fail to distinguish between trainees and board-certified dermatologists in their analyses.\nIt has also been suggested that AI could be used to automatically evaluate the outcome of maxillo-facial surgery or cleft palate therapy in regard to facial attractiveness or age appearance.\n\n\n=== Gastroenterology ===\nAI can play a role in various facets of the field of gastroenterology. Endoscopic exams such as esophagogastroduodenoscopies (EGD) and colonoscopies rely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of early stomach cancer have shown sensitivity close to expert endoscopists.\nAI can assist doctors treating ulcerative colitis in detecting the microscopic activity of the disease in people and predicting when flare-ups will happen. For example, an AI-powered tool  was developed to analyse digitised bowel samples (biopsies). The tool was able to distinguish with 80% accuracy between samples that show remission of colitis and those with active disease. It also predicted the risk of a flare-up happening with the same accuracy. These rates of successfully using microscopic disease activity to predict disease flare are similar to the accuracy of pathologists.\n\n\n=== Obstetrics and gynaecology ===\nArtificial intelligence utilises massive amounts of data to help with predicting illness, prevention, and diagnosis, as well as patient monitoring. In obstetrics, artificial intelligence is utilised in magnetic resonance imaging, ultrasound, and foetal cardiotocography. AI contributes in the resolution of a variety of obstetrical diagnostic issues.\n\n\n=== Infectious diseases ===\nAI has shown potential in both the laboratory and clinical spheres of infectious disease medicine. During the COVID-19 pandemic, AI has been used for early detection, tracking virus spread and analysing virus behaviour, among other things. However there were only a few examples of AI being used directly in clinical practice during the pandemic itself. \nOther applications  of AI around infectious diseases include support-vector machines identifying antimicrobial resistance, machine learning analysis of blood smears to detect malaria, and improved point-of-care testing of Lyme disease based on antigen detection. Additionally, AI has been investigated for improving diagnosis of meningitis, sepsis, and tuberculosis, as well as predicting treatment complications in hepatitis B and hepatitis C patients.\n\n\n=== Musculoskeletal ===\nAI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients. Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients\u2019 pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.\n\n\n=== Neurology ===\nThe use of AI technologies has been explored for use in the diagnosis and prognosis of Alzheimer's disease (AD). For diagnostic purposes, machine learning models have been developed that rely on structural MRI inputs. The input datasets for these models are drawn from databases such as the Alzheimer's Disease Neuroimaging Initiative. Researchers have developed models that rely on convolutional neural networks with the aim of improving early diagnostic accuracy. Generative adversarial networks are a form of deep learning that have also performed well in diagnosing AD. There have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with AD. Forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets. They suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases.\n\n\n=== Oncology ===\nAI has been explored for use in cancer diagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics. AI has been trialed in cancer diagnostics with the reading of imaging studies and pathology slides.\nIn January 2020, Google DeepMind announced an algorithm capable of surpassing human experts in breast cancer detection in screening scans. A number of researchers, including Trevor Hastie, Joelle Pineau, and Robert Tibshirani among others, published a reply claiming that DeepMind's research publication in Nature lacked key details on methodology and code, \"effectively undermin[ing] its scientific value\" and making it impossible for the scientific community to confirm the work. In the MIT Technology Review, author Benjamin Haibe-Kains characterized DeepMind's work as \"an advertisement\" having little to do with science.\nIn July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date in identifying prostate cancer, with 98% sensitivity and 97% specificity. In 2023 a study reported the use of AI for CT-based radiomics classification at grading the aggressiveness of retroperitoneal sarcoma with 82% accuracy compared with 44% for lab analysis of biopsies.\n\n\n=== Ophthalmology ===\nArtificial intelligence-enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness. In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm. Moreover, AI technology may be used to further improve \"diagnosis rates\" because of the potential to decrease detection time.\n\n\n=== Pathology ===\n\nFor many diseases, pathological analysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods of digital pathology allows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B, gastric cancer, and colorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes. AI is well-suited for use in low-complexity pathological analysis of large-scale screening samples, such as colorectal or breast cancer screening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis. Several deep learning and artificial neural network models have shown accuracy similar to that of human pathologists, and a study of deep learning assistance in diagnosing metastatic breast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone. Additionally, implementation of digital pathology is predicted to save over $12 million for a university center over the course of five years, though savings attributed to AI specifically have not yet been widely researched. The use of augmented and virtual reality could prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review. AI also has the potential to identify histological findings at levels beyond what the human eye can see, and has shown the ability to use genotypic and phenotypic data to more accurately detect the tumor of origin for metastatic cancer. One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlled trials in determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.\n\n\n=== Primary care ===\nPrimary care has become one key development area for AI technologies. AI in primary care has been used for supporting decision making, predictive modelling, and business analytics. There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.\n\n\n=== Psychiatry ===\nIn psychiatry, AI applications are still in a phase of proof-of-concept. Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes, chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.\nChallenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017. Such applications outside the healthcare system raise various professional, ethical and regulatory questions. Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.\nIn 2023, US-based National Eating Disorders Association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it.\n\n\n=== Radiology ===\nAI is being studied within the field of radiology to detect and diagnose diseases through computerized tomography (CT) and magnetic resonance (MR) imaging. It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers. Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated. AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality, and automatically assessing image quality. Further research investigating the use of AI in nuclear medicine focuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies. The analysis of images for supervised AI applications in radiology encompasses two primary techniques at present: (1) convolutional neural network-based analysis; and (2) utilization of radiomics.\n\n\n=== Pharmacy ===\n\n\n== Industry ==\nThe trend of large health companies merging allows for greater health data accessibility. Greater health data lays the groundwork for the implementation of AI algorithms.\nA large part of industry focus of implementation of AI in the healthcare sector is in the clinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions. Numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry. Many companies investigate the market opportunities through the realms of \"data assessment, storage, management, and analysis technologies\" which are all crucial parts of the healthcare industry.\nThe following are examples of large companies that have contributed to AI algorithms for use in healthcare:\n\nIBM's Watson Oncology is in development at Memorial Sloan Kettering Cancer Center and Cleveland Clinic. IBM is also working with CVS Health on AI applications in chronic disease treatment and with Johnson & Johnson on analysis of scientific papers to find new connections for drug development. In May 2017, IBM and Rensselaer Polytechnic Institute began a joint project entitled Health Empowerment by Analytics, Learning and Semantics (HEALS), to explore using AI technology to enhance healthcare.\nMicrosoft's Hanover project, in partnership with Oregon Health & Science University's Knight Cancer Institute, analyzes medical research to predict the most effective cancer drug treatment options for patients. Other projects include medical image analysis of tumor progression and the development of programmable cells.\nGoogle's DeepMind platform is being used by the UK National Health Service to detect certain health risks through data collected via a mobile app. A second project with the NHS involves the analysis of medical images collected from NHS patients to develop computer vision algorithms to detect cancerous tissues.\nTencent is working on several medical systems and services. These include AI Medical Innovation System (AIMIS), an AI-powered diagnostic medical imaging service; WeChat Intelligent Healthcare; and Tencent Doctorwork\nIntel's venture capital arm Intel Capital invested in 2016 in the startup Lumiata, which uses AI to identify at-risk patients and develop care options.\n\nNeuralink has come up with a next-generation neuroprosthetic which intricately interfaces with thousands of neural pathways in the brain. Their process allows a chip, roughly the size of a quarter, to be inserted in the place of a chunk of a skull by a precision surgical robot to avoid accidental injury .\nDigital consultant apps use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solution[buzzword] to the marketplace. These archetypes depend on the value generated for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders).\nIFlytek launched a service robot \"Xiao Man\", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH (\"Cruzr\") and Softbank Robotics (\"Pepper\").\nThe Indian startup Haptik recently developed a WhatsApp chatbot which answers questions associated with the deadly coronavirus in India. Similarly, a software platform ChatBot in partnership with medtech startup Infermedica launched COVID-19 Risk Assessment ChatBot.\nWith the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies. Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well. Companies such as BMW, GE, Tesla, Toyota, and Volvo all have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances.\n\n\n== Expanding care to developing nations ==\nArtificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public.  Many new technology companies such as SpaceX and the Raspberry Pi Foundation have enabled more developing countries to have access to computers and the internet than ever before. With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.\nUsing AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient. The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.\n\n\n== Regulation ==\nWhile research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may nonetheless introduce several new types of risk to patients and healthcare providers, such as algorithmic bias, Do not resuscitate implications, and other machine morality issues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection. These challenges of the clinical use of AI have brought about a potential need for regulations. AI studies need to be completely and transparently reported to have value to inform regulatory approval. Depending on the phase of study, international consensus-based reporting guidelines (TRIPOD+AI, DECIDE-AI, CONSORT-AI) have been developed to provide recommendations on the key details that need to be reported.\n\nCurrently, there are regulations pertaining to the collection of patient data. This includes policies such as the Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR). The GDPR pertains to patients within the EU and details the consent requirements for patient data use when entities collect patient healthcare data. Similarly, HIPAA protects healthcare data from patient records in the United States. In May 2016, the White House announced its plan to host a series of workshops and formation of the National Science and Technology Council (NSTC) Subcommittee on Machine Learning and Artificial Intelligence. In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield of health information technology is in development stages.\nThere is concern that large language models can overwhelm people with both accurate health information and also misinformation, leading to potential challenges in public health. This calls for the need for policy and user guidance related to health information through AI.\n\n\n=== United Nations (WHO/ITU) ===\nThe joint ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) has built a platform - known as the ITU-WHO AI for Health Framework - for the testing and benchmarking of AI applications in health domain. As of November 2018, eight use cases are being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions.\n\n\n=== US FDA ===\n\nIn January 2021, the US FDA published a new Action Plan, entitled Artificial Intelligence (AI) /Machine Learning (ML)-Based Software as a Medical Device (SaMD) Action Plan. This plan lays out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software. There are five main actions the FDA plans to take to increase regulation: 1. Tailored Regulatory Framework for Ai/M:-based SaMD, 2. Good Machine Learning Practice (GMLP), 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias & Robustness, and 5. Real-World Performance(RWP). This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA.\nAccording to the U.S. Department of Health and Human Services, the Office for Civil Rights (OCR) has issued guidance on the ethical use of AI in healthcare. The guidance outlines four core ethical principles that must be followed: respect for autonomy, beneficence, non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. Moreover, as of March 2021, the OCR hired a Chief Artificial Intelligence Officer (OCAIO) to pursue the \"implementation of the HHS AI strategy\". The OCR also has issued rules and regulations to protect the privacy of individuals\u2019 health information. These regulations require healthcare providers to follow certain privacy rules when using AI. The OCR also requires healthcare providers to keep a record of how they use AI and to ensure that their AI systems are secure. Overall, the U.S. has taken steps to protect individuals\u2019 privacy and ethical issues related to AI in healthcare\nThe U.S. is not the only country to develop or initiate regulations of data privacy with AI. Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on Data Ethics has adopted recommendations on 'Data for the Benefit of the People'. These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity. The importance of human dignity is stressed, as it is said to outweigh profit and must be respected in all data processes\nThe European Union has implemented the General Data Protection Regulation (GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency. With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.\n\n\n== Ethical concerns ==\n\n\n=== Data collection ===\nIn order to effectively train Machine Learning and use AI in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy in most cases and is not well received publicly. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology. The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare.\nFurthermore, the lack of current regulations surrounding AI in the United States has generated concerns about mismanagement of patient data, such as with corporations utilizing patient data for financial gain. For example, Roche, a Swiss healthcare company, was found to have purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of $1.9 billion. Naturally, this generates questions of ethical concern; Is there a monetary price that can be set for data, and should it depend on its perceived value or contributions to science? Is it fair to patients to sell their data? These concerns were addressed in a survey conducted by the Pew Research Center in 2022 that asked Americans for their opinions about the increased presence of AI in their daily lives, and the survey estimated that 37% of Americans were more concerned than excited about such increased presence, with 8% of participants specifically associating their concern with \"people misusing AI\". Ultimately, the current potential of artificial intelligence in healthcare is additionally hindered by  concerns about mismanagement of data collected, especially in the United States.\n\n\n=== Automation ===\nA systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.\nAccording to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years. However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.\nAutomation can provide benefits alongside doctors as well. It is expected that doctors who take advantage of AI in healthcare will provide greater quality healthcare than doctors and medical establishments who do not. AI will likely not completely replace healthcare workers but rather give them more time to attend to their patients. AI may avert healthcare worker burnout and cognitive overload.\nRecently, there have been many discussions between healthcare experts in terms of AI and elder care. In relation to elder care, AI bots have been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand\n\n\n=== Bias ===\nSince AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care. A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed using many-to-many mapping.\nThere can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities.  Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. White males are overly represented in medical data sets. Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations. Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients. In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult. However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data.\nA final source of bias, which has been called \"label choice bias\", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients. Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.\n\n\n== History ==\nResearch in the 1960s and 1970s produced the first problem-solving program, or expert system, known as Dendral. While it was designed for applications in organic chemistry, it provided the basis for a subsequent system MYCIN, considered one of the most significant early uses of artificial intelligence in medicine. MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.\nThe 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians. Approaches involving fuzzy set theory, Bayesian networks, and artificial neural networks, have been applied to intelligent computing systems in healthcare.\nMedical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include: \n\nImprovements in computing power resulting in faster data collection and data processing\nGrowth of genomic sequencing databases\nWidespread implementation of electronic health record systems\nImprovements in natural language processing and computer vision, enabling machines to replicate human perceptual processes\nEnhanced the precision of robot-assisted surgery\nIncreased tree-based machine learning models that allow flexibility in establishing health predictors\nImprovements in deep learning techniques and data logs for rare diseases\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==", "link": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare"}, "Ethics of artificial intelligence": {"title": "Ethics of artificial intelligence", "content": "The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. \nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n\n\n== Machine ethics ==\n\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions. And large language models are capable of approximating human moral judgments. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit \u2013 or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\nIn Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms, while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".\n\n\n=== Robot ethics ===\n\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\n\n\n=== Ethical principles ===\nIn the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, solidarity.\nLuciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle \u2013 explicability.\n\n\n== Current challenges ==\n\n\n=== Algorithmic biases ===\n\nAI has become increasingly inherent in facial and voice recognition systems. Some of these systems have real business applications and directly impact people. These systems are vulnerable to biases and errors introduced by its human creators. Also, the data used to train these AI systems itself can have biases. For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender; these AI systems were able to detect gender of white men more accurately than gender of darker skin men. Further, a 2020 study reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's.\nBias can creep into algorithms in many ways. The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over 10-year period that came mostly from male candidates. The algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turned out to be biased against female and minority candidates. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus \u2014 the source material the algorithm uses to learn about the relationships between different words.\nLarge companies such as IBM, Google, etc. that provide significant funding for research and development, have made efforts to research and address these biases. One solution for addressing bias is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.\nThe problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. There are some open-sourced tools that are looking to bring more awareness to AI biases. There are however some limitations to the current landscape of fairness in AI, due e.g. to the intrinsic ambiguities in the concept of discrimination, both at philosophical and legal level.\nAI is also being incorporated into the hiring processes for almost every major company. There are many examples of certain characteristics that the AI is less likely to choose. Including the association between typically white names being more qualified, and the exclusion of anyone who went to a women's college. Facial recognition is also proven to be highly biased against those with darker skin tones. AI systems may be less accurate for black people, as was the case in the development of an AI-based pulse oximeter that overestimated blood oxygen levels in patients with darker skin, causing issues with their hypoxia treatment. The word Muslims is shown to be more highly associated with violence than any other religions. Oftentimes being able to easily detect the faces of white people while being unable to register the faces of people who are black. This is even more disconcerting considering the unproportionate use of security cameras and surveillance in communities that have high percentages of black or brown people. This fact has even been acknowledged in some states and led to the ban of police usage of AI materials or software. Even within the justice system AI has been proven to have biases against black people, labeling black court participants as high risk at a much larger rate then white participants. Often AI struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally. The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. A good example of this being if a facial recognition system was only tested on people who were white then it would only have the data and face scans of white people making it much harder for it to interpret the facial structure and tones of other races and ethnicities. To stop these biases there is not one single answer that can be used. The most useful approach has seemed to be the use of data scientists, ethicists and other policymakers to improve AI's problems with biases. Oftentimes the reasons for biases within AI is the data behind the program rather than the algorithm of the bot itself. AI's information is often pulled from past human decisions or inequalities that can lead to biases in the decision-making processes for that bot.\nInjustice in the use of AI will be much harder to eliminate within healthcare systems, as oftentimes diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race. This can be perceived as a bias because each patient is a different case and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what is considered a biased decision on who receives what treatment. While it is known that there are differences in how diseases and injuries affect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there are already certain tests for diseases, such as breast cancer, that are recommended to certain groups of people over others because they are more likely to contract the disease in question. If AI implements these statistics and applies them to each patient, it could be considered biased.\nExamples of AI being proven to have bias include when the system used to predict which defendants would be more likely to commit crimes in the future, COMPAS, was found to predict higher risk values for black people than what their actual risk was. Another example being within Google's ads which targeted men with higher paying jobs and women with lower paying jobs. It can be hard to detect AI biases within an algorithm as often it is not linked to the actual words associated with bias but rather words that biases can be affected by. An example of this being a person's residential area which can be used to link them to a certain group. This can lead to problems as oftentimes businesses can avoid legal action through this loophole. This being because of the specific laws regarding the verbiage that is considered discriminatory by governments enforcing these policies.\n\n\n==== Language bias ====\nSince current large language models are predominately trained on English-language data, they often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. Luo et al. show that when queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent.\n\n\n==== Gender bias ====\nLarge language models often reinforces gender stereotypes, assigning roles and characteristics based on traditional gender norms. For instance, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men, perpetuating gendered expectations and roles.\n\n\n==== Political bias ====\nLanguage models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\n\n\n==== Stereotyping ====\nBeyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.\n\n\n=== Dominance by tech giants ===\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\n\n=== Open-source ===\nBill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Organizations like Hugging Face and EleutherAI have been actively open-sourcing AI software. Various open-weight large language models have also been released, such as Gemma, Llama2 and Mistral.\nHowever, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE Standards Association has published a technical standard on Transparency of Autonomous Systems: IEEE 7001-2021. The IEEE effort identifies multiple scales of transparency for different stakeholders.\nThere are also concerns that releasing AI models may lead to misuse. For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted a blog on this topic, asking for government regulation to help determine the right thing to do. Furthermore, open-weight AI models can be fine-tuned to remove any counter-measure, until the AI model complies with dangerous requests, without any filtering. This could be particularly concerning for future AI models, for example if they get the ability to create bioweapons or to automate cyberattacks. OpenAI, initially committed to an open-source approach to the development of artificial general intelligence, eventually switched to a closed-source approach, citing competitiveness and safety reasons. Ilya Sutskever, OpenAI's chief AGI scientist, further said in 2023 \"we were wrong\", expecting that the safety reasons for not open-sourcing the most potent AI models will become \"obvious\" in a few years.\n\n\n=== Transparency ===\nApproaches like machine learning with neural networks can result in computers making decisions that neither they nor their developers can explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence. Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.\nIn healthcare, the use of complex AI methods or techniques often results in models described as \"black-boxes\" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in fields like healthcare, where understanding the rationale behind decisions can be crucial for trust, ethical considerations, and compliance with regulatory standards.\n\n\n=== Accountability ===\nA special case of the opaqueness of AI is that caused by it being anthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of its moral agency. This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system. Some recent digital governance regulation, such as the EU's AI Act is set out to rectify this, by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability. This includes potentially AI audits.\n\n\n=== Regulation ===\n\nAccording to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller. Similarly, according to a five-country study by KPMG and the University of Queensland Australia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.\nNot only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.\nOn June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its \"Policy and investment recommendations for trustworthy Artificial Intelligence\". This is the AI HLEG's second deliverable, after the April 2019 publication of the \"Ethics Guidelines for Trustworthy AI\". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector. The European Commission claims that \"HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved\" and states that the EU aims to lead on the framing of policies governing AI internationally. To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.\n\n\n== Emergent or potential future challenges ==\n\n\n=== Increasing use ===\nAI has been slowly making its presence more known throughout the world, from chat bots that seemingly have answers for every homework question to Generative artificial intelligence that can create a painting about whatever one desires. AI has become increasingly popular in hiring markets, from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires. Events, such as COVID-19, has only sped up the adoption of AI programs in the application process, due to more people having to apply electronically, and with this increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI. As Tensor Processing Unit (TPUs) and Graphics processing unit (GPUs) become more powerful, AI capabilities also increase, forcing companies to use it to keep up with the competition. Managing customers' needs and automating many parts of the workplace leads to companies having to spend less money on employees.\nAI has also seen increased usage in criminal justice and healthcare. For medicinal means, AI is being used more often to analyze patient data to make predictions about future patients' conditions and possible treatments. These programs are called Clinical decision support system (DSS). AI's future in healthcare may develop into something further than just recommended treatments, such as referring certain patients over others, leading to the possibility of inequalities.\n\n\n=== Robot rights ===\n\n\"Robot rights\" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society. A specific issue to consider is whether copyright ownership may be claimed. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry.\nIn October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition. Some saw this gesture as openly denigrating of human rights and the rule of law.\nThe philosophy of sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.\nJoanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society. Pressure groups to recognise 'robot rights' significantly hinder the establishment of robust international safety regulations.\n\n\n=== AI welfare ===\nIn 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may become conscious, such as the global workspace theory or the integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an \"explosion of artificial suffering\", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of conscious instances.\nSeveral labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be \"slightly conscious\". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future. In the ethics of uncertain sentience, the precautionary principle is often invoked.\nAccording to Carl Shulman and Nick Bostrom, it may be possible to create machines that would be \"superhumanly efficient at deriving well-being from resources\", called \"super-beneficiaries\". One reason for this is that digital hardware could enable much faster information processing than biological brains, leading to a faster rate of subjective experience. These machines could also be engineered to feel intense and positive subjective experience, unaffected by the hedonic treadmill. Shulman and Bostrom caution that failing to appropriately consider the moral claims of digital minds could lead to a moral catastrophe, while uncritically prioritizing them over human interests could be detrimental to humanity.\n\n\n=== Threat to human dignity ===\n\nJoseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:\n\nA customer service representative (AI technology is already used today for telephone-based interactive voice response systems)\nA nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)\nA soldier\nA judge\nA police officer\nA therapist (as was proposed by Kenneth Colby in the 70s)\nWeizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an \"atrophy of the human spirit that comes from thinking of ourselves as computers.\"\nPamela McCorduck counters that, speaking for women and minorities \"I'd rather take my chances with an impartial computer\", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all. However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained, which makes them even more difficult to spot and fight against.\nWeizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.\nAI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. \"When moralizing is both vehement and vague, it invites authoritarian abuse,\" he writes. Bill Hibbard writes that \"Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.\"\n\n\n=== Liability for self-driving cars ===\n\nAs the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed. There have been debates about the legal liability of the responsible party if these cars get into accidents. In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.\nIn another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.\nCurrently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.\nExperts contend that autonomous vehicles ought to be able to distinguish between rightful and harmful decisions since they have the potential of inflicting harm. The two main approaches proposed to enable smart machines to render moral decisions are the bottom-up approach, which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies, and the top-down approach, which involves programming specific ethical principles into the machine's guidance system. However, there are significant challenges facing both strategies: the top-down technique is criticized for its difficulty in preserving certain moral convictions, while the bottom-up strategy is questioned for potentially unethical learning from human activities.\n\n\n=== Weaponization ===\n\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.\nOn October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.\nResearch has studied how to make autonomous power with the ability to learn using assigned moral responsibilities. \"The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.\" From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.\nThere has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a \"Future of Life\" petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.\n\"If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow\", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.\nPhysicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like \"dumb robots going rogue or a network that develops a mind of its own.\" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence \"escapes the constraints of biology\". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.\nRegarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios \"seem potentially as important as the risks related to loss of control\", but research investigating AI's long-run social impact have spent relatively little time on this concern: \"this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them\".\nAcademic Gao Qiqi writes that military use of AI risks escalating military competition between countries and that the impact of AI in military matters will not be limited to one country but will have spillover effects.:\u200a91\u200a Gao cites the example of U.S. military use of AI, which he contends has been used as a scapegoat to evade accountability for decision-making.:\u200a91\u200a\nA summit was held in 2023 in the Hague on the issue of using AI responsibly in the military domain.\n\n\n=== Singularity ===\n\nVernor Vinge, among numerous others, have suggested that a moment may come when some, if not all, computers are smarter than humans. The onset of this event is commonly referred to as \"the Singularity\" and is the central point of discussion in the philosophy of Singularitarianism. While opinions vary as to the ultimate fate of humanity in wake of the Singularity, efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists, philosophers, and the public at large.\nMany researchers have argued that, through an intelligence explosion, a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals. In his paper \"Ethical Issues in Advanced Artificial Intelligence\" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that an artificial superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.\nHowever, Bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease, poverty, and environmental destruction, and could help humans enhance themselves.\nUnless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation. AI researchers such as Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines.\n\n\n== Institutions in AI policy & ethics ==\nThere are many organizations concerned with AI ethics and policy, public and governmental as well as corporate and societal.\nAmazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.\nThe IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization. The IEEE's Ethics of Autonomous Systems initiative aims to address ethical dilemmas related to decision-making and the impact on society while developing guidelines for the development and use of autonomous systems. In particular in domains like artificial intelligence and robotics, the Foundation for Responsible Robotics is dedicated to promoting moral behavior as well as responsible robot design and use, ensuring that robots maintain moral principles and are congruent with human values.\nTraditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.\nAI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.\n\n\n=== Intergovernmental initiatives ===\nThe European Commission has a High-Level Expert Group on Artificial Intelligence. On 8 April 2019, this published its \"Ethics Guidelines for Trustworthy Artificial Intelligence\". The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit, which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020. The European Commission also proposed the Artificial Intelligence Act.\nThe OECD established an OECD AI Policy Observatory.\nIn 2021, UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence, the first global standard on the ethics of AI.\n\n\n=== Governmental initiatives ===\nIn the United States the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the \"American AI Initiative\" instructed NIST the (National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019).\nIn January 2020, in the United States, the Trump Administration released a draft executive order issued by the Office of Management and Budget (OMB) on \"Guidance for Regulation of Artificial Intelligence Applications\" (\"OMB AI Memorandum\"). The order emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market. There is a nod to the need for privacy concerns, but no further detail on enforcement. The advances of American AI technology seems to be the focus and priority. Additionally, federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill.\nThe Computing Community Consortium (CCC) weighed in with a 100-plus page draft report \u2013 A 20-Year Community Roadmap for Artificial Intelligence Research in the US\nThe Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.\nThe Non-Human Party is running for election in New South Wales, with policies around granting rights to robots, animals and generally, non-human entities whose intelligence has been overlooked.\nIn Russia, the first-ever Russian \"Codex of ethics of artificial intelligence\" for business was signed in 2021. It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others.\n\n\n=== Academic initiatives ===\nThere are three research institutes at the University of Oxford that are centrally focused on AI ethics. The Future of Humanity Institute that focuses both on AI Safety and the Governance of AI. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote AI ethics as a field proper in comparison to related applied ethics fields. The Oxford Internet Institute, directed by Luciano Floridi, focuses on the ethics of near-term AI technologies and ICTs.\nThe Centre for Digital Governance at the Hertie School in Berlin was co-founded by Joanna Bryson to research questions of ethics and technology.\nThe AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.\nThe Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment, and policy.\nThe Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph L\u00fctge conducts research across various domains such as mobility, employment, healthcare and sustainability.\nBarbara J. Grosz, the Higgins Professor of Natural Sciences at the Harvard John A. Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard's computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work.\n\n\n=== Private organizations ===\nAlgorithmic Justice League\nBlack in AI\nData for Black Lives\nWomen in AI (WAI)\n\n\n== History ==\nHistorically speaking, the investigation of moral and ethical implications of \"thinking machines\" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being, and so does Descartes, who describes what could be considered an early version of the Turing test.\nThe romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R \u2013 Rossum's Universal Robots, Karel \u010capek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.\nIn the 1950s, Isaac Asimov considered the issue of how to control machines in I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances. More recently, academics and many governments have challenged the idea that AI can itself be held accountable. A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.\nEliezer Yudkowsky, from the Machine Intelligence Research Institute suggested in 2004 a need to study how to build a \"Friendly AI\", meaning that there should also be efforts to make AI intrinsically friendly and humane.\nIn 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.\nAlso in 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique F\u00e9d\u00e9rale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.\n\n\n== Role and impact of fiction ==\n\nThe role of fiction with regards to AI ethics has been a complex one. One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Rob\u00f2tica i Inform\u00e0tica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes, in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.\n\n\n=== Impact on technological development ===\nWhile the anticipation of a future dominated by potentially indomitable technology has fueled the imagination of writers and film makers for a long time, one question has been less frequently analyzed, namely, to what extent fiction has played a role in providing inspiration for technological development. It has been documented, for instance, that the young Alan Turing saw and appreciated aforementioned Shaw's play Back to Methuselah in 1933 (just 3 years before the publication of his first seminal paper, which laid the groundwork for the digital computer), and he would likely have been at least aware of plays like R.U.R., which was an international success and translated into many languages.\nOne might also ask the question which role science fiction played in establishing the tenets and ethical implications of AI development: Isaac Asimov conceptualized his Three Laws of Robotics in the 1942 short story \"Runaround\", part of the short story collection I, Robot; Arthur C. Clarke's short The Sentinel, on which Stanley Kubrick's film 2001: A Space Odyssey is based, was written in 1948 and published in 1952. Another example (among many others) would be Philip K. Dick's numerous short stories and novels \u2013 in particular Do Androids Dream of Electric Sheep?, published in 1968, and featuring its own version of a Turing Test, the Voight-Kampff Test, to gauge emotional responses of androids indistinguishable from humans. The novel later became the basis of the influential 1982 movie Blade Runner by Ridley Scott.\nScience fiction has been grappling with ethical implications of AI developments for decades, and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved: Spike Jonze's 2013 film Her shows what can happen if a user falls in love with the seductive voice of his smartphone operating system; Ex Machina, on the other hand, asks a more difficult question: if confronted with a clearly recognizable machine, made only human by a face and an empathetic and sensual voice, would we still be able to establish an emotional connection, still be seduced by it? (The film echoes a theme already present two centuries earlier, in the 1817 short story The Sandmann by E. T. A. Hoffmann.)\nThe theme of coexistence with artificial sentient beings is also the theme of two recent novels: Machines Like Me by Ian McEwan, published in 2019, involves, among many other things, a love-triangle involving an artificial person as well as a human couple. Klara and the Sun by Nobel Prize winner Kazuo Ishiguro, published in 2021, is the first-person account of Klara, an 'AF' (artificial friend), who is trying, in her own way, to help the girl she is living with, who, after having been 'lifted' (i.e. having been subjected to genetic enhancements), is suffering from a strange illness.\n\n\n=== TV series ===\nWhile ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012\u20132013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013\u20132019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.\n\n\n=== Future visions in fiction and games ===\nThe movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story \"The Planck Dive\" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.\nThe ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games. It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.\nDetroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.\nOver time, debates have tended to focus less and less on possibility and more on desirability, as emphasized in the \"Cosmist\" and \"Terran\" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.\nExperts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== External links ==\nEthics of Artificial Intelligence at the Internet Encyclopedia of Philosophy\nEthics of Artificial Intelligence and Robotics at the Stanford Encyclopedia of Philosophy\nRussell S, Hauert S, Altman R, Veloso M (May 2015). \"Robotics: Ethics of artificial intelligence\". Nature. 521 (7553): 415\u2013418. Bibcode:2015Natur.521..415.. doi:10.1038/521415a. PMID 26017428. S2CID 4452826.\nBBC News: Games to take on a life of their own\nWho's Afraid of Robots? Archived 2018-03-22 at the Wayback Machine, an article on humanity's fear of artificial intelligence.\nA short history of computer ethics\nAI Ethics Guidelines Global Inventory by Algorithmwatch\nHagendorff T (March 2020). \"The Ethics of AI Ethics: An Evaluation of Guidelines\". Minds and Machines. 30 (1): 99\u2013120. arXiv:1903.03425. doi:10.1007/s11023-020-09517-8. S2CID 72940833.\nSheludko, M. (December, 2023). Ethical Aspects of Artificial Intelligence: Challenges and Imperatives. Software Development Blog.\nEisikovits N. \"AI Is an Existential Threat--Just Not the Way You Think\". Scientific American. Retrieved 2024-03-04.", "link": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence"}, "History of artificial intelligence": {"title": "History of artificial intelligence", "content": "The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College during the summer of 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars to make this vision come true.\nEventually, it became obvious that researchers had grossly underestimated the difficulty of the project. In 1974, criticism from James Lighthill and pressure from the U.S. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI and by the late 80s the industry had grown into the billions of dollars. However, investors' enthusiasm waned in the 1990s and the field was criticized in the press and avoided by industry (a period  known as the \"AI Winter\"). Nevertheless, research and funding continued to grow under other names.  \nIn the early 2000s, machine learning was applied to a wide range of problems in academic and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets and the application of solid mathematical methods. In 2012, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications. Investment in AI boomed in the 2020s.\n\n\n== Precursors ==\n\n\n=== Mythical, fictional, and speculative precursors ===\n\n\n==== Myth and legend ====\nIn Greek mythology, Talos was a giant made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless.\nPygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.\n\n\n==== Medieval legends of artificial beings ====\n\nIn Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant.\nThe earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God\u2019s names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak.\nTakwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.\nIn Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.\n\n\n==== Modern fiction ====\n\nBy the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley's Frankenstein  and Karel \u010capek's R.U.R. (Rossum's Universal Robots)\nexplored the concept of artificial life. Speculative essays, such as Samuel Butler's \"Darwin among the Machines\", and Edgar Allan Poe's \"Maelzel's Chess Player\" reflected society's growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today.\n\n\n==== Automata ====\n\nRealistic humanoid automata were built by craftsman from many civilizations, including Yan Shi, Hero of Alexandria, Al-Jazari, Haroun al-Rashid, Jacques de Vaucanson, Leonardo Torres y Quevedo, Pierre Jaquet-Droz and Wolfgang von Kempelen.\nThe oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion\u2014Hermes Trismegistus wrote that \"by discovering the true nature of the gods, man has been able to reproduce it\". English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.\nDuring the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of M\u00edmir. According to legend, M\u00edmir was known for his intellect and wisdom, and was beheaded in the \u00c6sir-Vanir War. Odin is said to have \"embalmed\" the head with herbs and spoke incantations over it such that M\u00edmir\u2019s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.\n\n\n=== Formal reasoning ===\nArtificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical\u2014or \"formal\"\u2014reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khw\u0101rizm\u012b (who developed algebra and gave his name to the word algorithm) and European scholastic philosophers such as William of Ockham and Duns Scotus.\nSpanish philosopher Ramon Llull (1232\u20131315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.\n\nIn the 17th century, Leibniz, Thomas Hobbes and Ren\u00e9 Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: \"For reason ... is nothing but reckoning, that is adding and subtracting\". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that \"there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.\" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.\nThe study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: \"can all of mathematical reasoning be formalized?\" His question was answered by G\u00f6del's incompleteness proof, Turing's machine and Church's Lambda calculus.\n\nTheir answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine\u2014a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.\n\n\n=== Computer science ===\n\nCalculating machines were designed or built in antiquity and throughout history by many people, including \nGottfried Leibniz,\nJoseph Marie Jacquard, \nCharles Babbage,\nPercy Ludgate,\nLeonardo Torres Quevedo,\nVannevar Bush,\nand others. Ada Lovelace speculated that Babbage's machine was \"a thinking or ... reasoning machine\", but warned \"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\" of the machine.\nThe first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's and ABC and ENIAC at the University of Pennsylvania). ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann, and proved to be the most influential.\n\n\n== Birth of artificial intelligence (1941-56) ==\n\nThe earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an \"electronic brain\".\nIn the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research. Alan Turing was among the first people to seriously investigate the theoretical possibility of \"machine intelligence\". The field of \"artificial intelligence research\" was founded as an academic discipline in 1956.\n\n\n=== Turing Test ===\n\nIn 1950 Turing published a landmark paper \"Computing Machinery and Intelligence\", in which he speculated about the possibility of creating machines that think. In the paper, he noted that \"thinking\" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \"thinking\". This simplified version of the problem allowed Turing to argue convincingly that a \"thinking machine\" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence.\n\n\n=== Artificial neural networks ===\nWalter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function. One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC. Minsky would later become one of the most important leaders and innovators in AI.\n\n\n=== Cybernetic robots ===\nExperimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.\n\n\n=== Game AI ===\nIn 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper \"Some Studies in Machine Learning Using the Game of Checkers\", eventually achieved sufficient skill to challenge a respectable amateur. Samuel's program was among the first uses of what would later be called machine learning. Game AI would continue to be used as a measure of progress in AI throughout its history.\n\n\n=== Symbolic reasoning and the Logic Theorist ===\n\nWhen access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.\nIn 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the \"Logic Theorist\", with help from J. C. Shaw. The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some. Simon said that they had \"solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.\" The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution.\n\n\n=== Dartmouth Workshop ===\n\nThe Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline. It was organized by Marvin Minsky and John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM. The proposal for the conference stated they intended to test the assertion that \"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\". The term \"Artificial Intelligence\" was introduced by John McCarthy at the workshop. \nThe participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research. At the workshop Newell and Simon debuted the \"Logic Theorist\". The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.\n\n\n=== Cognitive revolution ===\n\nIn the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT). At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper \"The Magical Number Seven, Plus or Minus Two\". Miller wrote \"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\"\nThis meeting was the beginning of the \"cognitive revolution\"\u2014an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.\nThe cognitive approach allowed researchers to consider \"mental objects\" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as \"unobservable\" by earlier paradigms such as behaviorism. Symbolic mental objects would become the major focus of AI research and funding for the next several decades.\n\n\n== Early successes (1956-1974) ==\nThe programs developed in the years after the Dartmouth Workshop were, to most people, simply \"astonishing\": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such \"intelligent\" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as \"ARPA\") poured money into the field. Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.\n\n\n=== Approaches ===\nThere were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:\n\n\n==== Reasoning, planning and problem solving as search ====\nMany early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. The principal difficulty was that, for many problems, the number of possible paths through the \"maze\" was astronomical (a situation known as a \"combinatorial explosion\"). Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution.\nNewell and Simon tried to capture a general version of this algorithm in a program called the \"General Problem Solver\". Other \"searching\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961. Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey.\n\n\n==== Natural language ====\n\nAn important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.\nA semantic net represents concepts (e.g. \"house\", \"door\") as nodes, and relations among concepts as links between the nodes (e.g. \"has-a\"). The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.\nJoseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect). But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first chatbot.\n\n\n==== Micro-worlds ====\nIn the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a \"blocks world,\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.\nThis paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented \"constraint propagation\"), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them.\n\n\n==== Perceptrons and early neural networks ====\n\nIn the 1960s funding was primarily directed towards laboratories researching symbolic AI, however several people still pursued research in neural networks.\n\nThe perceptron, a single-layer neural network was introduced in 1958 by Frank Rosenblatt (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science). Like most AI researchers, he was optimistic about their power, predicting that a perceptron \u201cmay eventually be able to learn, make decisions, and translate languages.\" Rosenblatt was primarily funded by Office of Naval Research. \nBernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights. A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II had 6600 adjustable weights, and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets. Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers.\nHowever, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s. In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded in connectionism for 10 years. The competition for government funding ended with the victory of symbolic AI approaches over neural networks. \nMinsky (who had worked on SNARC) became a staunch objector to pure connectionist AI. Widrow (who had worked on ADALINE) turned to adaptive signal processing. The SRI group (which worked on MINOS) turned to symbolic AI and robotics. \nThe main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers). The AI community became aware of backpropogation in the 80s, and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions. Rosenblatt did not live to see this, however, as he died in a boating accident in 1971.\n\n\n=== Optimism ===\nThe first generation of AI researchers made these predictions about their work:\n\n1958, H. A. Simon and Allen Newell: \"within ten years a digital computer will be the world's chess champion\" and \"within ten years a digital computer will discover and prove an important new mathematical theorem.\"\n1965, H. A. Simon: \"machines will be capable, within twenty years, of doing any work a man can do.\"\n1967, Marvin Minsky: \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.\"\n1970, Marvin Minsky (in Life magazine): \"In from three to eight years we will have a machine with the general intelligence of an average human being.\"\n\n\n=== Financing ===\nIn June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA). The money was used to fund project MAC which subsumed the \"AI Group\" founded by Minsky and McCarthy five years earlier. DARPA continued to provide $3 million each year until the 70s. DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963. Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965. These four institutions would continue to be the main centers of AI research and funding in academia for many years.\nThe money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should \"fund people, not projects!\" and allowed researchers to pursue whatever directions might interest them.  This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this \"hands off\" approach did not last.\n\n\n== First AI Winter (1974\u20131980) ==\nIn the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced. The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.\nThese setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories and the critiques were largely ignored. General public interest in the field continued to grow, the number of researchers increased dramatically, and new ideas were explored in logic programming, commonsense reasoning and many other areas. Historian Thomas Haigh argued in 2023 that there was no winter, and AI researcher Nils Nilsson described this period as the most \"exciting\" time to work in AI.\n\n\n=== Problems ===\nIn the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, \"toys\". AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s:\n\nLimited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example: Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only 20 words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. \"With enough horsepower,\" he wrote, \"anything will fly\".\nIntractability and the combinatorial explosion: In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can only be solved in exponential time. Finding optimal solutions to these problems requires extraordinary amounts of computer time, except when the problems are trivial. This limitation applied to all symbolic AI programs that used search trees and meant that many of the \"toy\" solutions used by AI would never scale to useful systems.\nMoravec's paradox: Early AI research had been very successful at getting computers to do \"intelligent\" tasks like proving theorems, solving geometry problems and playing chess. Their success at these intelligent tasks convinced them that the problem of intelligent behavior had been largely solved. However, they utterly failed to make progress on \"unintelligent\" tasks like recognizing a face or crossing a room without bumping into anything. By the 1980s, researchers would realize that symbolic reasoning was utterly unsuited for these perceptual and sensorimotor tasks and that there were limits to this approach.\nThe breadth of commonsense knowledge: Many important artificial intelligence applications like vision or natural language require enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a vast amount of information with billions of atomic facts. No one in 1970 could build a database large enough and no one knew how a program might learn so much information.\nRepresenting commonsense reasoning: A number of related problems appeared when researchers tried to represent commonsense reasoning using formal logic or symbols.  Descriptions of very ordinary deductions tended to get longer and longer the more one worked on them, as more and more exceptions, clarifications and distinctions were required. However, when people thought about ordinary concepts they did not rely on precise definitions, rather they seemed to make hundreds of imprecise assumptions, correcting them when necessary using their entire body of commonsense knowledge. Gerald Sussman observed that \"using precise language to describe essentially imprecise concepts doesn't make them any more precise.\"\n\n\n=== Decrease in funding ===\n\nThe agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support. In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country. (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.) DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.\nHans Moravec blamed the crisis on the unrealistic predictions of his colleagues. \"Many researchers were caught up in a web of increasing exaggeration.\" However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.\nThe major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.\n\n\n=== Philosophical and ethical critiques ===\n\nSeveral philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that G\u00f6del's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal of embodied, instinctive, unconscious \"know how\". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\".\nThese critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle \"they misunderstand, and should be ignored.\" Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\" Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he \"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\" and was unprofessional and childish.\nWeizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \"computer program which can conduct psychotherapeutic dialogue\" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.\n\n\n=== Logic at Stanford, CMU and Edinburgh ===\nLogic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal. In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog. Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.\nCritics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof. McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems\u2014not machines that think as people do.\n\n\n=== MIT's \"anti-logic\" approach ===\nAmong the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \"story understanding\" and \"object recognition\" that required a machine to think like a person. In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles. Schank described their \"anti-logic\" approaches as scruffy, as opposed to the neat paradigm used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.\nIn 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds). Minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals, or over-ridden as necessary. He called these structures frames. Schank used a version of frames he called \"scripts\" to successfully answer questions about short stories in English. Frames would eventually be widely used in software engineering under the name object-oriented programming.\nThe logicians rose to the challenge. Pat Hayes claimed that \"most of 'frames' is just a new syntax for parts of first-order logic.\" But he noted that \"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\".\nRay Reiter admitted that \"conventional logics, such as first-order\nlogic, lack the expressive power to adequately represent the knowledge required for reasoning by default\". He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its \"procedural equivalent\" as negation as failure in Prolog. The closed world assumption, as formulated by Reiter, \"is not a first-order notion. (It is a meta notion.)\" However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.\nDuring the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics.\n\n\n== Boom (1980\u20131987) ==\nIn the 1980s, a form of AI program called \"expert systems\" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative. \"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.\" \n\n\n=== Expert systems become widely used ===\nAn expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts.\nThe earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.\nExpert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.\nIn 1980, an expert system called R1 was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.\n\n\n=== Government funding increases ===\nIn 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they initially chose Prolog as the primary computer language for the project.\nOther countries responded with new programs of their own. The UK began the \u00a3350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or \"MCC\") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.\n\n\n=== Knowledge revolution ===\nThe power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. \"AI researchers were beginning to suspect\u2014reluctantly, for it violated the scientific canon of parsimony\u2014that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\" writes Pamela McCorduck. \"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s. It was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required. \nIn the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started a database called Cyc, argued that there is no shortcut \u2015 the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.\n\n\n== New directions in the 1980s ==\nAlthough symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as \"connectionism\", robotics, \"soft\" computing and reinforcement learning. Nils Nilsson called these approaches \"sub-symbolic\".\n\n\n=== Revival of neural networks: \"connectionism\" ===\n\nIn 1982, physicist John Hopfield was able to prove that a form of neural network (now called a \"Hopfield net\") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called \"backpropagation\". These two developments helped to revive the exploration of artificial neural networks. \nNeural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened \"connectionism\" and there was a considerable debate between advocates of symbolic AI the \"connectionists\". Hinton called symbols the \"luminous aether of AI\" \u2013 that is, an unworkable and misleading model of intelligence.\nIn 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks.\n\n\n=== Robotics and embodied reason ===\n\nRodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body \u2014 it needs to perceive, move, survive and deal with the world. Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence \"from the bottom up\".\nA precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)\nIn his 1990 paper \"Elephants Don't Play Chess,\" robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\"\nIn the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the \"embodied mind thesis\".\n\n\n=== Soft computing and probabilistic reasoning ===\nSoft computing uses methods that work with incomplete and imprecise information. They do not attempt to give precise, logical answers, but give results that are only \"probably\" correct. This allowed them to solve problems that precise symbolic methods could not handle. Press accounts often claimed these tools could \"think like a human\".\nJudea Pearl's Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book brought probability and decision theory into AI. Fuzzy logic, developed by Lofti Zadeh in the 60s, began to be more widely used in AI and robotics. Evolutionary computation and artificial neural networks also handle imprecise information, and are classified  as \"soft\". In the 90s and early 2000s many other soft computing tools were developed and put into use, including Bayesian networks, hidden Markov models, information theory and stochastic modeling. These tools in turn depended on advanced mathematical techniques such as classical optimization. For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called \"computational intelligence\".\n\n\n=== Reinforcement learning ===\nReinforcement learning gives an agent a reward every time it performs a desired action well, and may give negative rewards (or \u201cpunishments\u201d) when it performs poorly. It was described in the first half of the twentieth century by psychologists using animal models, such as Thorndike, Pavlov and Skinner. In the 1950s, Alan Turing and Arthur Samuels foresaw the role of reinforcement learning in AI. \nA successful and influential research program was led by Richard Sutton and Andrew Barto beginning 1972. Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades. In 1988, Sutton described machine learning in terms of decision theory (i.e., the Markov decision process). This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research.\nAlso in 1988, Sutton and Barto developed the \u201ctemporal difference\u201d (TD) learning algorithm, where the agent is rewarded only when its predictions about the future show improvement. It significantly outperformed previous algorithms. TD-learning was used by Gerald Tesauro in 1992 in the program TD-Gammon, which played backgammon as well as the best human players. The program learned the game by playing against itself with zero prior knowledge. In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the TD-learning algorithm. TD learning would be become highly influential in the 21st century, used in both AlphaGo and AlphaZero.\n\n\n== Bust: second AI winter ==\nThe business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable. The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\".\nOver the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability. By 2000, AI had achieved some of its oldest goals. The field was both more cautious and more successful than it had ever been. \n\n\n=== AI winter ===\nThe term \"AI winter\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.\nThe first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.\nEventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, and they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs). Expert systems proved useful, but only in a few special contexts.\nIn the late 1980s, the Strategic Computing Initiative cut funding to AI \"deeply and brutally\". New leadership at DARPA had decided that AI was not \"the next wave\" and directed funds towards projects that seemed more likely to produce immediate results.\nBy 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like \"carry on a casual conversation\" would not be accomplished for another 40 years. As with other AI projects, expectations had run much higher than what was actually possible.\nOver 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that \"The immediate future of artificial intelligence\u2014in its commercial form\u2014seems to rest in part on the continued success of neural networks.\"\n\n\n=== AI behind the scenes ===\nIn the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems and their solutions proved to be useful throughout the technology industry, such as data mining, industrial robotics, logistics, speech recognition, banking software, medical diagnosis and Google's search engine.\nThe field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nMany researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, \"cognitive systems\" or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"\n\n\n=== Mathematical rigor, greater collaboration and a narrow focus ===\nAI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. Most of the new directions in AI relied heavily on mathematical models, including artificial neural networks, probabilistic reasoning, soft computing and reinforcement learning. In the 90s and 2000s, many other highly mathematical tools were adapted for AI. These tools were applied to machine learning, perception and mobility. \nThere was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like statistics, mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \"scientific\" discipline.\nAnother key reason for the success in the 90s was that AI researchers focussed on specific problems with verifiable solutions (an approach later derided as narrow AI). This provided useful tools in the present, rather than speculation about the future.\n\n\n=== Intelligent agents ===\nA new paradigm called \"intelligent agents\" became widely accepted during the 1990s. Although earlier researchers had proposed modular \"divide and conquer\" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.\nAn intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are \"intelligent agents\", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as \"the study of intelligent agents\". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence. \nThe paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence.\n\n\n=== Milestones and Moore's law ===\nOn May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws.\nThese successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years. The fundamental problem of \"raw computer power\" was slowly being overcome.\n\n\n== Big data, deep learning, AGI (2005\u20132017) ==\nIn the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition. Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a \"frenzy\".\nIn 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence. By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind. During the same period, new insights into superintelligence raised concerns that AI was an existential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016. \n\n\n=== Big data and big machines ===\n\nThe success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers. Russell and Norvig wrote that the \"improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm.\" Geoffrey Hinton recalled that back in the 90s, the problem was that \u201cour labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\u201d This was no longer true by 2010.\nThe most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group at UMass Amherst released Labeled Faces in the Wild, an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades. Fei-Fei Li developed ImageNet, a database of three million images captioned by volunteers using the Amazon Mechanical Turk. Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems. Google released word2vec in 2013 as an open source resource. It used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze, London+England-France = Paris. This database in particular would be essential for the development of large language models in the late 2010s.\nThe explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped. And, for specific problems, large privately held databases contained the relevant data. McKinsey Global Institute reported that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\". This collection of information was known in the 2000s as big data.\nIn February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two best Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. Watson's expertise would have been impossible without the information available on the internet.\n\n\n=== Deep learning ===\n\nIn 2012, AlexNet, a deep learning model, developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second place winner. Krizhevsky worked with Geoffrey Hinton at the University of Toronto. This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning. \nDeep learning uses a multi-layer perceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data. Before these became \navailable, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement. Deep learning was simpler and more general. \nDeep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance. Investment and interest in AI boomed as a result.\n\n\n=== The alignment problem ===\nIt became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility of superintelligent machines and what they might mean for human society. Some of this was optimistic (such as Ray Kurzweil's The Singularity is Near), but others warned that a sufficiently powerful AI was existential threat to humanity, such as Nick Bostrom and Eliezer Yudkowsky. The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue. \nAI programs in the 21st century are defined by their goals \u2013 the specific measures that they are designed to optimize. Nick Bostrom's influential 2005 book Superintelligence argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal. Stuart J. Russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning \"you can't fetch the coffee if you're dead\". (This problem is known by the technical term \"instrumental convergence\".) The solution is to align the machine's goal function with the goals of its owner and humanity in general. Thus, the problem of mitigating the risks and unintended consequences of AI became known as \"the value alignment problem\" or AI alignment.\nAt the same time, machine learning systems had begun to have disturbing unintended consequences. Cathy O'Neil explained how statistical algorithms had been among the causes of the 2008 economic crash, Julia Angwin of ProPublica argued that the COMPAS system used by the criminal justice system exhibited racial bias under some measures, others showed that many machine learning systems exhibited some form of racial bias, and there were many other examples of dangerous outcomes that had resulted from machine learning systems.\nIn 2016, the election of Donald Trump and the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models. Issues of fairness and unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The value alignment problem became a serious field of academic study.\n\n\n=== Artificial general intelligence research ===\nIn the early 2000s, several researchers became concerned that mainstream AI was too focused on \"measurable performance in specific applications\" (known as \"narrow AI\") and had abandoned AI\u2019s original goal of creating versatile, fully intelligent machines. An early critic was Nils Nilsson in 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007-2009. Minsky organized a symposium on \"human-level AI\" in 2004. Ben Goertzel adopted the term \"artificial general intelligence\" for the new sub-field, founding a journal and holding conferences beginning in 2008. The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI.\nSeveral competing companies, laboratories and foundations were founded to develop AGI in the 2010s. DeepMind was founded in 2010 by three English scientists, Demis Hassabis, Shane Legg and Mustafa Suleyman, with funding from Peter Thiel and later Elon Musk. The founders and financiers were deeply concerned about AI safety and the existential risk of AI. DeepMind's founders had a personal connection with Yudkowsky and Musk was among those who was actively raising the alarm. Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could \"solve AI, then solve everything else.\"\nIn 2012, Geoffrey Hinton (who been leading neural network research since the 80s) was approached by Baidu, which wanted to hire him and all his students for an enormous sum. Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves to Google for a price of $44 million. Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board. \nLarry Page of Google, unlike Musk and Hassabis, was an optimist about the future of AI. Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party. They had been friends for decades but stopped speaking to each other shortly afterwards. Musk attended the one and only meeting of the DeepMind\u2019s ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI. Frustrated by his lack of influence he founded OpenAI in 2015, enlisting Sam Altman to run it and hiring top scientists. OpenAI began as a non-profit, \u201cfree from the economic incentives that were driving Google and other corporations.\u201d Musk became frustrated again and left the company in 2018. OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing.  \nIn 2021, Dario Amodei  and 14 other scientists left OpenAI over concerns that the company was putting profits above safety. They formed Anthropic, which soon had $6 billion in financing from Microsoft and Google.   \nThe New York Times wrote in 2023 \u201cAt the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth.\"\n\n\n== Large language models, AI boom (2020\u2013present) ==\n\nThe AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity. The new AI era began around 2020\u20132023, with the public release of scaled large language models (LLMs) such as ChatGPT.\n\n\n=== Transformer architecture and large language models ===\n\nIn 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and became widely used in large language models.\nLarge language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022. These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks.\nThese models can discuss a huge number of topics and display general knowledge. The question naturally arises: are these models an example of artificial general intelligence? Bill Gates was skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test. Gates was convinced. In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".\n\n\n=== Neurosymbolic AI ===\n\nDeepMind describes their approach as \"neurosymbolic\" because they use deep learning in combination with symbolic techniques. For example, AlphaZero uses deep learning to evaluate the strength of a position and to suggest policies (courses of action), but it uses Monte Carlo tree search to lookahead at new positions.\n\n\n=== AI boom ===\n\nInvestment in AI grew exponentially in after 2020. \nBy Mid-2024 several financial began to question the capacity of AI companies to produce a return on investment. Some observers speculated that AI was experiencing another bubble.\n\n\n== See also ==\nHistory of artificial neural networks\nHistory of knowledge representation and reasoning\nHistory of natural language processing\nOutline of artificial intelligence\nProgress in artificial intelligence\nTimeline of artificial intelligence\nTimeline of machine learning\n\n\n== Notes ==\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence"}, "Artificial Intelligence Act": {"title": "Artificial Intelligence Act", "content": "The Artificial Intelligence Act (AI Act) is a European Union regulation concerning artificial intelligence (AI). It establishes a common regulatory and legal framework for AI within the European Union (EU). It came into force on 1 August 2024, with provisions that shall come into operation gradually over the following 6 to 36 months.\nIt covers all types of AI across a broad range of sectors, with exceptions for AI systems used solely for military, national security, research and non-professional purposes. As a piece of product regulation, it does not confer rights on individuals, but regulates the providers of AI systems and entities using AI in a professional context.\nThe Act classifies non-exempt AI applications by their risk of causing harm. There are four levels \u2013 unacceptable, high, limited, minimal \u2013 plus an additional category for general-purpose AI. \n\nApplications with unacceptable risks are banned.\nHigh-risk applications must comply with security, transparency and quality obligations, and undergo conformity assessments.\nLimited-risk applications only have transparency obligations.\nMinimal-risk applications are not regulated.\nFor general-purpose AI, transparency requirements are imposed, with reduced requirements for open source models, and additional evaluations for high-capability models.\nThe Act also creates a European Artificial Intelligence Board to promote national cooperation and ensure compliance with the regulation. Like the EU's General Data Protection Regulation, the Act can apply extraterritorially to providers from outside the EU if they have users within the EU.\nProposed by the European Commission on 21 April 2021, it passed the European Parliament on 13 March 2024, and was unanimously approved by the EU Council on 21 May 2024. The draft Act was revised to address the rise in popularity of generative artificial intelligence systems, such as ChatGPT, whose general-purpose capabilities did not fit the main framework.\n\n\n== Provisions ==\n\n\n=== Risk categories ===\nThere are different risk categories depending on the type of application, with a specific category dedicated to general-purpose generative AI:\n\nUnacceptable risk \u2013 AI applications in this category are banned, except for specific exemptions. When no exemption applies, this includes AI applications that manipulate human behaviour, those that use real-time remote biometric identification (such as facial recognition) in public spaces, and those used for social scoring (ranking individuals based on their personal characteristics, socio-economic status, or behaviour).\nHigh-risk \u2013 AI applications that are expected to pose significant threats to health, safety, or the fundamental rights of persons. Notably, AI systems used in health, education, recruitment, critical infrastructure management, law enforcement or justice. They are subject to quality, transparency, human oversight and safety obligations, and in some cases require a \"Fundamental Rights Impact Assessment\" before deployment. They must be evaluated both before they are placed on the market and throughout their life cycle. The list of high-risk applications can be expanded over time, without the need to modify the AI Act itself.\nGeneral-purpose AI \u2013 Added in 2023, this category includes in particular foundation models like ChatGPT. Unless the weights and model architecture are released under free and open source licence, in which case only a training data summary and a copyright compliance policy are required, they are subject to transparency requirements. High-impact general-purpose AI systems including free and open source ones which could pose systemic risks (notably those trained using a computational capability exceeding 1025 FLOPS) must also undergo a thorough evaluation process.\nLimited risk \u2013 AI systems in this category have transparency obligations, ensuring users are informed that they are interacting with an AI system and allowing them to make informed choices. This category includes, for example, AI applications that make it possible to generate or manipulate images, sound, or videos (like deepfakes).\nMinimal risk \u2013 This category includes, for example, AI systems used for video games or spam filters. Most AI applications are expected to fall into this category. These systems are not regulated, and Member States cannot impose additional regulations due to maximum harmonisation rules. Existing national laws regarding the design or use of such systems are overridden. However, a voluntary code of conduct is suggested.\n\n\n=== Exemptions ===\nArticles 2.3 and 2.6 exempt AI systems used for military or national security purposes or pure scientific research and development from the AI Act.\nArticle 5.2 bans algorithmic video surveillance only if it is conducted in real time. Exceptions allowing real-time algorithmic video surveillance include policing aims including \"a real and present or real and foreseeable threat of terrorist attack\".\nRecital 31 of the act states that it aims to prohibit \"AI systems providing social scoring of natural persons by public or private actors\", but allows for \"lawful evaluation practices of natural persons that are carried out for a specific purpose in accordance with Union and national law.\" La Quadrature du Net interprets this exemption as permitting sector-specific social scoring systems, such as the suspicion score used by the French family payments agency Caisse d'allocations familiales.\n\n\n=== Governance ===\nThe AI Act establishes various new bodies in Article 64 and the following articles. These bodies are tasked with implementing and enforcing the Act. The approach combines EU-level coordination with national implementation, involving both public authorities and private sector participation.\nThe following new bodies will be established:\n\nAI Office: attached to the European Commission, this authority will coordinate the implementation of the AI Act in all Member States and oversee the compliance of general-purpose AI providers.\nEuropean Artificial Intelligence Board: composed of one representative from each Member State, the Board will advise and assist the Commission and Member States to facilitate the consistent and effective application of the AI Act. Its tasks include gathering and sharing technical and regulatory expertise, providing recommendations, written opinions, and other advice.\nAdvisory Forum: established to advise and provide technical expertise to the Board and the Commission, this forum will represent a balanced selection of stakeholders, including industry, start-ups, small and medium-sized enterprises, civil society, and academia, ensuring that a broad spectrum of opinions is represented during the implementation and application process.\nScientific Panel of Independent Experts: this panel will provide technical advice and input to the AI Office and national authorities, enforce rules for general-purpose AI models (notably by launching qualified alerts of possible risks to the AI Office), and ensure that the rules and implementations of the AI Act correspond to the latest scientific findings.\nWhile the establishment of new bodies is planned at the EU level, Member States will have to designate \"national competent authorities\". These authorities will be responsible for ensuring the application and implementation of the AI Act, and for conducting \"market surveillance\". They will verify that AI systems comply with the regulations, notably by checking the proper performance of conformity assessments and by appointing third-parties to carry out external conformity assessments.\n\n\n=== Enforcement ===\nThe Act regulates the entry to the EU internal market using the New Legislative Framework. It contains essential requirements that all AI systems must meet to access the EU market. These essential requirements are passed on to European Standardisation Organisations, which develop technical standards that further detail these requirements. These standards are developed by CEN/CENELEC JTC 21.\nThe Act mandates that member states establish their own notifying bodies. Conformity assessments are conducted to verify whether AI systems comply with the standards set out in the AI Act. This assessment can be done in two ways: either through self-assessment, where the AI system provider checks conformity, or through third-party conformity assessment, where the notifying body conducts the assessment. Notifying bodies also have the authority to carry out audits to ensure proper conformity assessments.\nCriticism has arisen regarding the fact that many high-risk AI systems do not require third-party conformity assessments. Some commentators argue that independent third-party assessments are necessary for high-risk AI systems to ensure safety before deployment. Legal scholars have suggested that AI systems capable of generating deepfakes for political misinformation or creating non-consensual intimate imagery should be classified as high-risk and subjected to stricter regulation.\n\n\n== Legislative procedure ==\n\nIn February 2020, the European Commission published \"White Paper on Artificial Intelligence \u2013 A European approach to excellence and trust\". In October 2020, debates between EU leaders took place in the European Council. On 21 April 2021, the AI Act was officially proposed by the Commission. On 6 December 2022, the European Council adopted the general orientation, allowing negotiations to begin with the European Parliament. On 9 December 2023, after three days of \"marathon\" talks, the EU Council and Parliament concluded an agreement.\nThe law was passed in the European Parliament on 13 March 2024, by a vote of 523 for, 46 against, and 49 abstaining. It was approved by the EU Council on 21 May 2024. It entered into force on 1 August 2024, 20 days after being published in the Official Journal on 12 July 2024. After coming into force, there will be a delay before it becomes applicable, which depends on the type of application. This delay is 6 months for bans on \"unacceptable risk\" AI systems, 9 months for codes of practice, 12 months for general-purpose AI systems, 36 months for some obligations related to \"high-risk\" AI systems, and 24 months for everything else.\n\n\n== Reactions ==\nExperts have argued that though the jurisdiction of the law is European, it could have far-ranging implications for international companies that plan to expand to Europe. Anu Bradford at Columbia has argued that the law provides significant momentum to the world-wide movement to regulate AI technologies. \nAmnesty International criticized the AI Act for not completely banning real-time facial recognition, which they said could damage \"human rights, civil space and rule of law\" in the European Union. It also criticized the absence of ban on exporting AI technologies that can harm human rights. \nSome tech watchdogs have argued that there were major loopholes in the law that would allow large tech monopolies to entrench their advantage in AI, or to lobby to weaken rules. Some startups welcomed the clarification the act provides, while others argued the additional regulation would make European startups uncompetitive compared to American and Chinese startups. La Quadrature du Net (LQDN) described the AI Act as \"tailor-made for the tech industry, European police forces as well as other large bureaucracies eager to automate social control\". LQDN described the role of self-regulation and exemptions in the act to render it \"largely incapable of standing in the way of the social, political and environmental damage linked to the proliferation of AI\".\n\n\n== See also ==\nAlgorithmic bias\nEthics of artificial intelligence\nRegulation of algorithms\nRegulation of artificial intelligence in the European Union\nExistential risk from artificial general intelligence\n\n\n== Notes ==\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Artificial_Intelligence_Act"}, "Artificial intelligence art": {"title": "Artificial intelligence art", "content": "Artificial intelligence art is visual artwork created through the use of an artificial intelligence (AI) program.   \nArtists began to create artificial intelligence art in the mid to late 20th century, when the discipline was founded. Throughout its history, artificial intelligence art has raised many philosophical concerns related to the human mind, artificial beings, and what can be considered art in a human\u2013AI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.\nDuring the AI boom of the early 2020s, text-to-image models such as Midjourney, DALL-E, and Stable Diffusion became widely available to the public, allowing non-artists to quickly generate imagery with little effort. Commentary about AI art in the 2020s has often focused on issues related to copyright, deception, defamation, and its impact on more traditional artists, including technological unemployment.\n\n\n== History ==\n\n\n=== Early history ===\n\nThe concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automatons has flourished throughout history, such as Maillardet's automaton, created around 1800 and capable of creating multiple drawings and poems stored in its \"cams,\" the brass disks that hold memory.\nIn 1950, with the publication of Alan Turing's paper Computing Machinery and Intelligence, there was a shift from defining intelligence in regards to machines in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly. Shortly after, the academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity.\n\n\n=== 1950s to 2000s: Early implementations ===\nSince the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. These works were sometimes referred to as algorithmic art, computer art, digital art, or new media.\nOne of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON uses a symbolic rule-based approach to generate technical images in the era of GOFAI programming, and it was developed by Cohen with the goal of being able to code the act of drawing. In its earliest form, AARON created abstract black-and-white drawings which would later be finished by Cohen painting them. Throughout the years, he also began to develop a way for AARON to paint as well, using special brushes and dyes that were chosen by the program itself without mediation from Cohen. After years of work, AARON was exhibited in 1972 at the Los Angeles County Museum of Art. From 1973 to 1975, Cohen refined AARON during a residency at the Artificial Intelligence Laboratory at Stanford University. In 2024, the Whitney Museum of American Art exhibited AI art from throughout Cohen's career, including re-created versions of his early robotic drawing machines.\nKarl Sims has exhibited art created with artificial life since the 1980s. He received an M.S. in computer graphics from the MIT Media Lab in 1987 and was artist-in-residence from 1990 to 1996 at the supercomputer manufacturer and artificial intelligence company Thinking Machines. In both 1991 and 1992, Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution. In 1997, Sims created the interactive installation Gal\u00e1pagos for the NTT InterCommunication Center in Tokyo. In this installation, viewers help evolve 3D animated creatures by selecting which ones will be allowed to live and produce new, mutated offspring. Furthermore, Sims received an Emmy Award in 2019 for outstanding achievement in engineering development.\nEric Millikin has been creating animated films using artificial intelligence since the 1980s, and began posting art on the internet using CompuServe in the early 1980s. \nIn 1999, Scott Draves and a team of several engineers created and released Electric Sheep as a free software screensaver. Electric Sheep is a volunteer computing project for animating and evolving fractal flames, which are in turn distributed to the networked computers, which display them as a screensaver. The screensaver used AI to create an infinite animation by learning from its audience. In 2001, Draves won the Fundacion Telefonica Life 4.0 prize for Electric Sheep.\n\n\n=== 2010s: Deep learning ===\nDuring the deep learning era, there are mainly these types of designs for generative art: autoregressive models, diffusion models, GANs, normalizing flows.\nIn 2014, Ian Goodfellow and colleagues at Universit\u00e9 de Montr\u00e9al developed the generative adversarial network (GAN), a type of deep neural network capable of learning to mimic the statistical distribution of input data such as images. The GAN uses a \"generator\" to create new images and a \"discriminator\" to decide which created images are considered successful. Unlike previous algorithmic art that followed hand-coded rules, generative adversarial networks could learn a specific aesthetic by analyzing a dataset of example images.\nIn 2015, a team at Google released DeepDream, a program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia. The process creates deliberately over-processed images with a dream-like appearance reminiscent of a psychedelic experience.\nIn 2017, a conditional GAN learned to generate 1000 image classes of ImageNet.\nAutoregressive models were used for image generation, such as PixelRNN (2016), which autoregressively generates one pixel after another with a recurrent neural network. Immediately after the Transformer architecture was proposed in Attention Is All You Need (2018), it was used for autoregressive generation of images, but without text conditioning.\nIn 2018, an auction sale of artificial intelligence art was held at Christie's Auction House in New York where the AI artwork Edmond de Belamy (a pun on Goodfellow's name) sold for US$432,500, which was almost 45 times higher than its estimate of US$7,000\u201310,000. The artwork was created by Obvious, a Paris-based collective. The website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN to allow users to generate and modify images such as faces, landscapes, and paintings.\nIn 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the \"interests and culture(s) of people of color.\" Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.\n\n\n=== 2020s ===\n\nIn the 2020s, text-to-image models, which generate images based on prompts, became widely used.\nIn 2021, using the influential large language generative pre-trained transformer models that are used in GPT-2 and GPT-3, OpenAI released a series of images created with the text-to-image AI model DALL-E 1. It was an autoregressive generative model with essentially the same architecture as GPT-3.\nLater in 2021, EleutherAI released the open source VQGAN-CLIP based on OpenAI's CLIP model.\nDiffusion models were proposed in 2015, but they only became better than GANs in early 2021. Latent diffusion model was published in December 2021, and became the basis for the later Stable Diffusion (August 2022).\nIn 2022, Midjourney was released, followed by Google Brain's Imagen and Parti, which were announced in May 2022, Microsoft's NUWA-Infinity, and the source-available Stable Diffusion, which was released in August 2022. DALL-E 2, a successor to DALL-E, was beta-tested and released. Unlike DALL-E 1, it was a diffusion model. Stability AI has a Stable Diffusion web interface called DreamStudio, plugins for Krita, Photoshop, Blender, and GIMP, and the Automatic1111 web-based open source user interface. Stable Diffusion's main pre-trained model is shared on the Hugging Face Hub.\nIn 2023, Eric Millikin released The Dance of the Nain Rouge, a documentary film created using AI deepfake technology about the Detroit folklore legend of the Nain Rouge. The film is described as \"an experimental decolonial Detroit demonology deepfake dream dance documentary.\" It was awarded the \"Best Innovative Technologies Award\" (\"Premio Migliori Tecnologie Innovative\") at the 2024 Pisa Robot Film Festival in Italy and \"Best Animation Film\" at the 2024 Absurd Film Festival in Italy.\n\n\n== Tools and processes ==\n\n\n=== Imagery ===\n\nThere are many tools available to the artist when working with diffusion models. They can define both positive and negative prompts, but they are also afforded a choice in using (or omitting the use of) VAEs, LorAs, hypernetworks, ipadapter, and embeddings/textual inversions. Variables, including CFG, seed, steps, sampler, scheduler, denoise, upscaler, and encoder, are sometimes available for adjustment. Additional influence can be exerted during pre-inference by means of noise manipulation, while traditional post-processing techniques are frequently used post-inference. Artists can also train their own models. \nIn addition, procedural \"rule-based\" generation of images using mathematical patterns, algorithms that simulate brush strokes and other painted effects, and deep learning algorithms such as generative adversarial networks (GANs) and transformers have been developed. Several companies have released apps and websites that allow one to forego all the options mentioned entirely while solely focusing on the positive prompt. There also exist programs which transform photos into art-like images in the style of well-known sets of paintings.\nThere are many options, ranging from simple consumer-facing mobile apps to Jupyter notebooks and webUIs that require powerful GPUs to run effectively. Additional functionalities include \"textual inversion,\" which refers to enabling the use of user-provided concepts (like an object or a style) learned from a few images. Novel art can then be generated from the associated word(s) (the text that has been assigned to the learned, often abstract, concept) and model extensions or fine-tuning (such as DreamBooth).\n\n\n==== Impact and applications ====\nAI has the potential for a societal transformation, which may include enabling the expansion of noncommercial niche genres (such as cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, fast prototyping, increasing art-making accessibility, and artistic output per effort and/or expenses and/or time\u2014e.g., via generating drafts, draft-refinitions, and image components (inpainting). Generated images are sometimes used as sketches, low-cost experiments, inspiration, or illustrations of proof-of-concept-stage ideas. Additional functionalities or improvements may also relate to post-generation manual editing (i.e., polishing), such as subsequent tweaking with an image editor.\n\n\n=== Prompt engineering and sharing ===\n\nPrompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like \"in the style of [name of an artist]\" in the prompt and/or selection of a broad aesthetic/art style. There are platforms for sharing, trading, searching, forking/refining, and/or collaborating on prompts for generating specific imagery from image generators. Prompts are often shared along with images on image-sharing websites such as Reddit and AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image; additional inputs that determine the generated image include the output resolution, random seed, and random sampling parameters.\n\n\n==== Related terminology ====\nSynthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years. Synthography is a proposed term for the practice of generating images that are similar to photographs using AI.\n\n\n== Impact ==\n\n\n=== Copyright ===\n\nLegal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century.\nIn 1985, intellectual property law professor Pamela Samuelson argued that US copyright should allocate algorithmically generated artworks to the user of the computer program. A 2019 Florida Law Review article presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define \"author\" as a natural person or a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the \"work for hire\" doctrine, under which ownership of a copyright is transferred to the \"employer.\" In the third situation, copyright assignments would never take place, and such works would be in the public domain, as copyright assignments require an act of authorship.\nIn 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation. In September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that \"there are no safeguards for artists to be able to identify works in databases that are being used and opt out.\" Some have claimed that images generated with these models can bear resemblance to extant artwork, sometimes including the remains of the original artist's signature. In December 2022, users of the portfolio platform ArtStation staged an online protest against non-consensual use of their artwork within datasets; this resulted in opt-out services, such as \"Have I Been Trained?\" increasing in profile, as well as some online art platforms promising to offer their own opt-out options. According to the US Copyright Office, artificial intelligence programs are unable to hold copyright, a decision upheld at the Federal District level as of August 2023 followed the reasoning from the monkey selfie copyright dispute.\nIn January 2023, three artists\u2014Sarah Andersen, Kelly McKernan, and Karla Ortiz\u2014filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web. In July 2023, U.S. District Judge William Orrick was inclined to dismiss most of the lawsuits filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint. Also in 2023, Stability AI was sued by Getty Images for using its images in the training data. A tool built by Simon Willison allowed people to search 0.5% of the training data for Stable Diffusion V1.1, i.e., 12 million of the 2.3 billion instances from LAION 2B. Artist Karen Hallion discovered that her copyrighted images were used as training data without their consent.\nIn March 2024, Tennessee enacted the ELVIS Act, which prohibits the use of AI to mimic a musician's voice without permission. A month later in that year, Adam Schiff introduced the Generative AI Copyright Disclosure Act which, if passed, would require that AI companies to submit copyrighted works in their datasets to the Register of Copyrights before releasing new generative AI systems.\n\n\n=== Income and employment stability ===\n\nAs generative AI image software such as Stable Diffusion and DALL-E continue to advance, the potential problems and concerns that these systems pose for creativity and artistry have risen. In 2022, artists working in various media raised concerns about the impact that generative artificial intelligence could have on their ability to earn money, particularly if AI-based images started replacing artists working in the illustration and design industries. In August 2022, digital artist R. J. Palmer stated that \"I could easily envision a scenario where using AI, a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don\u2019t have to hire an artist.\" Scholars Jiang et al. state that \"Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently.\"\nAI-based images have become more commonplace in art markets and search engines because AI-based text-to-image systems are trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles. For example, Polish digital artist Greg Rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style. Furthermore, some training databases on which AI systems are based are not accessible to the public.\nThe ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed. Works of AI-generated art, such as Th\u00e9\u00e2tre D'op\u00e9ra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at the Colorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists. The Netflix short film The Dog & the Boy, released in January 2023, received backlash online for its use of artificial intelligence art to create the film's background artwork.\nAI art has sometimes been deemed to be able to replace traditional stock images. In 2023, Shutterstock announced a beta test of an AI tool that can regenerate partial content of other Shutterstock's images. Getty Images and Nvidia have partnered with the launch of Generative AI by iStock, a model trained on Getty\u2019s library and iStock\u2019s photo library using Nvidia\u2019s Picasso model.\n\n\n=== Power usage ===\n\nResearchers from Hugging Face and Carnegie Mellon University reported in a 2023 paper that generating one thousand 1024\u00d71024 images using Stable Diffusion's XL 1.0 base model requires 11.49 kWh of energy and generates 1,594 grams (56.2 oz) of carbon dioxide, which is roughly equivalent to driving an average gas-powered car a distance of 4.1 miles (6.6 km). Comparing 88 different models, the paper concluded that image-generation models used on average around 2.9 kWh of energy per 1,000 inferences.\n\n\n== Reception ==\nA 2022 case study found that AI-produced images created by technology like DALL-E caused some traditional artists to be concerned about losing work, while other artists thought the technology can help them work more efficiently. Some artists use AI art to critique and explore  the ethics of using gathered data to produce new artwork.\n\n\n=== Deception ===\nAs with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading and can be made to damage a person's reputation, such as deepfakes. Artist Sarah Andersen, who previously had her art copied and edited to depict Neo-Nazi beliefs, stated that the spread of hate speech online can be worsened by the use of image generators. Some also generate images or videos for the purpose of catfishing.\nAI systems have the ability to create deepfake content, which is often viewed as harmful and offensive. The creation of deepfakes poses a risk to individuals who have not consented to it. This mainly refers to revenge porn, where sexually explicit material is disseminated to humiliate or harm another person. AI-generated child pornography has been deemed a potential danger to society due to its unlawful nature.\n\nTo mitigate some deceptions, there has been a tool that tries to detect images that were generated by Dall-E.\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\nAfter winning the 2023 \"Creative\" \"Open competition\" Sony World Photography Awards, Boris Eldagsen stated that his entry was actually created with artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had \"clearly shown that even experienced photographers and art experts can be fooled\". Smaller contests have been affected as well; in 2023, a contest run by author Mark Lawrence as Self-Published Fantasy Blog-Off was cancelled after the winning entry was allegedly exposed to be a collage of images generated with Midjourney.\nIn May 2023, on social media sites such as Reddit and Twitter, attention was given to a Midjourney-generated image of Pope Francis wearing a white puffer coat. Additionally, an AI-generated image of an attack on the Pentagon went viral as part of a hoax news story on Twitter.\nIn the days before March 2023 indictment of Donald Trump as part of the Stormy Daniels\u2013Donald Trump scandal, several AI-generated images allegedly depicting Trump's arrest went viral online. On March 20th, British journalist Eliot Higgins generated various images of Donald Trump being arrested or imprisoned using Midjourney v5 and posted them on Twitter; two images of Trump struggling against arresting officers went viral under the mistaken impression that they were genuine, accruing more than 5 million views in three days. According to Higgins, the images were not meant to mislead, but he was banned from using Midjourney services as a result. As of April 2024, the tweet had garnered more than 6.8 million views.\nIn February 2024, the paper Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway was published using AI-generated images. It was later retracted from Frontiers in Cell and Developmental Biology because the paper \"does not meet the standards\".\n\n\n=== Bias ===\nAnother major concern raised about AI-generated images and art is sampling bias within model training data leading towards discriminatory output from AI art models. In 2023, University of Washington researchers found evidence of racial bias within the Stable Diffusion model, with images of a \"person\" corresponding most frequently with images of males from Europe or North America.\nIn 2024, Google's chatbot Gemini's AI image generator was criticized for perceived racial bias, with claims that Gemini deliberately underrepresented white people in its results. Users reported that it generated images of white historical figures like the Founding Fathers, Nazi soldiers, and Vikings as other races, and that it refused to process prompts such as \"happy white people\" and \"ideal nuclear family\". Google later apologized for \"missing the mark\" and took Gemini's image generator offline for updates.\n\n\n== Analysis of existing art using AI ==\nIn addition to the creation of original art, research methods that use AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. According to CETINIC and SHE (2022), using artificial intelligence to analyse already-existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences.\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics. Synthetic images can also be used to train AI algorithms for art authentication and to detect forgeries.\nResearchers have also introduced models that predict emotional responses to art such as ArtEmis, a large-scale dataset with machine learning models that contain emotional reactions to visual art as well as predictions of emotion from images or text.\n\n\n== Other forms of art ==\nSome prototype cooking robots can dynamically taste.\nThere is also AI-assisted writing beyond copy editing (such as helping with writer's block, inspiration, or rewriting segments). Generative AI has been used in video game production beyond imagery, especially for level design (e.g., for custom maps) and creating new content (e.g., quests or dialogue) or interactive stories in video games. Some AI can also generate videos, either from text, an image, or a video. This is known as a text-to-video model. Examples of this are Runway's Gen-2, OpenAI's Sora, and Google's VideoPoet.\n\n\n== See also ==\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Artificial_intelligence_art"}}, "Machine Learning": {"Attention (machine learning)": {"title": "Attention (machine learning)", "content": "Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network language translation system, but the later transformer design removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\n\n\n== History ==\n\nAcademic reviews of the history of the attention mechanism are provided in Niu et al. and Soydaner.\n\n\n=== Predecessors ===\nSelective attention in humans had been well studied in neuroscience and cognitive psychology. In 1953, Colin Cherry studied selective attention in the context of audition, known as the cocktail party effect.\nIn 1958, Donald Broadbent proposed the filter model of attention. Selective attention of vision was studied in the 1960s by George Sperling's partial report paradigm. It was also noticed that saccade control is modulated by cognitive processes, insofar as the eye moves preferentially towards areas of high salience. As the fovea of the eye is small, the eye cannot sharply resolve the entire visual field at once. The use of saccade control allows the eye to quickly scan important features of a scene.\nThese research developments inspired algorithms such as the Neocognitron and its variants. Meanwhile, developments in neural networks had inspired circuit models of biological visual attention. One well-cited network from 1998, for example, was inspired by the low-level primate visual system. It produced saliency maps of images using handcrafted (not learned) features, which were then used to guide a second neural network in processing patches of the image in order of reducing saliency.\nA key aspect of attention mechanism can be written (schematically) as \n  \n    \n      \n        \n          \u2211\n          \n            i\n          \n        \n        \u27e8\n        (\n        \n          query\n        \n        \n          )\n          \n            i\n          \n        \n        ,\n        (\n        \n          key\n        \n        \n          )\n          \n            i\n          \n        \n        \u27e9\n        (\n        \n          value\n        \n        \n          )\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\sum _{i}\\langle ({\\text{query}})_{i},({\\text{key}})_{i}\\rangle ({\\text{value}})_{i}}\n  \nwhere the angled brackets denote dot product. This shows that it involves a multiplicative operation. Multiplicative operations within artificial neural networks had been studied under the names of Group Method of Data Handling (1965) (where Kolmogorov-Gabor polynomials implement multiplicative units or \"gates\"), higher-order neural networks, multiplication units, sigma-pi units, fast weight controllers, and hyper-networks.\nIn fast weight controller (Schmidhuber, 1992), one of its two networks has  \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer. A follow-up paper developed a similar system with active weight changing.\n\n\n=== Recurrent attention ===\nDuring the deep learning era, attention mechanism was developed to solve similar problems in encoding-decoding.\nIn machine translation, the seq2seq model, as it was proposed in 2014, would encode an input text into a fixed-length vector, which would then be decoded into an output text. If the input text is long, the fixed-length vector would be unable to carry enough information for accurate decoding. An attention mechanism was proposed to solve this problem.\nAn image captioning model was proposed in 2015, citing inspiration from the seq2seq model. that would encode an input image into a fixed-length vector. (Xu et al 2015), citing (Bahdanau et al 2014), applied the attention mechanism as used in the seq2seq model to image captioning.\n\n\n=== Transformer ===\n\nOne problem with seq2seq models was their use of recurrent neural networks, which are not parallelizable as both the encoder and the decoder must process the sequence token-by-token. Decomposable attention attempted to solve this problem by processing the input sequence in parallel, before computing a \"soft alignment matrix\" (alignment is the terminology used by Bahdanau et al) in order to allow for parallel processing.\nThe idea of using the attention mechanism for self-attention, instead of in an encoder-decoder (cross-attention), was also proposed during this period, such as in differentiable neural computers and neural Turing machines. It was termed intra-attention where an LSTM is augmented with a memory network as it encodes an input sequence.\nThese strands of development were brought together in 2017 with the Transformer architecture, published in the Attention Is All You Need paper.\n\n\n== seq2seq ==\n\nThe seq2seq method developed in the early 2010s uses two neural networks:  an encoder network converts an input sentence into numerical vectors, and a decoder network converts those vectors to sentences in the target language.  The Attention mechanism was grafted onto this structure in 2014, and later refined into the Transformer design.\n\n\n=== Problem statement ===\n\nConsider the seq2seq language English-to-French translation task. To be concrete, let us consider the translation of \"the zone of international control <end>\", which should translate to \"la zone de contr\u00f4le international <end>\". Here, we use the special <end> token as a control character to delimit the end of input for both the encoder and the decoder.\nAn input sequence of text \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle x_{0},x_{1},\\dots }\n  \n is processed by a neural network (which can be an LSTM, a Transformer encoder, or some other network) into a sequence of real-valued vectors \n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n, where \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n stands for \"hidden vector\".\nAfter the encoder has finished processing, the decoder starts operating over the hidden vectors, to produce an output sequence \n  \n    \n      \n        \n          y\n          \n            0\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle y_{0},y_{1},\\dots }\n  \n, autoregressively. That is, it always takes as input both the hidden vectors produced by the encoder, and what the decoder itself has produced before, to produce the next output word:\n\n(\n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n, \"<start>\") \u2192 \"la\"\n(\n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n, \"<start> la\") \u2192 \"la zone\"\n(\n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n, \"<start> la zone\") \u2192 \"la zone de\"\n...\n(\n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n, \"<start> la zone de contr\u00f4le international\") \u2192 \"la zone de contr\u00f4le international <end>\"\nHere, we use the special <start> token as a control character to delimit the start of input for the decoder. The de\ncoding terminates as soon as \"<end>\" appears in the decoder output.\n\n\n=== Word alignment ===\nIn translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. In the I love you example above, the second word love is aligned with the third word aime. Stacking soft row vectors together for je, t', and aime yields an alignment matrix:\n\nSometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le. Thus, \"soft\" attention weights work better than \"hard\" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than \"the best one\", as there may not be a best hidden vector.\nThis view of the attention weights addresses some of the neural network explainability problem. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced. On the first pass through the decoder, 94% of the attention weight is on the first English word I, so the network offers the word je. On the second pass of the decoder, 88% of the attention weight is on the third English word you, so it offers t'. On the last pass, 95% of the attention weight is on the second English word love, so it offers aime.\n\n\n=== Attention weights ===\nAs hand-crafting weights defeats the purpose of machine learning, the model must compute the attention weights on its own. Taking analogy from the language of database queries, we make the model construct a triple of vectors: key, query, and value. The rough idea is that we have a \"database\" in the form of a list of key-value pairs. The decoder send in a query, and obtain a reply in the form of a weighted sum of the values, where the weight is proportional to how closely the query resembles each key.\nThe decoder first processes the \"<start>\" input partially, to obtain an intermediate vector \n  \n    \n      \n        \n          h\n          \n            0\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle h_{0}^{d}}\n  \n, the 0th hidden vector of decoder. Then, the intermediate vector is transformed by a linear map \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n into a query vector \n  \n    \n      \n        \n          q\n          \n            0\n          \n        \n        =\n        \n          h\n          \n            0\n          \n          \n            d\n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle q_{0}=h_{0}^{d}W^{Q}}\n  \n. Meanwhile, the hidden vectors outputted by the encoder are transformed by another linear map \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n into key vectors \n  \n    \n      \n        \n          k\n          \n            0\n          \n        \n        =\n        \n          h\n          \n            0\n          \n        \n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          k\n          \n            1\n          \n        \n        =\n        \n          h\n          \n            1\n          \n        \n        \n          W\n          \n            K\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle k_{0}=h_{0}W^{K},k_{1}=h_{1}W^{K},\\dots }\n  \n. The linear maps are useful for providing the model with enough freedom to find the best way to represent the data.\nNow, the query and keys are compared by taking dot products: \n  \n    \n      \n        \n          q\n          \n            0\n          \n        \n        \n          k\n          \n            0\n          \n          \n            T\n          \n        \n        ,\n        \n          q\n          \n            0\n          \n        \n        \n          k\n          \n            1\n          \n          \n            T\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle q_{0}k_{0}^{T},q_{0}k_{1}^{T},\\dots }\n  \n. Ideally, the model should have learned to compute the keys and values, such that \n  \n    \n      \n        \n          q\n          \n            0\n          \n        \n        \n          k\n          \n            0\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle q_{0}k_{0}^{T}}\n  \n is large, \n  \n    \n      \n        \n          q\n          \n            0\n          \n        \n        \n          k\n          \n            1\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle q_{0}k_{1}^{T}}\n  \n is small, and the rest are very small. This can be interpreted as saying that the attention weight should be mostly applied to the 0th hidden vector of the encoder, a little to the 1st, and essentially none to the rest.\nIn order to make a properly weighted sum, we need to transform this list of dot products into a probability distribution over \n  \n    \n      \n        0\n        ,\n        1\n        ,\n        \u2026\n      \n    \n    {\\displaystyle 0,1,\\dots }\n  \n. This can be accomplished by the softmax function, thus giving us the attention weights:\n  \n    \n      \n        (\n        \n          w\n          \n            00\n          \n        \n        ,\n        \n          w\n          \n            01\n          \n        \n        ,\n        \u2026\n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        \n          q\n          \n            0\n          \n        \n        \n          k\n          \n            0\n          \n          \n            T\n          \n        \n        ,\n        \n          q\n          \n            0\n          \n        \n        \n          k\n          \n            1\n          \n          \n            T\n          \n        \n        ,\n        \u2026\n        )\n      \n    \n    {\\displaystyle (w_{00},w_{01},\\dots )=\\mathrm {softmax} (q_{0}k_{0}^{T},q_{0}k_{1}^{T},\\dots )}\n  \nThis is then used to compute the context vector:\n  \n    \n      \n        \n          c\n          \n            0\n          \n        \n        =\n        \n          w\n          \n            00\n          \n        \n        \n          v\n          \n            0\n          \n        \n        +\n        \n          w\n          \n            01\n          \n        \n        \n          v\n          \n            1\n          \n        \n        +\n        \u22ef\n      \n    \n    {\\displaystyle c_{0}=w_{00}v_{0}+w_{01}v_{1}+\\cdots }\n  \n\nwhere \n  \n    \n      \n        \n          v\n          \n            0\n          \n        \n        =\n        \n          h\n          \n            0\n          \n        \n        \n          W\n          \n            V\n          \n        \n        ,\n        \n          v\n          \n            1\n          \n        \n        =\n        \n          h\n          \n            1\n          \n        \n        \n          W\n          \n            V\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle v_{0}=h_{0}W^{V},v_{1}=h_{1}W^{V},\\dots }\n  \n are the value vectors, linearly transformed by another matrix to provide the model with freedom to find the best way to represent values. Without the matrices \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{Q},W^{K},W^{V}}\n  \n, the model would be forced to use the same hidden vector for both key and value, which might not be appropriate, as these two tasks are not the same.This is the dot-attention mechanism. The particular version described in this section is \"decoder cross-attention\", as the output context vector is used by the decoder, and the input keys and values come from the encoder, but the query comes from the decoder, thus \"cross-attention\".\nMore succinctly, we can write it as\n  \n    \n      \n        \n          c\n          \n            0\n          \n        \n        =\n        \n          A\n          t\n          t\n          e\n          n\n          t\n          i\n          o\n          n\n        \n        (\n        \n          h\n          \n            0\n          \n          \n            d\n          \n        \n        \n          W\n          \n            Q\n          \n        \n        ,\n        H\n        \n          W\n          \n            K\n          \n        \n        ,\n        H\n        \n          W\n          \n            V\n          \n        \n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        (\n        \n          h\n          \n            0\n          \n          \n            d\n          \n        \n        \n          W\n          \n            Q\n          \n        \n        )\n        \n        (\n        H\n        \n          W\n          \n            K\n          \n        \n        \n          )\n          \n            T\n          \n        \n        )\n        (\n        H\n        \n          W\n          \n            V\n          \n        \n        )\n      \n    \n    {\\displaystyle c_{0}=\\mathrm {Attention} (h_{0}^{d}W^{Q},HW^{K},HW^{V})=\\mathrm {softmax} ((h_{0}^{d}W^{Q})\\;(HW^{K})^{T})(HW^{V})}\n  \nwhere the matrix \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n is the matrix whose rows are \n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n. Note that the querying vector, \n  \n    \n      \n        \n          h\n          \n            0\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle h_{0}^{d}}\n  \n, is not necessarily the same as the key-value vector \n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle h_{0}}\n  \n. In fact, it is theoretically possible for query, key, and value vectors to all be different, though that is rarely done in practice.\n\n\n== Overview ==\n\n\n== Terminology ==\nThis attention scheme has been compared to the Query-Key analogy of relational databases.  That comparison suggests an asymmetric role for the Query and Key vectors, where one item of interest (the Query vector \"that\") is matched against all possible items (the Key vectors of each word in the sentence).   However, Attention's parallel calculations matches all words of a sentence with itself; therefore the roles of these vectors are symmetric.  Possibly because the simplistic database analogy is flawed, much effort has gone into understand Attention further by studying their roles in focused settings, such as in-context learning, masked language tasks, stripped down transformers, bigram statistics, N-gram statistics, pairwise convolutions, and arithmetic factoring.\n\n\n== Variants ==\nMany variants of attention implement soft weights, such as\n\nfast weight programmers, or fast weight controllers (1992). A \"slow\" neural network outputs the \"fast\" weights of another neural network through outer products. The slow network learns by gradient descent. It was later renamed as \"linearized self-attention\".\nBahdanau-style attention, also referred to as additive attention,\nLuong-style attention, which is known as multiplicative attention,\nhighly parallelizable self-attention introduced in 2016 as decomposable attention and successfully used in transformers a year later,\npositional attention and factorized positional attention.\nFor convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations.\nMuch effort has gone into understand Attention further by studying their roles in focused settings, such as in-context learning, masked language tasks, stripped down transformers, bigram statistics, N-gram statistics, pairwise convolutions, and arithmetic factoring.\nThese variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients.  In the figures below, W is the matrix of context attention weights, similar to the formula in Core Calculations section above.\n\n\n=== Self-attention ===\n\nSelf-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences.\nFor encoder self-attention, we can start with a simple encoder without self-attention, such as an \"embedding layer\", which simply converts each input word into a vector by a fixed lookup table. This gives a sequence of hidden vectors \n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n. These can then be applied to a dot-product attention mechanism, to obtain\n  \n    \n      \n        \n          \n            \n              \n                \n                  h\n                  \n                    0\n                  \n                  \u2032\n                \n              \n              \n                \n                =\n                \n                  A\n                  t\n                  t\n                  e\n                  n\n                  t\n                  i\n                  o\n                  n\n                \n                (\n                \n                  h\n                  \n                    0\n                  \n                \n                \n                  W\n                  \n                    Q\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    K\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    V\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  h\n                  \n                    1\n                  \n                  \u2032\n                \n              \n              \n                \n                =\n                \n                  A\n                  t\n                  t\n                  e\n                  n\n                  t\n                  i\n                  o\n                  n\n                \n                (\n                \n                  h\n                  \n                    1\n                  \n                \n                \n                  W\n                  \n                    Q\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    K\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    V\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                \u22ef\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}h_{0}'&=\\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\\\h_{1}'&=\\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\\\&\\cdots \\end{aligned}}}\n  \nor more succinctly, \n  \n    \n      \n        \n          H\n          \u2032\n        \n        =\n        \n          A\n          t\n          t\n          e\n          n\n          t\n          i\n          o\n          n\n        \n        (\n        H\n        \n          W\n          \n            Q\n          \n        \n        ,\n        H\n        \n          W\n          \n            K\n          \n        \n        ,\n        H\n        \n          W\n          \n            V\n          \n        \n        )\n      \n    \n    {\\displaystyle H'=\\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})}\n  \n. This can be applied repeatedly, to obtain a multilayered encoder. This is the \"encoder self-attention\", sometimes called the \"all-to-all attention\", as the vector at every position can attend to every other.\n\n\n=== Masking ===\nFor decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle w_{ij}=0}\n  \n for all \n  \n    \n      \n        i\n        <\n        j\n      \n    \n    {\\displaystyle i<j}\n  \n, called \"causal masking\". This attention mechanism is the \"causally masked self-attention\".\n\n\n== Mathematical representation ==\n\n\n==== Standard Scaled Dot-Product Attention ====\nFor matrices: \n  \n    \n      \n        \n          Q\n        \n        \u2208\n        \n          \n            R\n            \n              m\n              \u00d7\n              \n                d\n                \n                  k\n                \n              \n            \n          \n        \n        ,\n        \n          K\n        \n        \u2208\n        \n          \n            R\n            \n              n\n              \u00d7\n              \n                d\n                \n                  k\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {Q} \\in \\mathbb {R^{m\\times d_{k}}} ,\\mathbf {K} \\in \\mathbb {R^{n\\times d_{k}}} }\n  \n and \n  \n    \n      \n        \n          V\n        \n        \u2208\n        \n          \n            R\n            \n              n\n              \u00d7\n              \n                d\n                \n                  v\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {V} \\in \\mathbb {R^{n\\times d_{v}}} }\n  \n, the scaled dot-product, or QKV attention is defined as:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        \n          Q\n        \n        ,\n        \n          K\n        \n        ,\n        \n          V\n        \n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                \n                  Q\n                \n                \n                  \n                    K\n                  \n                  \n                    T\n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        \n          V\n        \n        \u2208\n        \n          \n            R\n          \n          \n            m\n            \u00d7\n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(\\mathbf {Q} ,\\mathbf {K} ,\\mathbf {V} )={\\text{softmax}}\\left({\\frac {\\mathbf {Q} \\mathbf {K} ^{T}}{\\sqrt {d_{k}}}}\\right)\\mathbf {V} \\in \\mathbb {R} ^{m\\times d_{v}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {}^{T}}\n  \n denotes transpose and the softmax function is applied independently to every row of its argument. The matrix \n  \n    \n      \n        \n          Q\n        \n      \n    \n    {\\displaystyle \\mathbf {Q} }\n  \n contains \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n queries, while matrices \n  \n    \n      \n        \n          K\n        \n        ,\n        \n          V\n        \n      \n    \n    {\\displaystyle \\mathbf {K} ,\\mathbf {V} }\n  \n jointly contain an unordered set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n key-value pairs. Value vectors in matrix \n  \n    \n      \n        \n          V\n        \n      \n    \n    {\\displaystyle \\mathbf {V} }\n  \n are weighted using the weights resulting from the softmax operation, so that the rows of the \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n-by-\n  \n    \n      \n        \n          d\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle d_{v}}\n  \n output matrix are confined to the convex hull of the points in \n  \n    \n      \n        \n          \n            R\n          \n          \n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{d_{v}}}\n  \n given by the rows of \n  \n    \n      \n        \n          V\n        \n      \n    \n    {\\displaystyle \\mathbf {V} }\n  \n.\nTo understand the permutation invariance and permutation equivariance properties of QKV attention, let \n  \n    \n      \n        \n          A\n        \n        \u2208\n        \n          \n            R\n          \n          \n            m\n            \u00d7\n            m\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {A} \\in \\mathbb {R} ^{m\\times m}}\n  \n and \n  \n    \n      \n        \n          B\n        \n        \u2208\n        \n          \n            R\n          \n          \n            n\n            \u00d7\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {B} \\in \\mathbb {R} ^{n\\times n}}\n  \n be permutation matrices; and \n  \n    \n      \n        \n          D\n        \n        \u2208\n        \n          \n            R\n          \n          \n            m\n            \u00d7\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {D} \\in \\mathbb {R} ^{m\\times n}}\n  \n an arbitrary matrix. The softmax function is permutation equivariant in the sense that:\n\n  \n    \n      \n        \n          softmax\n        \n        (\n        \n          A\n        \n        \n          D\n        \n        \n          B\n        \n        )\n        =\n        \n          A\n        \n        \n        \n          softmax\n        \n        (\n        \n          D\n        \n        )\n        \n          B\n        \n      \n    \n    {\\displaystyle {\\text{softmax}}(\\mathbf {A} \\mathbf {D} \\mathbf {B} )=\\mathbf {A} \\,{\\text{softmax}}(\\mathbf {D} )\\mathbf {B} }\n  \n\nBy noting that the transpose of a permutation matrix is also its inverse, it follows that:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        \n          A\n        \n        \n          Q\n        \n        ,\n        \n          B\n        \n        \n          K\n        \n        ,\n        \n          B\n        \n        \n          V\n        \n        )\n        =\n        \n          A\n        \n        \n        \n          Attention\n        \n        (\n        \n          Q\n        \n        ,\n        \n          K\n        \n        ,\n        \n          V\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(\\mathbf {A} \\mathbf {Q} ,\\mathbf {B} \\mathbf {K} ,\\mathbf {B} \\mathbf {V} )=\\mathbf {A} \\,{\\text{Attention}}(\\mathbf {Q} ,\\mathbf {K} ,\\mathbf {V} )}\n  \n\nwhich shows that QKV attention is equivariant with respect to re-ordering the queries (rows of \n  \n    \n      \n        \n          Q\n        \n      \n    \n    {\\displaystyle \\mathbf {Q} }\n  \n); and invariant to re-ordering of the key-value pairs in \n  \n    \n      \n        \n          K\n        \n        ,\n        \n          V\n        \n      \n    \n    {\\displaystyle \\mathbf {K} ,\\mathbf {V} }\n  \n. These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks. For example, a simple self-attention function defined as:\n\n  \n    \n      \n        \n          X\n        \n        \u21a6\n        \n          Attention\n        \n        (\n        \n          X\n        \n        \n          \n            T\n          \n          \n            q\n          \n        \n        ,\n        \n          X\n        \n        \n          \n            T\n          \n          \n            k\n          \n        \n        ,\n        \n          X\n        \n        \n          \n            T\n          \n          \n            v\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {X} \\mapsto {\\text{Attention}}(\\mathbf {X} \\mathbf {T} _{q},\\mathbf {X} \\mathbf {T} _{k},\\mathbf {X} \\mathbf {T} _{v})}\n  \n\nis permutation equivariant with respect to re-ordering the rows of the input matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n in a non-trivial way, because every row of the output is a function of all the rows of the input. Similar properties hold for multi-head attention, which is defined below.\n\n\n==== Masked Attention ====\nWhen QKV attention is used as a building block for an autoregressive decoder, and when at training time all input and output matrices have \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n rows, a masked attention variant is used:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        \n          Q\n        \n        ,\n        \n          K\n        \n        ,\n        \n          V\n        \n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                \n                  \n                    Q\n                  \n                  \n                    \n                      K\n                    \n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    d\n                    \n                      k\n                    \n                  \n                \n              \n            \n            +\n            \n              M\n            \n          \n          )\n        \n        \n          V\n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(\\mathbf {Q} ,\\mathbf {K} ,\\mathbf {V} )={\\text{softmax}}\\left({\\frac {\\mathbf {Q} \\mathbf {K} ^{T}}{\\sqrt {d_{k}}}}+\\mathbf {M} \\right)\\mathbf {V} }\n  \n\nwhere the mask, \n  \n    \n      \n        \n          M\n        \n        \u2208\n        \n          \n            R\n          \n          \n            n\n            \u00d7\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {M} \\in \\mathbb {R} ^{n\\times n}}\n  \n is a stricly upper triangular matrix, with zeros on and below the diagonal and \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n  \n in every element above the diagonal. The softmax output, also in \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n            \u00d7\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n\\times n}}\n  \n is then lower triangular, with zeros in all elements above the diagonal. The masking ensures that for all \n  \n    \n      \n        1\n        \u2264\n        i\n        <\n        j\n        \u2264\n        n\n      \n    \n    {\\displaystyle 1\\leq i<j\\leq n}\n  \n, row \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n of the attention ouput is independent of row \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n of any of the three input matrices. The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant.\n\n\n==== Multi-Head Attention ====\n\nMulti-head attention\n\n  \n    \n      \n        \n          MultiHead\n        \n        (\n        \n          Q\n        \n        ,\n        \n          K\n        \n        ,\n        \n          V\n        \n        )\n        =\n        \n          Concat\n        \n        (\n        \n          \n            head\n          \n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            head\n          \n          \n            h\n          \n        \n        )\n        \n          \n            W\n          \n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiHead}}(\\mathbf {Q} ,\\mathbf {K} ,\\mathbf {V} )={\\text{Concat}}({\\text{head}}_{1},...,{\\text{head}}_{h})\\mathbf {W} ^{O}}\n  \n\nwhere each head is computed with QKV attention as:\n\n  \n    \n      \n        \n          \n            head\n          \n          \n            i\n          \n        \n        =\n        \n          Attention\n        \n        (\n        \n          Q\n        \n        \n          \n            W\n          \n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          K\n        \n        \n          \n            W\n          \n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          V\n        \n        \n          \n            W\n          \n          \n            i\n          \n          \n            V\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{head}}_{i}={\\text{Attention}}(\\mathbf {Q} \\mathbf {W} _{i}^{Q},\\mathbf {K} \\mathbf {W} _{i}^{K},\\mathbf {V} \\mathbf {W} _{i}^{V})}\n  \n\nand \n  \n    \n      \n        \n          \n            W\n          \n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          \n            W\n          \n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          \n            W\n          \n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {W} _{i}^{Q},\\mathbf {W} _{i}^{K},\\mathbf {W} _{i}^{V}}\n  \n, and \n  \n    \n      \n        \n          \n            W\n          \n          \n            O\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {W} ^{O}}\n  \n are parameter matrices.\nThe permutation properties of (standard, unmasked) QKV attention apply here also. For permutation matrices, \n  \n    \n      \n        \n          A\n        \n        ,\n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {A} ,\\mathbf {B} }\n  \n:\n\n  \n    \n      \n        \n          MultiHead\n        \n        (\n        \n          A\n        \n        \n          Q\n        \n        ,\n        \n          B\n        \n        \n          K\n        \n        ,\n        \n          B\n        \n        \n          V\n        \n        )\n        =\n        \n          A\n        \n        \n        \n          MultiHead\n        \n        (\n        \n          Q\n        \n        ,\n        \n          K\n        \n        ,\n        \n          V\n        \n        )\n      \n    \n    {\\displaystyle {\\text{MultiHead}}(\\mathbf {A} \\mathbf {Q} ,\\mathbf {B} \\mathbf {K} ,\\mathbf {B} \\mathbf {V} )=\\mathbf {A} \\,{\\text{MultiHead}}(\\mathbf {Q} ,\\mathbf {K} ,\\mathbf {V} )}\n  \n\nfrom which we also see that multi-head self-attention:\n\n  \n    \n      \n        \n          X\n        \n        \u21a6\n        \n          MultiHead\n        \n        (\n        \n          X\n        \n        \n          \n            T\n          \n          \n            q\n          \n        \n        ,\n        \n          X\n        \n        \n          \n            T\n          \n          \n            k\n          \n        \n        ,\n        \n          X\n        \n        \n          \n            T\n          \n          \n            v\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {X} \\mapsto {\\text{MultiHead}}(\\mathbf {X} \\mathbf {T} _{q},\\mathbf {X} \\mathbf {T} _{k},\\mathbf {X} \\mathbf {T} _{v})}\n  \n\nis equivariant with respect to re-ordering of the rows of input matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n.\n\n\n==== Bahdanau (Additive) Attention ====\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        (\n        e\n        )\n        V\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}(e)V}\n  \n\nwhere \n  \n    \n      \n        e\n        =\n        tanh\n        \u2061\n        (\n        \n          W\n          \n            Q\n          \n        \n        Q\n        +\n        \n          W\n          \n            K\n          \n        \n        K\n        )\n      \n    \n    {\\displaystyle e=\\tanh(W_{Q}Q+W_{K}K)}\n  \n and \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W_{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W_{K}}\n  \n are learnable weight matrices.\n\n\n==== Luong Attention (General) ====\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        (\n        Q\n        \n          W\n          \n            a\n          \n        \n        \n          K\n          \n            T\n          \n        \n        )\n        V\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}(QW_{a}K^{T})V}\n  \n\nwhere \n  \n    \n      \n        \n          W\n          \n            a\n          \n        \n      \n    \n    {\\displaystyle W_{a}}\n  \n is a learnable weight matrix.\n\n\n== See also ==\nRecurrent neural network\nseq2seq\nTransformer (deep learning architecture)\nAttention\nDynamic neural network\n\n\n== References ==\n\n\n== External links ==\nOlah, Chris; Carter, Shan (September 8, 2016). \"Attention and Augmented Recurrent Neural Networks\". Distill. 1 (9). Distill Working Group. doi:10.23915/distill.00001.\nDan Jurafsky and James H. Martin (2022) Speech and Language Processing (3rd ed. draft, January 2022), ch. 10.4 Attention and ch. 9.7 Self-Attention Networks: Transformers\nAlex Graves (4 May 2020), Attention and Memory in Deep Learning (video lecture), DeepMind / UCL, via YouTube", "link": "https://en.wikipedia.org/wiki/Attention_(machine_learning)"}, "Quantum machine learning": {"title": "Quantum machine learning", "content": "Quantum machine learning is the integration of quantum algorithms within machine learning programs.\nThe most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.\nBeyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.\nQuantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.\nFurthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\n\n\n== Machine learning with quantum computers ==\nQuantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\n\n\n=== Quantum associative memories and quantum pattern recognition ===\nAssociative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.\nTypical classical associative memories store p patterns in the \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n  \n interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.\nUnfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, \n  \n    \n      \n        p\n        \u2264\n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle p\\leq O(n)}\n  \n.\nQuantum associative memories (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is \n  \n    \n      \n        O\n        (\n        p\n        n\n        )\n      \n    \n    {\\displaystyle O(pn)}\n  \n. One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns.\n\n\n=== Linear algebra simulation with quantum amplitudes ===\nA number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations. Since a state of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n qubits is described by \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n, which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input.\nMany quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank. For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g. \n  \n    \n      \n        O\n        \n          \n            \n              (\n              \n                n\n                \n                  2.373\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle O{\\mathord {\\left(n^{2.373}\\right)}}}\n  \n), but they are not restricted to sparse matrices.\nQuantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes.\nA crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task.\n\n\n=== Variational quantum algorithms (VQAs) ===\nVQAs are one of the most studied quantum algorithms as researchers expect that all the needed applications for the quantum computer will be using the VQAs and also VQAs seem to fulfill the expectation for gaining quantum supremacy.  VQAs is a mixed quantum-classical approach where the quantum processor prepares quantum states and measurement is made and the optimization is done by a classical computer. VQAs are considered best for NISQ as VQAs are noise tolerant compared to other algorithms and give quantum superiority with only a few hundred qubits. Researchers have studied circuit-based algorithms to solve optimization problems and find the ground state energy of complex systems, which were difficult to solve or required a large time to perform the computation using a classical computer.\n\n\n=== Variational quantum circuits (VQCs) ===\nVariational Quantum Circuits also known as Parametrized Quantum Circuits (PQCs) are based on Variational Quantum Algorithms (VQAs). VQCs consist of three parts: preparation of initial states, quantum circuit, and measurement. Researchers are extensively studying VQCs, as it uses the power of quantum computation to learn in a short time and also use fewer parameters than its classical counterparts. It is theoretically and numerically proven that we can approximate non-linear functions, like those used in neural networks, on quantum circuits. Due to VQCs superiority, neural network has been replaced by VQCs in Reinforcement Learning tasks and Generative Algorithms. The intrinsic nature of quantum devices towards decoherence, random gate error and measurement errors caused to have high potential to limit the training of the variation circuits. Training the VQCs on the classical devices before employing them on quantum devices helps to overcome the problem of decoherence noise that came through the number of repetitions for training.\n\n\n=== Quantum binary classifier ===\nPattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns. Binary classification is used in supervised learning and in unsupervised learning. In quantum machine learning, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space. By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time.\n\n\n=== Quantum machine learning algorithms based on Grover search ===\nAnother approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms. Other applications include quadratic speedups in the training of perceptron and the computation of attention.\nAn example of amplitude amplification being used in a machine learning algorithm is Grover's search algorithm minimization. In which a subroutine uses Grover's search algorithm to find an element less than some previously defined element. This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one. Grover's algorithm can then find an element such that our condition is met. The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set. This minimization is notably used in quantum k-medians, and it has a speed up of at least \n  \n    \n      \n        O\n        (\n        \n          \n            n\n            \n              /\n            \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {n/k}})}\n  \n compared to classical versions of k-medians, where \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the number of data points and \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n is the number of clusters.\nAmplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework.\n\n\n=== Quantum-enhanced reinforcement learning ===\nReinforcement learning is a branch of machine learning distinct from supervised and unsupervised learning, which also admits quantum enhancements. In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions, which allows the agent to adapt its behavior\u2014in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols have been proposed for systems of trapped ions and superconducting circuits. A quantum speedup of the agent's internal decision-making time has been experimentally demonstrated in trapped ions, while a quantum speedup of the learning time in a fully coherent (`quantum') interaction between agent and environment has been experimentally realized in a photonic setup.\n\n\n=== Quantum annealing ===\n\nQuantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent Schr\u00f6dinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system.\n\n\n=== NISQ Circuit as Quantum Model ===\nAs the depth of the quantum circuit advances on NISQ devices, the noise level rises, posing a significant challenge to accurately computing costs and gradients on training models. The noise tolerance will be improved by using the quantum perceptron and the quantum algorithm on the currently accessible quantum hardware.\nA regular connection of similar components known as neurons forms the basis of even the most complex brain networks. Typically, a neuron has two operations: the inner product and an activation function. As opposed to the activation function, which is typically nonlinear, the inner product is a linear process. With quantum computing, linear processes may be easily accomplished additionally,  due to the simplicity of implementation, the threshold function is preferred by the majority of quantum neurons for activation functions.\n\n\n=== Quantum sampling techniques ===\nSampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.\nA computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.\nSome research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks. The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.\nThe D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward. Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine.\nInspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.\nQuantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.\n\n\n=== Quantum neural networks ===\n\nQuantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models. Quantum neural networks are often defined as an expansion on Deutsch's model of a quantum computational network. Within this model, nonlinear and irreversible gates, dissimilar to the Hamiltonian operator, are deployed to speculate the given data set. Such gates make certain phases unable to be observed and generate specific oscillations. Quantum neural networks apply the principals quantum information and quantum computation to classical neurocomputing. Current research shows that QNN can exponentially increase the amount of computing power and the degrees of freedom for a computer, which is limited for a classical computer to its size. A quantum neural network has computational capabilities to decrease the number of steps, qubits used, and computation time. The wave function to quantum mechanics is the neuron for Neural networks. To test quantum applications in a neural network, quantum dot molecules are deposited on a substrate of GaAs or similar to record how they communicate with one another. Each quantum dot can be referred as an island of electric activity, and when such dots are close enough (approximately 10 - 20 nm) electrons can tunnel underneath the islands. An even distribution across the substrate in sets of two create dipoles and ultimately two spin states, up or down. These states are commonly known as qubits with corresponding states of \n  \n    \n      \n        \n          |\n        \n        0\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle }\n  \n  and \n  \n    \n      \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |1\\rangle }\n  \n in Dirac notation.\n\n\n=== Quantum Convolution Neural Network ===\nA novel design for multi-dimensional vectors that uses circuits as convolution filters is QCNN. It was inspired by the advantages of CNNs and the power of QML. It is made using a combination of a variational quantum circuit(VQC) and a deep neural network(DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits. The main strategy is to carry out an iterative optimization process in the NISQ devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction.\nThe quantum circuit must effectively handle spatial information in order for QCNN to function as CNN. The convolution filter is the most basic technique for making use of spatial information. One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way. Three parts  that make up the quantum convolutional filter are:  the encoder, the parameterized quantum circuit (PQC), and the measurement. The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters.\nQuantum neural networks take advantage of the hierarchical structures, and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two. For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth. Additionally, they are able to avoid \"barren plateau,\" one of the most significant issues with PQC-based algorithms, ensuring trainability. Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity. In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers. Its function is to shrink the representation's spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting.  Such process can be accomplished applying full Tomography on the state to reduce it all the way down to one qubit and then processed it in subway. The most frequently used unit type in the pooling layer is max pooling, although there are other types as well. Similar to conventional feed-forward neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer. Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture.\n\n\n==== Dissipative Quantum Neural Network ====\nDissipative QNNs (DQNNs) are constructed from layers of qubits coupled by perceptron called building blocks, which have an arbitrary unitary design. Each node in the network layer of a DQNN is given a distinct collection of qubits, and each qubit is also given a unique quantum perceptron unitary to characterize it. The input states information are transported through the network in a feed-forward fashion, layer-to-layer transition mapping on the qubits of the two adjacent layers, as the name implies. Dissipative term also refers to the fact that the output layer is formed by the ancillary qubits while the input layers are dropped while tracing out the final layer. When performing a broad supervised learning task, DQNN are used to learn a unitary matrix connecting the input and output quantum states. The training data for this task consists of the quantum state and the corresponding classical labels.\nInspired by the extremely successful classical Generative adversarial network(GAN), dissipative quantum generative adversarial network (DQGAN) is introduced for unsupervised learning of the unlabeled training data . The generator and the discriminator are the two DQNNs that make up a single DQGAN. The generator's goal is to create false training states that the discriminator cannot differentiate from the genuine ones, while the discriminator's objective is to separate the real training states from the fake states created by the generator. The relevant features of the training set are learned by the generator by alternate and adversarial training of the networks that aid in the production of sets that extend the training set. DQGAN has a fully quantum architecture and is trained in quantum data.\n\n\n=== Hidden quantum Markov models ===\nHidden quantum Markov models (HQMMs) are a quantum-enhanced version of classical Hidden Markov Models (HMMs), which are typically used to model sequential data in various fields like robotics and natural language processing. Unlike the approach taken by other quantum-enhanced machine learning algorithms, HQMMs can be viewed as models inspired by quantum mechanics that can be run on classical computers as well. Where classical HMMs use probability vectors to represent hidden 'belief' states, HQMMs use the quantum analogue: density matrices. Recent work has shown that these models can be successfully learned by maximizing the log-likelihood of the given data via classical optimization, and there is some empirical evidence that these models can better model sequential data compared to classical HMMs in practice, although further work is needed to determine exactly when and how these benefits are derived. Additionally, since classical HMMs are a particular kind of Bayes net, an exciting aspect of HQMMs is that the techniques used show how we can perform quantum-analogous Bayesian inference, which should allow for the general construction of the quantum versions of probabilistic graphical models.\n\n\n=== Fully quantum machine learning ===\nIn the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.\nOne class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case. (This also relates to work on quantum pattern matching.) The problem of learning unitary transformations can be approached in a similar way.\nGoing beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum. Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in, where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning. Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup.\n\n\n=== Explainable quantum machine learning ===\nThe need for models that can be understood by humans emerges in quantum machine learning in analogy to classical machine learning and drives the research field of explainable quantum machine learning (or XQML in analogy to XAI/XML). These efforts are often also referred to as Interpretable Machine Learning (IML, and by extension IQML). XQML/IQML can be considered as an alternative research direction instead of finding a quantum advantage. For example, XQML has been used in the context of mobile malware detection and classification. Quantum Shapley values have also been proposed to interpret gates within a circuit based on a game-theoretic approach. For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest. Additionally, a quantum version of the classical technique known as LIME (Linear Interpretable Model-Agnostic Explanations) has also been proposed, known as Q-LIME.\n\n\n== Classical learning applied to quantum problems ==\n\nThe term \"quantum machine learning\" sometimes refers to classical machine learning performed on data from quantum systems. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other applications include learning Hamiltonians and automatically generating quantum experiments.\n\n\n== Quantum learning theory ==\nQuantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.\nThe starting point in learning theory is typically a concept class, a set of possible concepts. Usually a concept is a function on some domain, such as \n  \n    \n      \n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{0,1\\}^{n}}\n  \n. For example, the concept class could be the set of disjunctive normal form (DNF) formulas on n bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.\nIn active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of quantum exact learning, the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more. If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).\nA natural model of passive learning is Valiant's probably approximately correct (PAC) learning. Here the learner receives random examples (x,c(x)), where x is distributed according to some unknown distribution D. The learner's goal is to output a hypothesis function h such that h(x)=c(x) with high probability when x is drawn according to D. The learner has to be able to produce such an 'approximately correct' h for every D and every target concept c in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples \n  \n    \n      \n        \n          \u2211\n          \n            x\n          \n        \n        \n          \n            D\n            (\n            x\n            )\n          \n        \n        \n          |\n        \n        x\n        ,\n        c\n        (\n        x\n        )\n        \u27e9\n      \n    \n    {\\displaystyle \\sum _{x}{\\sqrt {D(x)}}|x,c(x)\\rangle }\n  \n. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors. However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution. When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).\nThis passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis h is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.\n\n\n== Implementations and experiments ==\nThe earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009. Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer. A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.\nUsing a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number \u20186\u2019 and \u20189\u2019 on a liquid-state  quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.\nPhotonic implementations are attracting more attention, not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013. Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule. A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.\nRecently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor. This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed, and an experiment with quantum dots performed. A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.\nSince 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods.\nIn October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs). However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found. The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs.\nA paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware.\nIn March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment. The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor.\n\n\n== Skepticism ==\nWhile machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research, quantum machine learning remains a purely theoretical field of studies. Attempts to experimentally demonstrate concepts of quantum machine learning remain insufficient.\nMany of the leading scientists that extensively publish in the field of quantum machine learning warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future. Sophia Chen collected some of the statements made by well known scientists in the field:\n\n\"I think we haven't done our homework yet. This is an extremely new scientific field,\" - physicist Maria Schuld of Canada-based quantum computing startup Xanadu.\n\u201cWhen mixing machine learning with \u2018quantum,\u2019 you catalyse a hype-condensate.\u201d - Jacob Biamonte a contributor to the theory of quantum computation.\n\"There is a lot more work that needs to be done before claiming quantum machine learning will actually work,\" - computer scientist Iordanis Kerenidis, the head of quantum algorithms at the Silicon Valley-based quantum computing startup QC Ware.\n\"I have not seen a single piece of evidence that there exists a meaningful [machine learning] task for which it would make sense to use a quantum computer and not a classical computer,\" - physicist Ryan Sweke of the Free University of Berlin in Germany.\n\u201cDon't fall for the hype!\u201d -  Frank Zickert, who is the author of probably the most practical book related to the subject beware that \u201dquantum computers are far away from advancing machine learning for their representation ability\u201d, and even speaking about evaluation and optimization for any kind of useful task quantum supremacy is not yet achieved. Furthermore, nobody among the active researchers in the field make any forecasts about when it could possibly become practical.\n\n\n== See also ==\nDifferentiable programming\nQuantum computing\nQuantum algorithm for linear systems of equations\nQuantum annealing\nQuantum neural network\nQuantum image\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Quantum_machine_learning"}, "Neural network (machine learning)": {"title": "Neural network (machine learning)", "content": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.\nAn ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\n\n\n== Training ==\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\n\n\n== History ==\n\n\n=== Early work ===\nToday's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\nHistorically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\nWarren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\nIn the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). \nIn 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.\nR. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\"\nThe perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.\nThe first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962):\u200asection 16\u200a cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.\n\n\n=== Deep learning breakthroughs in the 1960s and 1970s ===\nFundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in Ukraine (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"\nThe first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\nIn 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.\nNevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.\n\n\n=== Backpropagation ===\nBackpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.\n\n\n=== Convolutional neural networks ===\nKunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.\nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32\u00d732 pixel images.\nFrom 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.\n\n\n=== Recurrent neural networks ===\nOne origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network by John Hopfield(1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.\nTwo early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. \nIn the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, J\u00fcrgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\nIn 1991, Sepp Hochreiter's diploma thesis  identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture.\nDuring 1985\u20131995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models.\n\n\n=== Deep learning ===\nBetween 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.\nIn October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.\nIn 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".\nRadial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.\nGenerative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014\u20132018 period. The GAN principle was originally published in 1991 by J\u00fcrgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL\u00b7E 2 (2022) and Stable Diffusion (2022).\nIn 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015. ResNet behaves like an open-gated Highway Net. \n\nDuring the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.\nIt requires computation time that is quadratic in the size of the context window. J\u00fcrgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.\n\n\n== Models ==\n\nANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.\nAn artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.\n\n\n=== Artificial neurons ===\nANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.\nTo find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.\n\n\n=== Organization ===\nThe neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\n\n\n=== Hyperparameter ===\n\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.\n\n\n=== Learning ===\n\nLearning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.\n\n\n==== Learning rate ====\n\nThe learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.\n\n\n==== Cost function ====\nWhile it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).\n\n\n==== Backpropagation ====\n\nBackpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\n\n=== Learning paradigms ===\n\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\n\n\n==== Supervised learning ====\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\n\n==== Unsupervised learning ====\nIn unsupervised learning, input data is given along with the cost function, some function of the data \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n          =\n          a\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)=a}\n  \n where \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n is a constant and the cost \n  \n    \n      \n        \n          C\n          =\n          E\n          [\n          (\n          x\n          \u2212\n          f\n          (\n          x\n          )\n          \n            )\n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}\n  \n. Minimizing this cost produces a value of \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)}\n  \n, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\n\n==== Reinforcement learning ====\n\nIn applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\nFormally the environment is modeled as a Markov decision process (MDP) with states \n  \n    \n      \n        \n          \n            \n              s\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              s\n              \n                n\n              \n            \n          \n          \u2208\n          S\n        \n      \n    \n    {\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S}\n  \n and actions \n  \n    \n      \n        \n          \n            \n              a\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              a\n              \n                m\n              \n            \n          \n          \u2208\n          A\n        \n      \n    \n    {\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}\n  \n. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n  \n    \n      \n        \n          P\n          (\n          \n            c\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(c_{t}|s_{t})}\n  \n, the observation distribution \n  \n    \n      \n        \n          P\n          (\n          \n            x\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(x_{t}|s_{t})}\n  \n and the transition distribution \n  \n    \n      \n        \n          P\n          (\n          \n            s\n            \n              t\n              +\n              1\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          ,\n          \n            a\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}\n  \n, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\n\n==== Self-learning ====\nSelf-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n\n In situation s perform action a;\n Receive consequence situation s';\n Compute emotion of being in consequence situation v(s');\n Update crossbar memory w'(a,s) = w(a,s) + v(s').\n\nThe backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.\n\n\n==== Neuroevolution ====\n\nNeuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\n\n\n=== Stochastic neural network ===\nStochastic neural networks originating from Sherrington\u2013Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.\n\n\n=== Other ===\nIn a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation\u2013maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.\n\n\n==== Modes ====\n\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n\n\n== Types ==\n\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\nSome of the main breakthroughs include: \n\nConvolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads;\nCompetitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.\n\n\n== Network design ==\nUsing artificial neural networks requires an understanding of their characteristics.\n\nChoice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc. ). Overly complex models learn slowly.\nLearning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.\nRobustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.\nNeural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\n\nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters:\n\n\n== Applications ==\nBecause of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. These include:\n\nFunction approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling)\nData processing (including filtering, clustering, blind source separation, and compression)\nNonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management)\nPattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making)\nSequence recognition (including gesture, speech, and handwritten and printed text recognition)\nSensor data analysis (including image analysis)\nRobotics (including directing manipulators and prostheses)\nData mining (including knowledge discovery in databases)\nFinance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets)\nQuantum chemistry\nGeneral game playing\nGenerative AI\nData visualization\nMachine translation\nSocial network filtering\nE-mail spam filtering\nMedical diagnosis\nANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.\nANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.\nANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\nIt is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.\nBeyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.\n\n\n== Theoretical properties ==\n\n\n=== Computational power ===\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\nA specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\n\n=== Capacity ===\nA model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.\n\n\n=== Convergence ===\nModels may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\nAnother issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.\nThe convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\n\n\n=== Generalization and statistics ===\n\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.\nThe second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\nThe softmax activation function is:\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \n            \n              e\n              \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n            \n              \n                \u2211\n                \n                  j\n                  =\n                  1\n                \n                \n                  c\n                \n              \n              \n                e\n                \n                  \n                    x\n                    \n                      j\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}}\n  \n\n\n== Criticism ==\n\n\n=== Training ===\nA common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.\nAny learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.\nDean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns\u2014it should not learn to always turn right).\n\n\n=== Theory ===\nA central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\n\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\n\n=== Hardware ===\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons \u2013  which require enormous CPU power and time.\nSome argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.\nNeuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\n\n=== Practical counterexamples ===\nAnalyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.\n\n\n=== Hybrid approaches ===\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\n\n\n=== Dataset bias ===\nNeural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.\n\n\n== Gallery ==\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n== Recent advancements and future directions ==\nArtificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.\n\n\n=== Image processing ===\nIn the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.\n\n\n=== Speech recognition ===\nBy modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.\n\n\n=== Natural language processing ===\nIn natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies.\n\n\n=== Control systems ===\nIn the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.\n\n\n=== Finance ===\n\nANNs are used for stock market prediction and credit scoring: \n\nIn investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.\nIn credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.\nANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.\n\n\n=== Medicine ===\nANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.\n\n\n=== Content creation ===\nANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\nA Brief Introduction to Neural Networks (D. Kriesel) \u2013 Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\nReview of Neural Networks in Materials Science Archived 7 June 2015 at the Wayback Machine\nArtificial Neural Networks Tutorial in three languages (Univ. Polit\u00e9cnica de Madrid)\nAnother introduction to ANN\nNext Generation of Neural Networks Archived 24 January 2011 at the Wayback Machine \u2013 Google Tech Talks\nPerformance of Neural Networks\nNeural Networks and Information Archived 9 July 2009 at the Wayback Machine\nSanderson G (5 October 2017). \"But what is a Neural Network?\". 3Blue1Brown. Archived from the original on 7 November 2021 \u2013 via YouTube.", "link": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)"}, "Adversarial machine learning": {"title": "Adversarial machine learning", "content": "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.\nMost machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.\n\n\n== History ==\nAt the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine-learning spam filter could be used to defeat another machine-learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam.\nIn 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012\u20132013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.\nRecently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain's Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state-of-the-art approaches.\nWhile adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks.\n\n\n=== Examples ===\nExamples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users' template galleries that adapt to updated traits over time.\nResearchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle with a texture engineered to make Google's object detection AI classify it as a rifle regardless of the angle from which the turtle was viewed. Creating the turtle required only low-cost commercially available 3-D printing technology.\nA machine-tweaked image of a dog was shown to look like a cat to both computers and humans. A 2019 study reported that humans can guess how machines will classify adversarial images. Researchers discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle classified it as a merge or speed limit sign.\nMcAfee attacked Tesla's former Mobileye system, fooling it into driving 50 mph over the speed limit, simply by adding a two-inch strip of black tape to a speed limit sign.\nAdversarial patterns on glasses or clothing designed to deceive facial-recognition systems or license-plate readers, have led to a niche industry of \"stealth streetwear\".\nAn adversarial attack on a neural network can allow an attacker to inject algorithms into the target system. Researchers can also create adversarial audio inputs to disguise commands to intelligent assistants in benign-seeming audio; a parallel literature explores human perception of such stimuli.\nClustering algorithms are used in security applications. Malware and computer virus analysis aims to identify malware families, and to generate specific detection signatures.\n\n\n== Attack modalities ==\n\n\n=== Taxonomy ===\nAttacks against (supervised) machine learning algorithms have been categorized along three primary axes: influence on the classifier, the security violation and their specificity.\n\nClassifier influence: An attack can influence the classifier by disrupting the classification phase. This may be preceded by an exploration phase to identify vulnerabilities. The attacker's capabilities might be restricted by the presence of data manipulation constraints.\nSecurity violation: An attack can supply malicious data that gets classified as legitimate. Malicious data supplied during training can cause legitimate data to be rejected after training.\nSpecificity: A targeted attack attempts to allow a specific intrusion/disruption. Alternatively, an indiscriminate attack creates general mayhem.\nThis taxonomy has been extended into a more comprehensive threat model that allows explicit assumptions about the adversary's goal, knowledge of the attacked system, capability of manipulating the input data/system components, and on attack strategy. This taxonomy has further been extended to include dimensions for defense strategies against adversarial attacks.\n\n\n=== Strategies ===\nBelow are some of the most commonly encountered attack scenarios.\n\n\n==== Data poisoning ====\nPoisoning consists of contaminating the training dataset with data designed to increase errors in the output. Given that learning algorithms are shaped by their training datasets, poisoning can effectively reprogram algorithms with potentially malicious intent. Concerns have been raised especially for user-generated training data, e.g. for content recommendation or natural language models. The ubiquity of fake accounts offers many opportunities for poisoning. Facebook reportedly removes around 7 billion fake accounts per year. Poisoning has been reported as the leading concern for industrial applications.\nOn social medias, disinformation campaigns attempt to bias recommendation and moderation algorithms, to push certain content over others.\nA particular case of data poisoning is the backdoor attack, which aims to teach a specific behavior for inputs with a given trigger, e.g. a small defect on images, sounds, videos or texts.\nFor instance, intrusion detection systems are often trained using collected data. An attacker may poison this data by injecting malicious samples during operation that subsequently disrupt retraining.\nData poisoning techniques can also be applied to text-to-image models to alter their output.\nData poisoning can also happen unintentionally through model collapse, where models are trained on synthetic data.\n\n\n==== Byzantine attacks ====\nAs machine learning is scaled, it often relies on multiple computing machines. In federated learning, for instance, edge devices collaborate with a central server, typically by sending gradients or model parameters. However, some of these devices may deviate from their expected behavior, e.g. to harm the central server's model or to bias algorithms towards certain behaviors (e.g., amplifying the recommendation of disinformation content). On the other hand, if the training is performed on a single machine, then the model is very vulnerable to a failure of the machine, or an attack on the machine; the machine is a single point of failure. In fact, the machine owner may themselves insert provably undetectable backdoors.\nThe current leading solutions to make (distributed) learning algorithms provably resilient to a minority of malicious (a.k.a. Byzantine) participants are based on robust gradient aggregation rules. The robust aggregation rules do not always work especially when the data across participants has a non-iid distribution. Nevertheless, in the context of heterogeneous honest participants, such as users with different consumption habits for recommendation algorithms or writing styles for language models, there are provable impossibility theorems on what any robust learning algorithm can guarantee.\n\n\n==== Evasion ====\nEvasion attacks consist of exploiting the imperfection of a trained model. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware. Samples are modified to evade detection; that is, to be classified as legitimate. This does not involve influence over the training data. A clear example of evasion is image-based spam in which the spam content is embedded within an attached image to evade textual analysis by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems.\nEvasion attacks can be generally split into two different categories: black box attacks and white box attacks.\n\n\n==== Model extraction ====\nModel extraction involves an adversary probing a black box machine learning system in order to extract the data it was trained on.  This can cause issues when either the training data or the model itself is sensitive and confidential. For example, model extraction could be used to extract a proprietary stock trading model which the adversary could then use for their own financial benefit.\nIn the extreme case, model extraction can lead to model stealing, which corresponds to extracting a sufficient amount of data from the model to enable the complete reconstruction of the model.\nOn the other hand, membership inference is a targeted model extraction attack, which infers the owner of a data point, often by leveraging the overfitting resulting from poor machine learning practices. Concerningly, this is sometimes achievable even without knowledge or access to a target model's parameters, raising security concerns for models trained on sensitive data, including but not limited to medical records and/or personally identifiable information. With the emergence of transfer learning and public accessibility of many state of the art machine learning models, tech companies are increasingly drawn to create models based on public ones, giving attackers freely accessible information to the structure and type of model being used.\n\n\n== Categories ==\n\n\n=== Adversarial deep reinforcement learning ===\nAdversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area, some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.\n\n\n=== Adversarial natural language processing ===\nAdversarial attacks on speech recognition have been introduced for speech-to-text applications, in particular for Mozilla's implementation of DeepSpeech.\n\n\n=== Adversarial attacks and training in linear models ===\nThere is a growing literature about adversarial attacks in \nlinear models. Indeed, since the seminal work from Goodfellow at al.  studying these models in linear models has been an important tool to understand how adversarial attacks affect machine learning models. \nThe analysis of these models is simplified because the computation of adversarial attacks can be simplified in linear regression and classification problems. Moreover, adversarial training is convex in this case. \nLinear models allow for analytical analysis while still reproducing phenomena observed in state-of-the-art models.\nOne prime example of that is how this model can be used to explain the trade-off between robustness and accuracy. \nDiverse work indeed provides analysis of adversarial attacks in linear models, including asymptotic analysis for  classification  and for linear regression. And, finite-sample analysis based on Rademacher complexity.\n\n\n== Specific attack types ==\nThere are a large variety of different adversarial attacks that can be used against machine learning systems. Many of these work on both deep learning systems as well as traditional machine learning models such as SVMs and  linear regression. A high level sample of these attack types include:\n\nAdversarial Examples\nTrojan Attacks / Backdoor Attacks\nModel Inversion\nMembership Inference\n\n\n=== Adversarial examples ===\nAn adversarial example refers to specially crafted input that is designed to look \"normal\" to humans but causes misclassification to a machine learning model.  Often, a form of specially designed \"noise\"  is used to elicit the misclassifications. Below are some current techniques for generating adversarial examples in the literature (by no means an exhaustive list).\n\nGradient-based evasion attack\nFast Gradient Sign Method (FGSM)\nProjected Gradient Descent (PGD)\nCarlini and Wagner (C&W) attack\nAdversarial patch attack\n\n\n==== Black box attacks ====\nBlack box attacks in adversarial machine learning assume that the adversary can only get outputs for provided inputs and has no knowledge of the model structure or parameters. In this case, the adversarial example is generated either using a model created from scratch, or without any model at all (excluding the ability to query the original model). In either case, the objective of these attacks is to create adversarial examples that are able to transfer to the black box model in question.\n\n\n===== Simple Black-box Adversarial Attacks =====\nSimple Black-box Adversarial Attacks is a query-efficient way to attack black-box image classifiers. Take a random orthonormal basis \n  \n    \n      \n        \n          v\n          \n            1\n          \n        \n        ,\n        \n          v\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          v\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle v_{1},v_{2},\\dots ,v_{d}}\n  \n in \n  \n    \n      \n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{d}}\n  \n. The authors suggested the discrete cosine transform of the standard basis (the pixels).  \nFor a correctly classified image \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, try \n  \n    \n      \n        x\n        +\n        \u03f5\n        \n          v\n          \n            1\n          \n        \n        ,\n        x\n        \u2212\n        \u03f5\n        \n          v\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x+\\epsilon v_{1},x-\\epsilon v_{1}}\n  \n, and compare the amount of error in the classifier upon \n  \n    \n      \n        x\n        +\n        \u03f5\n        \n          v\n          \n            1\n          \n        \n        ,\n        x\n        ,\n        x\n        \u2212\n        \u03f5\n        \n          v\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x+\\epsilon v_{1},x,x-\\epsilon v_{1}}\n  \n. Pick the one that causes the largest amount of error.\n\nRepeat this for \n  \n    \n      \n        \n          v\n          \n            2\n          \n        \n        ,\n        \n          v\n          \n            3\n          \n        \n        ,\n        \u2026\n      \n    \n    {\\displaystyle v_{2},v_{3},\\dots }\n  \n until the desired level of error in the classifier is reached.It was discovered when the authors designed a simple baseline to compare with a previous black-box adversarial attack algorithm based on gaussian processes, and were surprised that the baseline worked even better.\n\n\n===== Square Attack =====\nThe Square Attack was introduced in 2020 as a black box evasion adversarial attack based on querying classification scores without the need of gradient information. As a score based black box attack, this adversarial approach is able to query probability distributions across model output classes, but has no other access to the model itself. According to the paper's authors, the proposed Square Attack required fewer queries than when compared to state-of-the-art score-based black box attacks at the time.\nTo describe the function objective, the attack defines the classifier as \n  \n    \n      \n        f\n        :\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n        \u2192\n        \n          \n            R\n          \n          \n            K\n          \n        \n      \n    \n    {\\textstyle f:[0,1]^{d}\\rightarrow \\mathbb {R} ^{K}}\n  \n, with \n  \n    \n      \n        d\n      \n    \n    {\\textstyle d}\n  \n representing the dimensions of the input and \n  \n    \n      \n        K\n      \n    \n    {\\textstyle K}\n  \n as the total number of output classes. \n  \n    \n      \n        \n          f\n          \n            k\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\textstyle f_{k}(x)}\n  \n returns the score (or a probability between 0 and 1) that the input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n belongs to class \n  \n    \n      \n        k\n      \n    \n    {\\textstyle k}\n  \n, which allows the classifier's class output for any input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n to be defined as \n  \n    \n      \n        \n          \n            argmax\n          \n          \n            k\n            =\n            1\n            ,\n            .\n            .\n            .\n            ,\n            K\n          \n        \n        \n          f\n          \n            k\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\textstyle {\\text{argmax}}_{k=1,...,K}f_{k}(x)}\n  \n. The goal of this attack is as follows:\n\n  \n    \n      \n        \n          \n            argmax\n          \n          \n            k\n            =\n            1\n            ,\n            .\n            .\n            .\n            ,\n            K\n          \n        \n        \n          f\n          \n            k\n          \n        \n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n        \u2260\n        y\n        ,\n        \n          |\n        \n        \n          |\n        \n        \n          \n            \n              x\n              ^\n            \n          \n        \n        \u2212\n        x\n        \n          |\n        \n        \n          \n            |\n          \n          \n            p\n          \n        \n        \u2264\n        \u03f5\n        \n           and \n        \n        \n          \n            \n              x\n              ^\n            \n          \n        \n        \u2208\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle {\\text{argmax}}_{k=1,...,K}f_{k}({\\hat {x}})\\neq y,||{\\hat {x}}-x||_{p}\\leq \\epsilon {\\text{ and }}{\\hat {x}}\\in [0,1]^{d}}\n  \n\nIn other words, finding some perturbed adversarial example \n  \n    \n      \n        \n          \n            \n              x\n              ^\n            \n          \n        \n      \n    \n    {\\textstyle {\\hat {x}}}\n  \n such that the classifier incorrectly classifies it to some other class under the constraint that \n  \n    \n      \n        \n          \n            \n              x\n              ^\n            \n          \n        \n      \n    \n    {\\textstyle {\\hat {x}}}\n  \n and \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n are similar. The paper then defines loss \n  \n    \n      \n        L\n      \n    \n    {\\textstyle L}\n  \n as \n  \n    \n      \n        L\n        (\n        f\n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n        ,\n        y\n        )\n        =\n        \n          f\n          \n            y\n          \n        \n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n        \u2212\n        \n          max\n          \n            k\n            \u2260\n            y\n          \n        \n        \n          f\n          \n            k\n          \n        \n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\textstyle L(f({\\hat {x}}),y)=f_{y}({\\hat {x}})-\\max _{k\\neq y}f_{k}({\\hat {x}})}\n  \n and proposes the solution to finding adversarial example \n  \n    \n      \n        \n          \n            \n              x\n              ^\n            \n          \n        \n      \n    \n    {\\textstyle {\\hat {x}}}\n  \n as solving the below constrained optimization problem:\n\n  \n    \n      \n        \n          min\n          \n            \n              \n                \n                  x\n                  ^\n                \n              \n            \n            \u2208\n            [\n            0\n            ,\n            1\n            \n              ]\n              \n                d\n              \n            \n          \n        \n        L\n        (\n        f\n        (\n        \n          \n            \n              x\n              ^\n            \n          \n        \n        )\n        ,\n        y\n        )\n        ,\n        \n           s.t. \n        \n        \n          |\n        \n        \n          |\n        \n        \n          \n            \n              x\n              ^\n            \n          \n        \n        \u2212\n        x\n        \n          |\n        \n        \n          \n            |\n          \n          \n            p\n          \n        \n        \u2264\n        \u03f5\n      \n    \n    {\\displaystyle \\min _{{\\hat {x}}\\in [0,1]^{d}}L(f({\\hat {x}}),y),{\\text{ s.t. }}||{\\hat {x}}-x||_{p}\\leq \\epsilon }\n  \n\nThe result in theory is an adversarial example that is highly confident in the incorrect class but is also very similar to the original image. To find such example, Square Attack utilizes the iterative random search technique to randomly perturb the image in hopes of improving the objective function. In each step, the algorithm perturbs only a small square section of pixels, hence the name Square Attack, which terminates as soon as an adversarial example is found in order to improve query efficiency. Finally, since the attack algorithm uses scores and not gradient information, the authors of the paper indicate that this approach is not affected by gradient masking, a common technique formerly used to prevent evasion attacks.\n\n\n===== HopSkipJump Attack =====\nThis black box attack was also proposed as a query efficient attack, but one that relies solely on access to any input's predicted output class. In other words, the HopSkipJump attack does not require the ability to calculate gradients or access to score values like the Square Attack, and will require just the model's class prediction output (for any given input). The proposed attack is split into two different settings, targeted and untargeted, but both are built from the general idea of adding minimal perturbations that leads to a different model output. In the targeted setting, the goal is to cause the model to misclassify the perturbed image to a specific target label (that is not the original label). In the untargeted setting, the goal is to cause the model to misclassify the perturbed image to any label that is not the original label. The attack objectives for both are as follows where \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n is the original image, \n  \n    \n      \n        \n          x\n          \n            \u2032\n          \n        \n      \n    \n    {\\textstyle x^{\\prime }}\n  \n is the adversarial image, \n  \n    \n      \n        d\n      \n    \n    {\\textstyle d}\n  \n is a distance function between images, \n  \n    \n      \n        \n          c\n          \n            \u2217\n          \n        \n      \n    \n    {\\textstyle c^{*}}\n  \n is the target label, and \n  \n    \n      \n        C\n      \n    \n    {\\textstyle C}\n  \n is the model's classification class label function:\n\n  \n    \n      \n        \n          \n            Targeted:\n          \n        \n        \n          min\n          \n            \n              x\n              \n                \u2032\n              \n            \n          \n        \n        d\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        ,\n        x\n        )\n        \n           subject to \n        \n        C\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        )\n        =\n        \n          c\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle {\\textbf {Targeted:}}\\min _{x^{\\prime }}d(x^{\\prime },x){\\text{ subject to }}C(x^{\\prime })=c^{*}}\n  \n\n  \n    \n      \n        \n          \n            Untargeted:\n          \n        \n        \n          min\n          \n            \n              x\n              \n                \u2032\n              \n            \n          \n        \n        d\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        ,\n        x\n        )\n        \n           subject to \n        \n        C\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        )\n        \u2260\n        C\n        (\n        x\n        )\n      \n    \n    {\\displaystyle {\\textbf {Untargeted:}}\\min _{x^{\\prime }}d(x^{\\prime },x){\\text{ subject to }}C(x^{\\prime })\\neq C(x)}\n  \n\nTo solve this problem, the attack proposes the following boundary function \n  \n    \n      \n        S\n      \n    \n    {\\textstyle S}\n  \n for both the untargeted and targeted setting:\n\n  \n    \n      \n        S\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        )\n        :=\n        \n          \n            {\n            \n              \n                \n                  \n                    max\n                    \n                      c\n                      \u2260\n                      C\n                      (\n                      x\n                      )\n                    \n                  \n                  \n                    F\n                    (\n                    \n                      x\n                      \n                        \u2032\n                      \n                    \n                    \n                      )\n                      \n                        c\n                      \n                    \n                  \n                  \u2212\n                  F\n                  (\n                  \n                    x\n                    \n                      \u2032\n                    \n                  \n                  \n                    )\n                    \n                      C\n                      (\n                      x\n                      )\n                    \n                  \n                  ,\n                \n                \n                  \n                    (Untargeted)\n                  \n                \n              \n              \n                \n                  F\n                  (\n                  \n                    x\n                    \n                      \u2032\n                    \n                  \n                  \n                    )\n                    \n                      \n                        c\n                        \n                          \u2217\n                        \n                      \n                    \n                  \n                  \u2212\n                  \n                    max\n                    \n                      c\n                      \u2260\n                      \n                        c\n                        \n                          \u2217\n                        \n                      \n                    \n                  \n                  \n                    F\n                    (\n                    \n                      x\n                      \n                        \u2032\n                      \n                    \n                    \n                      )\n                      \n                        c\n                      \n                    \n                  \n                  ,\n                \n                \n                  \n                    (Targeted)\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle S(x^{\\prime }):={\\begin{cases}\\max _{c\\neq C(x)}{F(x^{\\prime })_{c}}-F(x^{\\prime })_{C(x)},&{\\text{(Untargeted)}}\\\\F(x^{\\prime })_{c^{*}}-\\max _{c\\neq c^{*}}{F(x^{\\prime })_{c}},&{\\text{(Targeted)}}\\end{cases}}}\n  \n\nThis can be further simplified to better visualize the boundary between different potential adversarial examples:\n\n  \n    \n      \n        S\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        )\n        >\n        0\n        \n        \u27fa\n        \n        \n          \n            {\n            \n              \n                \n                  a\n                  r\n                  g\n                  m\n                  a\n                  \n                    x\n                    \n                      c\n                    \n                  \n                  F\n                  (\n                  \n                    x\n                    \n                      \u2032\n                    \n                  \n                  )\n                  \u2260\n                  C\n                  (\n                  x\n                  )\n                  ,\n                \n                \n                  \n                    (Untargeted)\n                  \n                \n              \n              \n                \n                  a\n                  r\n                  g\n                  m\n                  a\n                  \n                    x\n                    \n                      c\n                    \n                  \n                  F\n                  (\n                  \n                    x\n                    \n                      \u2032\n                    \n                  \n                  )\n                  =\n                  \n                    c\n                    \n                      \u2217\n                    \n                  \n                  ,\n                \n                \n                  \n                    (Targeted)\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle S(x^{\\prime })>0\\iff {\\begin{cases}argmax_{c}F(x^{\\prime })\\neq C(x),&{\\text{(Untargeted)}}\\\\argmax_{c}F(x^{\\prime })=c^{*},&{\\text{(Targeted)}}\\end{cases}}}\n  \n\nWith this boundary function, the attack then follows an iterative algorithm to find adversarial examples \n  \n    \n      \n        \n          x\n          \n            \u2032\n          \n        \n      \n    \n    {\\textstyle x^{\\prime }}\n  \n for a given image \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n that satisfies the attack objectives.\n\nInitialize \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n to some point where \n  \n    \n      \n        S\n        (\n        x\n        )\n        >\n        0\n      \n    \n    {\\textstyle S(x)>0}\n  \n\nIterate below\nBoundary search\nGradient update\nCompute the gradient\nFind the step size\nBoundary search uses a modified binary search to find the point in which the boundary (as defined by \n  \n    \n      \n        S\n      \n    \n    {\\textstyle S}\n  \n) intersects with the line between \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n and \n  \n    \n      \n        \n          x\n          \n            \u2032\n          \n        \n      \n    \n    {\\textstyle x^{\\prime }}\n  \n. The next step involves calculating the gradient for \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n, and update the original \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n using this gradient and a pre-chosen step size. HopSkipJump authors prove that this iterative algorithm will converge, leading \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n to a point right along the boundary that is very close in distance to the original image.\nHowever, since HopSkipJump is a proposed black box attack and the iterative algorithm above requires the calculation of a gradient in the second iterative step (which black box attacks do not have access to), the authors propose a solution to gradient calculation that requires only the model's output predictions alone. By generating many random vectors in all directions, denoted as \n  \n    \n      \n        \n          u\n          \n            b\n          \n        \n      \n    \n    {\\textstyle u_{b}}\n  \n, an approximation of the gradient can be calculated using the average of these random vectors weighted by the sign of the boundary function on the image \n  \n    \n      \n        \n          x\n          \n            \u2032\n          \n        \n        +\n        \n          \u03b4\n          \n            \n              u\n              \n                b\n              \n            \n          \n        \n      \n    \n    {\\textstyle x^{\\prime }+\\delta _{u_{b}}}\n  \n, where \n  \n    \n      \n        \n          \u03b4\n          \n            \n              u\n              \n                b\n              \n            \n          \n        \n      \n    \n    {\\textstyle \\delta _{u_{b}}}\n  \n is the size of the random vector perturbation:\n\n  \n    \n      \n        \u2207\n        S\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        ,\n        \u03b4\n        )\n        \u2248\n        \n          \n            1\n            B\n          \n        \n        \n          \u2211\n          \n            b\n            =\n            1\n          \n          \n            B\n          \n        \n        \u03d5\n        (\n        \n          x\n          \n            \u2032\n          \n        \n        +\n        \n          \u03b4\n          \n            \n              u\n              \n                b\n              \n            \n          \n        \n        )\n        \n          u\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle \\nabla S(x^{\\prime },\\delta )\\approx {\\frac {1}{B}}\\sum _{b=1}^{B}\\phi (x^{\\prime }+\\delta _{u_{b}})u_{b}}\n  \n\nThe result of the equation above gives a close approximation of the gradient required in step 2 of the iterative algorithm, completing HopSkipJump as a black box attack.\n\n\n==== White box attacks ====\nWhite box attacks assumes that the adversary has access to model parameters on top of being able to get labels for provided inputs.\n\n\n===== Fast gradient sign method =====\nOne of the first proposed attacks for generating adversarial examples was proposed by Google researchers Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. The attack was called fast gradient sign method (FGSM), and it consists of adding a linear amount of in-perceivable noise to the image and causing a model to incorrectly classify it. This noise is calculated by multiplying the sign of the gradient with respect to the image we want to perturb by a small constant epsilon. As epsilon increases, the model is more likely to be fooled, but the perturbations become easier to identify as well. Shown below is the equation to generate an adversarial example where \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n is the original image, \n  \n    \n      \n        \u03f5\n      \n    \n    {\\textstyle \\epsilon }\n  \n is a very small number, \n  \n    \n      \n        \n          \u0394\n          \n            x\n          \n        \n      \n    \n    {\\textstyle \\Delta _{x}}\n  \n is the gradient function, \n  \n    \n      \n        J\n      \n    \n    {\\textstyle J}\n  \n is the loss function, \n  \n    \n      \n        \u03b8\n      \n    \n    {\\textstyle \\theta }\n  \n is the model weights, and \n  \n    \n      \n        y\n      \n    \n    {\\textstyle y}\n  \n is the true label.\n\n  \n    \n      \n        a\n        d\n        \n          v\n          \n            x\n          \n        \n        =\n        x\n        +\n        \u03f5\n        \u22c5\n        s\n        i\n        g\n        n\n        (\n        \n          \u0394\n          \n            x\n          \n        \n        J\n        (\n        \u03b8\n        ,\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle adv_{x}=x+\\epsilon \\cdot sign(\\Delta _{x}J(\\theta ,x,y))}\n  \n\nOne important property of this equation is that the gradient is calculated with respect to the input image since the goal is to generate an image that maximizes the loss for the original image of true label \n  \n    \n      \n        y\n      \n    \n    {\\textstyle y}\n  \n. In traditional gradient descent (for model training), the gradient is used to update the weights of the model since the goal is to minimize the loss for the model on a ground truth dataset. The Fast Gradient Sign Method was proposed as a fast way to generate adversarial examples to evade the model, based on the hypothesis that neural networks cannot resist even linear amounts of perturbation to the input. FGSM has shown to be effective in adversarial attacks for image classification and skeletal action recognition.\n\n\n===== Carlini & Wagner (C&W) =====\nIn an effort to analyze existing adversarial attacks and defenses, researchers at the University of California, Berkeley, Nicholas Carlini and David Wagner in 2016 propose a faster and more robust method to generate adversarial examples.\nThe attack proposed by Carlini and Wagner begins with trying to solve a difficult non-linear optimization equation:\n\n  \n    \n      \n        min\n        (\n        \n          |\n        \n        \n          |\n        \n        \u03b4\n        \n          |\n        \n        \n          \n            |\n          \n          \n            p\n          \n        \n        )\n        \n           subject to \n        \n        C\n        (\n        x\n        +\n        \u03b4\n        )\n        =\n        t\n        ,\n        x\n        +\n        \u03b4\n        \u2208\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\min(||\\delta ||_{p}){\\text{ subject to }}C(x+\\delta )=t,x+\\delta \\in [0,1]^{n}}\n  \n\nHere the objective is to minimize the noise (\n  \n    \n      \n        \u03b4\n      \n    \n    {\\textstyle \\delta }\n  \n), added to the original input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n, such that the machine learning algorithm (\n  \n    \n      \n        C\n      \n    \n    {\\textstyle C}\n  \n) predicts the original input with delta (or \n  \n    \n      \n        x\n        +\n        \u03b4\n      \n    \n    {\\textstyle x+\\delta }\n  \n) as some other class \n  \n    \n      \n        t\n      \n    \n    {\\textstyle t}\n  \n. However instead of directly the above equation, Carlini and Wagner propose using a new function \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n  \n such that:\n\n  \n    \n      \n        C\n        (\n        x\n        +\n        \u03b4\n        )\n        =\n        t\n        \n        \u27fa\n        \n        f\n        (\n        x\n        +\n        \u03b4\n        )\n        \u2264\n        0\n      \n    \n    {\\displaystyle C(x+\\delta )=t\\iff f(x+\\delta )\\leq 0}\n  \n\nThis condenses the first equation to the problem below:\n\n  \n    \n      \n        min\n        (\n        \n          |\n        \n        \n          |\n        \n        \u03b4\n        \n          |\n        \n        \n          \n            |\n          \n          \n            p\n          \n        \n        )\n        \n           subject to \n        \n        f\n        (\n        x\n        +\n        \u03b4\n        )\n        \u2264\n        0\n        ,\n        x\n        +\n        \u03b4\n        \u2208\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\min(||\\delta ||_{p}){\\text{ subject to }}f(x+\\delta )\\leq 0,x+\\delta \\in [0,1]^{n}}\n  \n\nand even more to the equation below:\n\n  \n    \n      \n        min\n        (\n        \n          |\n        \n        \n          |\n        \n        \u03b4\n        \n          |\n        \n        \n          \n            |\n          \n          \n            p\n          \n        \n        +\n        c\n        \u22c5\n        f\n        (\n        x\n        +\n        \u03b4\n        )\n        )\n        ,\n        x\n        +\n        \u03b4\n        \u2208\n        [\n        0\n        ,\n        1\n        \n          ]\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\min(||\\delta ||_{p}+c\\cdot f(x+\\delta )),x+\\delta \\in [0,1]^{n}}\n  \n\nCarlini and Wagner then propose the use of the below function in place of \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n  \n using \n  \n    \n      \n        Z\n      \n    \n    {\\textstyle Z}\n  \n, a function that determines class probabilities for given input \n  \n    \n      \n        x\n      \n    \n    {\\textstyle x}\n  \n. When substituted in, this equation can be thought of as finding a target class that is more confident than the next likeliest class by some constant amount:\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        (\n        [\n        \n          max\n          \n            i\n            \u2260\n            t\n          \n        \n        Z\n        (\n        x\n        \n          )\n          \n            i\n          \n        \n        ]\n        \u2212\n        Z\n        (\n        x\n        \n          )\n          \n            t\n          \n        \n        \n          )\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle f(x)=([\\max _{i\\neq t}Z(x)_{i}]-Z(x)_{t})^{+}}\n  \n\nWhen solved using gradient descent, this equation is able to produce stronger adversarial examples when compared to fast gradient sign method that is also able to bypass defensive distillation, a defense that was once proposed to be effective against adversarial examples.\n\n\n== Defenses ==\n\nResearchers have proposed a multi-step approach to protecting machine learning.\n\nThreat modeling \u2013 Formalize the attackers goals and capabilities with respect to the target system.\nAttack simulation \u2013 Formalize the optimization problem the attacker tries to solve according to possible attack strategies.\nAttack impact evaluation\nCountermeasure design\nNoise detection (For evasion based attack)\nInformation laundering \u2013 Alter the information received by adversaries (for model stealing attacks)\n\n\n=== Mechanisms ===\nA number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed, including:\n\nSecure learning algorithms\nByzantine-resilient algorithms\nMultiple classifier systems\nAI-written algorithms.\nAIs that explore the training environment; for example, in image recognition, actively navigating a 3D environment rather than passively scanning a fixed set of 2D images.\nPrivacy-preserving learning\nLadder algorithm for Kaggle-style competitions\nGame theoretic models\nSanitizing training data\nAdversarial training\nBackdoor detection algorithms\nGradient masking/obfuscation techniques: to prevent the adversary exploiting the gradient in white-box attacks. This family of defenses is deemed unreliable as these models are still vulnerable to black-box attacks or can be circumvented in other ways.\nEnsembles of models have been proposed in the literature but caution should be applied when relying on them: usually ensembling weak classifiers results in a more accurate model but it does not seem to apply in the adversarial context.\n\n\n== See also ==\nPattern recognition\nFawkes (image cloaking software)\nGenerative adversarial network\n\n\n== References ==\n\n\n== External links ==\nMITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems\nNIST 8269 Draft: A Taxonomy and Terminology of Adversarial Machine Learning\nNIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security\nAlfaSVMLib Archived 2020-09-24 at the Wayback Machine \u2013 Adversarial Label Flip Attacks against Support Vector Machines\nLaskov, Pavel; Lippmann, Richard (2010). \"Machine learning in adversarial environments\". Machine Learning. 81 (2): 115\u2013119. doi:10.1007/s10994-010-5207-6. S2CID 12567278.\nDagstuhl Perspectives Workshop on \"Machine Learning Methods for Computer Security\"\nWorkshop on Artificial Intelligence and Security, (AISec) Series", "link": "https://en.wikipedia.org/wiki/Adversarial_machine_learning"}, "Boosting (machine learning)": {"title": "Boosting (machine learning)", "content": "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance. It is used in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.\nThe concept of boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined as a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). A strong learner is a classifier that is arbitrarily well-correlated with the true classification. Robert Schapire answered the question in the affirmative in a paper published in 1990. This has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.\nInitially, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. Algorithms that achieve this quickly became known as \"boosting\". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\n\n\n== Algorithms ==\nWhile boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners' accuracy.  After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\n\nThere are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation), and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious G\u00f6del Prize.\nOnly algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms.\nThe main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.\n\n\n== Object categorization in computer vision ==\n\nGiven images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.\n\n\n=== Problem of object categorization ===\nObject categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object.  The idea is closely related with recognition, identification, and detection.  Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples.  There are many ways to represent a category of objects, e.g. from shape analysis, bag of words models, or local descriptors such as SIFT, etc.  Examples of supervised classifiers are Naive Bayes classifiers, support vector machines, mixtures of Gaussians, and neural networks.  However, research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well.\n\n\n=== Status quo for object categorization ===\nThe recognition of object categories in images is a challenging problem in computer vision, especially when the number of categories is large.  This is due to high intra class variability and the need for generalization across variations of objects within the same category. Objects within one category may look quite different. Even the same object may appear unalike under different viewpoint, scale, and illumination. Background clutter and partial occlusion add difficulties to recognition as well.  Humans are able to recognize thousands of object types, whereas most of the existing object recognition systems are trained to recognize only a few, e.g. human faces, cars, simple objects, etc.  Research has been very active on dealing with more categories and enabling incremental additions of new categories, and although the general problem remains unsolved, several multi-category objects detectors (for up to hundreds or thousands of categories) have been developed.  One means is by feature sharing and boosting.\n\n\n=== Boosting for binary categorization ===\nAdaBoost can be used for face detection as an example of binary categorization. The two categories are faces versus background. The general algorithm is as follows:\n\nForm a large set of simple features\nInitialize weights for training images\nFor T rounds\nNormalize the weights\nFor available features from the set, train a classifier using a single feature and evaluate the training error\nChoose the classifier with the lowest error\nUpdate the weights of the training images: increase if classified wrongly by this classifier, decrease if correctly\nForm the final strong classifier as the linear combination of the T classifiers (coefficient larger if training error is small)\nAfter boosting, a classifier constructed from 200 features could yield a 95% detection rate under a \n  \n    \n      \n        \n          10\n          \n            \u2212\n            5\n          \n        \n      \n    \n    {\\displaystyle 10^{-5}}\n  \n false positive rate.\nAnother application of boosting for binary categorization is a system that detects pedestrians using patterns of motion and appearance. This work is the first to combine both motion information and appearance information as features to detect a walking person. It takes a similar approach to the Viola-Jones object detection framework.\n\n\n=== Boosting for multi-class categorization ===\nCompared with binary categorization, multi-class categorization looks for common features that can be shared across the categories at the same time.  They turn to be more generic edge like features. During learning, the detectors for each category can be trained jointly. Compared with training separately, it generalizes better, needs less training data, and requires fewer features to achieve the same performance.\nThe main flow of the algorithm is similar to the binary case. What is different is that a measure of the joint training error shall be defined in advance. During each iteration the algorithm chooses a classifier of a single feature (features that can be shared by more categories shall be encouraged). This can be done via converting multi-class classification into a binary one (a set of categories versus the rest), or by introducing a penalty error from the categories that do not have the feature of the classifier.\nIn the paper \"Sharing visual features for multiclass and multiview object detection\", A. Torralba et al. used GentleBoost for boosting and showed that when training data is limited, learning via sharing features does a much better job than no sharing, given same boosting rounds. Also, for a given performance level, the total number of features required (and therefore the run time cost of the classifier) for the feature sharing detectors, is observed to scale approximately logarithmically with the number of class, i.e., slower than linear growth in the non-sharing case. Similar results are shown in the paper \"Incremental learning of object detectors using a visual shape alphabet\", yet the authors used AdaBoost for boosting.\n\n\n== Convex vs. non-convex boosting algorithms ==\nBoosting algorithms can be based on convex or non-convex optimization algorithms.  Convex algorithms, such as AdaBoost and LogitBoost, can be \"defeated\" by random  noise such that they can't learn basic and learnable combinations of weak hypotheses. This limitation was pointed out by Long & Servedio in 2008.  However, by 2009, multiple authors demonstrated that  boosting algorithms based on non-convex optimization, such as BrownBoost, can learn from noisy datasets and can specifically learn the underlying classifier of the Long\u2013Servedio dataset.\n\n\n== See also ==\n\n\n== Implementations ==\nscikit-learn, an open source machine learning library for Python\nOrange, a free data mining software suite, module Orange.ensemble\nWeka is a machine learning set of tools that offers variate implementations of boosting algorithms like AdaBoost and LogitBoost\nR package GBM (Generalized Boosted Regression Models) implements extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine.\njboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and alternating decision trees\nR package adabag: Applies Multiclass AdaBoost.M1, AdaBoost-SAMME and Bagging\nR package xgboost: An implementation of gradient boosting for linear and tree-based models.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nFreund, Yoav; Schapire, Robert E. (1997). \"A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting\" (PDF). Journal of Computer and System Sciences. 55 (1): 119\u2013139. doi:10.1006/jcss.1997.1504.\nSchapire, Robert E. (1990). \"The strength of weak learnability\". Machine Learning. 5 (2): 197\u2013227. doi:10.1007/BF00116037. S2CID 6207294.\nSchapire, Robert E.; Singer, Yoram (1999). \"Improved Boosting Algorithms Using Confidence-Rated Predictors\". Machine Learning. 37 (3): 297\u2013336. doi:10.1023/A:1007614523901. S2CID 2329907.\nZhou, Zhihua (2008). \"On the margin explanation of boosting algorithm\" (PDF). In: Proceedings of the 21st Annual Conference on Learning Theory (COLT'08): 479\u2013490.\nZhou, Zhihua (2013). \"On the doubt about margin explanation of boosting\" (PDF). Artificial Intelligence. 203: 1\u201318. arXiv:1009.3613. doi:10.1016/j.artint.2013.07.002. S2CID 2828847.\n\n\n== External links ==\nRobert E. Schapire (2003); The Boosting Approach to Machine Learning: An Overview, MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification\nZhou Zhi-Hua (2014) Boosting 25 years Archived 2016-08-20 at the Wayback Machine, CCL 2014 Keynote.", "link": "https://en.wikipedia.org/wiki/Boosting_(machine_learning)"}, "Active learning (machine learning)": {"title": "Active learning (machine learning)", "content": "Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary.  In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer.\nLarge-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\n\n\n== Definitions ==\nLet T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.\nDuring each iteration, i, T is broken up into three subsets\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            K\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{K,i}}\n  \n: Data points where the label is known.\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            U\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{U,i}}\n  \n: Data points where the label is unknown.\n\n  \n    \n      \n        \n          \n            T\n          \n          \n            C\n            ,\n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {T} _{C,i}}\n  \n: A subset of TU,i that is chosen to be labeled.\nMost of the current research in active learning involves the best method to choose the data points for TC,i.\n\n\n== Scenarios ==\nPool-Based Sampling: In this approach, which is the most well known scenario, the learning algorithm attempts to evaluate the entire dataset before selecting data points (instances) for labeling. It is often initially trained on a fully labeled subset of the data using a machine-learning method such as logistic regression or SVM that yields class-membership probabilities for individual data instances. The candidate instances are those for which the prediction is most ambiguous. Instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner \"understands\" the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels. The theoretical drawback of pool-based sampling is that it is memory-intensive and is therefore limited in its capacity to handle enormous datasets, but in practice, the rate-limiting factor is that the teacher is typically a (fatiguable) human expert who must be paid for their effort, rather than computer memory.\nStream-Based Selective Sampling: Here, each consecutive unlabeled instance is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint. As contrasted with Pool-based sampling, the obvious drawback of stream-based methods is that the learning algorithm does not have sufficient information, early in the process, to make a sound assign-label-vs ask-teacher decision, and it does not capitalize as efficiently on the presence of already labeled data. Therefore, the teacher is likely to spend more effort in supplying labels than with the pool-based approach.\nMembership Query Synthesis: This is where the learner generates synthetic data from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if the dataset is small. The challenge here, as with all synthetic-data-generation efforts, is in ensuring that the synthetic data is consistent in terms of meeting the constraints on real data. As the number of variables/features in the input data increase, and strong dependencies between variables exist, it becomes increasingly difficult to generate synthetic data with sufficient fidelity. For example, to create a synthetic data set for human laboratory-test values, the sum of the various white blood cell (WBC) components in a White Blood Cell differential must equal 100, since the component numbers are really percentages. Similarly, the enzymes Alanine Transaminase (ALT) and Aspartate Transaminase (AST) measure liver function (though AST is also produced by other tissues, e.g., lung, pancreas) A synthetic data point with AST at the lower limit of normal range (8-33 Units/L) with an ALT several times above normal range (4-35 Units/L) in a simulated chronically ill patient would be physiologically impossible.\n\n\n== Query strategies ==\nAlgorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose:\n\nBalance exploration and exploitation: the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al. propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label.\nExpected model change: label those points that would most change the current model.\nExpected error reduction: label those points that would most reduce the model's generalization error.\nExponentiated Gradient Exploration for Active Learning: In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration.\nRandom Sampling: a sample is randomly selected.\nUncertainty sampling: label those points for which the current model is least certain as to what the correct output should be.\nEntropy Sampling: The entropy formula is used on each sample, and the sample with the highest entropy is considered to be the least certain.\nMargin Sampling: The sample with the smallest difference between the two highest class probabilities is considered to be the most uncertain.\nLeast Confident Sampling: The sample with the smallest best probability is considered to be the most uncertain.\nQuery by committee: a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the \"committee\" disagrees the most\nQuerying from diverse subspaces or partitions: When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original feature space. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling.\nVariance reduction: label those points that would minimize output variance, which is one of the components of error.\nConformal prediction: predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction.\nMismatch-first farthest-traversal: The primary selection criterion is the prediction mismatch between the current model and nearest-neighbour prediction. It targets on wrongly predicted data points. The second selection criterion is the distance to previously selected data, the farthest first. It aims at optimizing the diversity of selected data.\nUser Centered Labeling Strategies: Learning is accomplished by applying dimensionality reduction to graphs and figures like scatter plots. Then the user is asked to label the compiled data (categorical, numerical, relevance scores, relation between two instances.\nA wide variety of algorithms have been studied that fall into these categories. While the  traditional AL  strategies can  achieve  remarkable  performance, it  is often challenging to predict in advance which strategy is the most suitable in aparticular situation. In recent years, meta-learning algorithms have been gaining in popularity. Some of them have been proposed to tackle the problem of learning AL strategies instead of relying on manually designed strategies. A benchmark which compares 'meta-learning approaches to active learning' to 'traditional heuristic-based Active Learning' may give intuitions if 'Learning active learning' is at the crossroads \n\n\n== Minimum marginal hyperplane ==\nSome active learning algorithms are built upon support-vector machines (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the margin, W, of each unlabeled datum in TU,i and treat W as an n-dimensional distance from that datum to the separating hyperplane.\nMinimum Marginal Hyperplane methods assume that the data with the smallest W are those that the SVM is most uncertain about and therefore should be placed in TC,i to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest W. Tradeoff methods choose a mix of the smallest and largest Ws.\n\n\n== See also ==\nList of datasets for machine learning research\nSample complexity\nBayesian Optimization\nReinforcement learning\n\n\n== Literature ==\nImproving Generalization with Active Learning, David Cohn, Les Atlas & Richard Ladner, Machine Learning 15, 201\u2013221 (1994). https://doi.org/10.1007/BF00993277\nBalcan, Maria-Florina & Hanneke, Steve & Wortman, Jennifer. (2008). The True Sample Complexity of Active Learning.. 45-56. https://link.springer.com/article/10.1007/s10994-010-5174-y\nActive Learning and Bayesian Optimization: a Unified Perspective to Learn with a Goal, Francesco Di Fiore, Michela Nardelli, Laura Mainini, https://arxiv.org/abs/2303.01560v2\nLearning how to Active Learn: A Deep Reinforcement Learning Approach, Meng Fang, Yuan Li, Trevor Cohn, https://arxiv.org/abs/1708.02383v1\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Active_learning_(machine_learning)"}, "Timeline of machine learning": {"title": "Timeline of machine learning", "content": "This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.\n\n\n== Overview ==\n\n\n== Timeline ==\n\n\n== See also ==\nHistory of artificial intelligence\nTimeline of artificial intelligence\nTimeline of machine translation\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Works cited ===\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York: BasicBooks. ISBN 0-465-02997-3.\nMarr, Bernard (19 February 2016). \"A Short History of Machine Learning -- Every Manager Should Read\". Forbes. Archived from the original on 2022-12-05. Retrieved 2022-12-25.\nRussell, Stuart; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach. London: Pearson Education. ISBN 0-137-90395-2.", "link": "https://en.wikipedia.org/wiki/Timeline_of_machine_learning"}, "Transformer (deep learning architecture)": {"title": "Transformer (deep learning architecture)", "content": "A transformer is a deep learning architecture developed by researchers at Google and based on the multi-head attention mechanism, proposed in the 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\n\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\n\n\n== History ==\n\n\n=== Predecessors ===\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.  The linearly scaling fast weight controller (1992) learns  to compute a  weight matrix for further processing depending on the input. One of its two networks has  \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\n\n\n=== Attention with seq2seq ===\n\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see  for previous papers). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\n(Sutskever et al, 2014)  was a 380M-parameter model for machine translation using two long short-term memory (LSTM). The architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, (Cho et al, 2014) was 130M-parameter model that used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved, since the input is processed sequentially by one recurrent network into a fixed-size output vector, which was then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, and the output quality degrades. As evidence, reversing the input sentence improved seq2seq translation.\n(Bahdanau et al, 2014) introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of fixed-size output vector), allowing the model to process long-distance dependencies more easily. They called their model RNNsearch, as it \"emulates searching through a source sentence during decoding a translation\".\n(Luong et al, 2015) compared the relative performance of global (that of (Bahdanau et al, 2014)) and local (sliding window) attention model architectures for machine translation, and found that a mixed attention architecture had higher quality than global attention, while the use of a local attention architecture reduced translation time.\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it achieved a higher level of performance than the statistical approach, which took ten years to develop.\n\n\n=== Parallelizing attention ===\n\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them to be accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude less parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom of the time, and even his father, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. Its parallelizability was an important factor to its widespread use in large neural networks.\n\n\n=== AI boom era ===\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used in many generative models that contribute to the ongoing AI boom.\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model.\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), are based on the Transformer architecture.\n\n\n== Training ==\n\n\n=== Methods for stabilizing training ===\nThe plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\nA 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.\n\n\n=== Pretrain-finetune ===\nTransformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n\nlanguage modeling\nnext-sentence prediction\nquestion answering\nreading comprehension\nsentiment analysis\nparaphrasing\nThe T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:\n\nrestoring or repairing incomplete or corrupted text. For example, the input, \"Thank you\u202f~~\u202fme to your party\u202f~~\u202fweek\", might generate the output, \"Thank you for inviting me to your party last week\".\ntranslation between natural languages (machine translation)\njudging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.\nNote that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\n\n\n=== Tasks ===\n\nIn general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\nIn a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n  \n    \n      \n        \n          Loss\n        \n        =\n        \u2212\n        \n          \u2211\n          \n            t\n            \u2208\n            \n              masked tokens\n            \n          \n        \n        ln\n        \u2061\n        (\n        \n          probability of \n        \n        t\n        \n           conditional on its context\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n  \nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\nIn an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\nIn a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\nNote that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).\n\n\n== Architecture ==\nAll transformers have the same primary components:\n\nTokenizers, which convert text into tokens.\nEmbedding layer, which converts tokens and positions of the tokens into vector representations.\nTransformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.\nUn-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.\nThe following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\nBy convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n  \n    \n      \n        x\n        W\n      \n    \n    {\\displaystyle xW}\n  \n.\n\n\n=== Tokenization ===\n\nAs the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between token sequences and texts is a tokenizer.\nThe set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \n  \n    \n      \n        \n          n\n          \n            vocabulary\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{vocabulary}}}\n  \n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\nSome commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.\n\n\n=== Embedding ===\n\nEach token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. For example, if the input token is \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  \n, then the one-hot representation is \n  \n    \n      \n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        \u2026\n        ]\n      \n    \n    {\\displaystyle [0,0,0,1,0,0,\\dots ]}\n  \n, and its embedding vector is\n  \n    \n      \n        \n          E\n          m\n          b\n          e\n          d\n        \n        (\n        3\n        )\n        =\n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        \u2026\n        ]\n        M\n      \n    \n    {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n  \nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. \nThe number of dimensions in an embedding vector is called hidden size or embedding size and written as \n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb}}}\n  \n \n\n\n=== Un-embedding ===\nAn un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\nThe un-embedding layer is a linear-softmax layer:\n  \n    \n      \n        \n          U\n          n\n          E\n          m\n          b\n          e\n          d\n        \n        (\n        x\n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        x\n        W\n        +\n        b\n        )\n      \n    \n    {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n  \nThe matrix has shape \n  \n    \n      \n        (\n        \n          d\n          \n            emb\n          \n        \n        ,\n        \n          n\n          \n            vocabulary\n          \n        \n        )\n      \n    \n    {\\displaystyle (d_{\\text{emb}},n_{\\text{vocabulary}})}\n  \n. The embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n and the un-embedding matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n are sometimes required to be transposes of each other, a practice called weight tying.\n\n\n=== Positional encoding ===\n\nA positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. Without positional encoding, the model would be unable to process input sequence as more than a bag of words, as for example, both \"man bites dog\" and \"dog bites man\" would be processed exactly the same way.\nThe positional encoding is defined as a function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        \u2192\n        \n          \n            R\n          \n          \n            d\n          \n        \n        ;\n        d\n        \u2208\n        \n          Z\n        \n        ,\n        d\n        >\n        0\n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0}\n  \n, where \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n is a positive even integer. The full positional encoding defined in the original paper is:\n  \n    \n      \n        (\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n          \n        \n        ,\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n        =\n        (\n        sin\n        \u2061\n        (\n        \u03b8\n        )\n        ,\n        cos\n        \u2061\n        (\n        \u03b8\n        )\n        )\n        \n        \u2200\n        k\n        \u2208\n        {\n        0\n        ,\n        1\n        ,\n        \u2026\n        ,\n        d\n        \n          /\n        \n        2\n        \u2212\n        1\n        }\n      \n    \n    {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n  \nwhere \n  \n    \n      \n        \u03b8\n        =\n        \n          \n            t\n            \n              r\n              \n                k\n              \n            \n          \n        \n        ,\n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n  \n.\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a free parameter that should be significantly larger than the biggest \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n that would be input into the positional encoding function. The original paper uses \n  \n    \n      \n        N\n        =\n        10000\n      \n    \n    {\\displaystyle N=10000}\n  \n.\nThe function is in a simpler form when written as a complex function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        \u2192\n        \n          \n            C\n          \n          \n            d\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n  \n\n  \n    \n      \n        f\n        (\n        t\n        )\n        =\n        \n          \n            (\n            \n              e\n              \n                i\n                t\n                \n                  /\n                \n                \n                  r\n                  \n                    k\n                  \n                \n              \n            \n            )\n          \n          \n            k\n            =\n            0\n            ,\n            1\n            ,\n            \u2026\n            ,\n            \n              \n                d\n                2\n              \n            \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n  \nwhere \n  \n    \n      \n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle r=N^{2/d}}\n  \n.\nThe main reason for using this positional encoding function is that using it, shifts are linear transformations:\n  \n    \n      \n        f\n        (\n        t\n        +\n        \u0394\n        t\n        )\n        =\n        \n          d\n          i\n          a\n          g\n        \n        (\n        f\n        (\n        \u0394\n        t\n        )\n        )\n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n  \nwhere \n  \n    \n      \n        \u0394\n        t\n        \u2208\n        \n          R\n        \n      \n    \n    {\\displaystyle \\Delta t\\in \\mathbb {R} }\n  \n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\nBy taking a linear sum, any convolution can also be implemented as linear transformations:\n  \n    \n      \n        \n          \u2211\n          \n            j\n          \n        \n        \n          c\n          \n            j\n          \n        \n        f\n        (\n        t\n        +\n        \u0394\n        \n          t\n          \n            j\n          \n        \n        )\n        =\n        \n          (\n          \n            \n              \u2211\n              \n                j\n              \n            \n            \n              c\n              \n                j\n              \n            \n            \n            \n              d\n              i\n              a\n              g\n            \n            (\n            f\n            (\n            \u0394\n            \n              t\n              \n                j\n              \n            \n            )\n            )\n          \n          )\n        \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n  \nfor any constants \n  \n    \n      \n        \n          c\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle c_{j}}\n  \n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\nIn typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\n\n\n=== Encoder-decoder (overview) ===\n\nLike earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\nThe purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).\nBoth the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model.\n\n\n=== Feedforward network ===\n\nThe feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\n  \n    \n      \n        \n          F\n          F\n          N\n        \n        (\n        x\n        )\n        =\n        \u03d5\n        (\n        x\n        \n          W\n          \n            (\n            1\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            1\n            )\n          \n        \n        )\n        \n          W\n          \n            (\n            2\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n  \nwhere \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  \n is its activation function. The original Transformer used ReLU activation.\nThe number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n  \n    \n      \n        \n          d\n          \n            ffn\n          \n        \n        =\n        4\n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n  \n.\n\n\n=== Scaled dot-product attention ===\n\n\n==== Attention head ====\n\nThe attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n, the key weights \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n, and the value weights \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n.\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n  \n    \n      \n        \n          \u2113\n          \n            seq, query\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, query}}}\n  \n, and each entry is a vector of dimension \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}}\n  \n. Similarly for the key and value sequences.\nFor each vector \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i,{\\text{query}}}}\n  \n in the query sequence, it is multiplied by a matrix \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n to produce a query vector \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n  \n. The matrix of all query vectors is the query matrix:\n  \n    \n      \n        Q\n        =\n        \n          X\n          \n            query\n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle Q=X_{\\text{query}}W^{Q}}\n  \nSimilarly, we construct the key matrix \n  \n    \n      \n        K\n        =\n        \n          X\n          \n            key\n          \n        \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle K=X_{\\text{key}}W^{K}}\n  \n and the value matrix \n  \n    \n      \n        V\n        =\n        \n          X\n          \n            value\n          \n        \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle V=X_{\\text{value}}W^{V}}\n  \n.\nIt is usually the case that all \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{Q},W^{K},W^{V}}\n  \n are square matrices, meaning \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n        =\n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n  \n, etc.\nAttention weights are calculated using the query and key vectors: the attention weight \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n is the dot product between \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and \n  \n    \n      \n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k_{j}}\n  \n. The attention weights are divided by the square root of the dimension of the key vectors, \n  \n    \n      \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {d_{k}}}}\n  \n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n are different matrices allows attention to be non-symmetric: if token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n attends to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        \u22c5\n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle q_{i}\\cdot k_{j}}\n  \n is large), this does not necessarily mean that token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n will attend to token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            j\n          \n        \n        \u22c5\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{j}\\cdot k_{i}}\n  \n could be small). The output of the attention unit for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is the weighted sum of the value vectors of all tokens, weighted by \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n, the attention from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to each token.\nThe attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are defined as the matrices where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \nth rows are vectors \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n, \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n, and \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  \n respectively. Then we can represent the attention as\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        Q\n                        \n                          K\n                          \n                            \n                              T\n                            \n                          \n                        \n                      \n                      \n                        \n                          d\n                          \n                            k\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n\nwhere the softmax is applied over each of the rows of the matrix.\nThe number of dimensions in a query vector is query size \n  \n    \n      \n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{query}}}\n  \n and similarly for the key size \n  \n    \n      \n        \n          d\n          \n            key\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{key}}}\n  \n and value size \n  \n    \n      \n        \n          d\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{value}}}\n  \n. The output dimension of an attention head is its head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n. The attention mechanism requires the following three equalities to hold:\n  \n    \n      \n        \n          \u2113\n          \n            seq, key\n          \n        \n        =\n        \n          \u2113\n          \n            seq, value\n          \n        \n        ,\n        \n        \n          d\n          \n            query\n          \n        \n        =\n        \n          d\n          \n            key\n          \n        \n        ,\n        \n        \n          d\n          \n            value\n          \n        \n        =\n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n  \nbut is otherwise unconstrained.\nIf the attention head is used in a self-attention fashion, then \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        =\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n  \n. If the attention head is used in a cross-attention fashion, then usually \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        \u2260\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n  \n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\n\n\n==== Multiheaded attention ====\n\nOne set of \n  \n    \n      \n        \n          (\n          \n            \n              W\n              \n                Q\n              \n            \n            ,\n            \n              W\n              \n                K\n              \n            \n            ,\n            \n              W\n              \n                V\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n  \n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n, in combination with the part of the output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\nConcretely, let the multiple attention heads be indexed by \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, then we have\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        (\n        \n          Attention\n        \n        (\n        X\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}\n  \n where the matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is the concatenation of word embeddings, and the matrices \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n are \"projection matrices\" owned by individual attention head \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n is a final projection matrix owned by the whole multi-headed attention head.\nIt is theoretically possible for each attention head to have a different head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n, but that is rarely the case in practice.\nAs an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n        =\n        768\n        ,\n        \n          n\n          \n            head\n          \n        \n        =\n        12\n        ,\n        \n          d\n          \n            head\n          \n        \n        =\n        64\n      \n    \n    {\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n  \nSince \n  \n    \n      \n        12\n        \u00d7\n        64\n        =\n        768\n      \n    \n    {\\displaystyle 12\\times 64=768}\n  \n, its output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n        \u2208\n        \n          \n            R\n          \n          \n            (\n            12\n            \u00d7\n            64\n            )\n            \u00d7\n            768\n          \n        \n      \n    \n    {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n  \n is a square matrix.\n\n\n==== Masked attention ====\nIt may be necessary to cut out attention links between some word-pairs. For example, the decoder, when decoding for the token position \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, should not have access to the token at position \n  \n    \n      \n        t\n        +\n        1\n      \n    \n    {\\displaystyle t+1}\n  \n. This may be accomplished before the softmax stage by adding a mask matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n that is \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n  \n at entries where the attention link must be cut, and \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at other places:\n  \n    \n      \n        \n          \n            \n              \n                \n                  MaskedAttention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    M\n                    +\n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \nA non-masked attention module can be thought of as a masked attention module where the mask has all entries zero.\nFor example, the following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n  \n    \n      \n        \n          M\n          \n            causal\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2212\n                  \u221e\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2026\n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n                \n                  \u22ee\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  \u2026\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n  \nIn words, it means that each token can pay attention to itself, and every token before it, but not any after it. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n  \n    \n      \n        P\n        \n          M\n          \n            causal\n          \n        \n        \n          P\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle PM_{\\text{causal}}P^{-1}}\n  \n, where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is a random permutation matrix.\n\n\n=== Encoder ===\n\nAn encoder consists of an embedding layer, followed by multiple encoder layers.\nEach encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  given input vectors \n                \n              \n              \n                \n                  h\n                  \n                    0\n                  \n                \n                ,\n                \n                  h\n                  \n                    1\n                  \n                \n                ,\n                \u2026\n              \n            \n            \n              \n                \n                  combine them into a matrix \n                \n                H\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            h\n                            \n                              0\n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                            h\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \n                        \n                          \u22ee\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n              \n                \n                  EncoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              0\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              1\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \u22ee\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n stands for \"feed-forward network\". We can more succinctly write it as\n  \n    \n      \n        \n          EncoderLayer\n        \n        (\n        H\n        )\n        =\n        \n          FFN\n        \n        (\n        \n          MultiheadedAttention\n        \n        (\n        H\n        ,\n        H\n        ,\n        H\n        )\n        )\n      \n    \n    {\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H))}\n  \nwith the implicit convention that the \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n is applied to each row of the matrix individually.\nThe encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\nAs the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\n\n\n=== Decoder ===\n\nA decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\nEach decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.\nLike the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\nIn contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\nSchematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  H\n                  \u2032\n                \n              \n              \n                \n                =\n                \n                  MaskedMultiheadedAttention\n                \n                (\n                H\n                ,\n                H\n                ,\n                H\n                )\n              \n            \n            \n              \n                \n                  DecoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  FFN\n                \n                (\n                \n                  MultiheadedAttention\n                \n                (\n                \n                  H\n                  \u2032\n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                )\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        \n          H\n          \n            E\n          \n        \n      \n    \n    {\\displaystyle H^{E}}\n  \n is the matrix with rows being the output vectors from the encoder.\nThe last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.\n\n\n== Full transformer architecture ==\n\n\n=== Sublayers ===\nEach encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n\nThe final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. \nThere are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n  \n    \n      \n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n  \nwhere \n  \n    \n      \n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {Sublayer} (x)}\n  \n is the function implemented by the sublayer itself.\nIn the pre-LN convention, the output of each sublayer is\n  \n    \n      \n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n  \nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.\n\n\n=== Pseudocode ===\nThe following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from\n\ninput: Encoder input t_e\n       Decoder input t_d\noutput: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))\n\n/* encoder */\nz_e \u2190 encoder.tokenizer(t_e)\n\nfor each t in 1:length(z_e) do\n    z_e[t] \u2190 encoder.embedding(z_e[t]) + encoder.positional_embedding(t)\n\nfor each l in 1:length(encoder.layers) do\n    layer \u2190 encoder.layers[l]\n\n    /* first sublayer */\n    z_e_copy \u2190 copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 layer.layer_norm(z_e[t])\n    z_e \u2190 layer.multiheaded_attention(z_e, z_e, z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 z_e[t] + z_e_copy[t]\n\n    /* second sublayer */\n    z_e_copy \u2190 copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 layer.layer_norm(z_e[t])\n    z_e \u2190 layer.feedforward(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] \u2190 z_e[t] + z_e_copy[t]\n\nfor each t in 1:length(z_e) do\n    z_e[t] \u2190 encoder.final_layer_norm(z_e[t])\n\n/* decoder */\nz_d \u2190 decoder.tokenizer(t_d)\n\nfor each t in 1:length(z_d) do\n    z_d[t] \u2190 decoder.embedding(z_d[t]) + decoder.positional_embedding(t)\n\nfor each l in 1:length(decoder.layers) do\n        layer \u2190 decoder.layers[l]\n\n        /* first sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.masked_multiheaded_attention(z_d, z_d, z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\n        /* second sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.multiheaded_attention(z_d, z_e, z_e) \n        for each i in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\n        /* third sublayer */\n        z_d_copy \u2190 copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 layer.layer_norm(z_d[t])\n        z_d \u2190 layer.feedforward(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] \u2190 z_d[t] + z_d_copy[t]\n\nz_d \u2190 decoder.final_layer_norm(z_d)\n\noutput_distributions \u2190 []\nfor each t in 1:length(z_d) do\n    output_distributions.append(decoder.unembed(z_d[t]))\n\nreturn output_distributions\n\n\n=== Terminology ===\nThe Transformer architecture, being modular, allows variations. Several common variations are described here.\nAn \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.\nA \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\nAn \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.\nA \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form:\u200aFigure 3\u200a\n  \n    \n      \n        \n          M\n          \n            prefixLM\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \u2212\n                  \u221e\n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    M\n                    \n                      causal\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n  \nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.\nThere are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.\n\n\n== Subsequent work ==\n\n\n=== Alternative activation functions ===\nThe original transformer uses ReLU activation function. Other activation functions were developed. The Llama series used SwiGLU; both GPT-1 and BERT used GELU. \nAlternative activation functions are often used in combination with Gated Linear Units in the feedforward module.\n\n\n=== Alternative normalizations ===\nThe normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include ScaleNorm, or FixNorm.\n\n\n=== Alternative positional encodings ===\nTransformers may use other positional encoding methods than sinusoidal. \nThe original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later,  found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\n\n\n==== RoPE ====\nRoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors \n  \n    \n      \n        [\n        (\n        \n          x\n          \n            1\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            2\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            3\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        .\n        .\n        .\n        ]\n      \n    \n    {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n  \n. Now pick some angle \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  \n. Then RoPE encoding is\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                \n                \n                  \u2212\n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n              \n                \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n                \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n            \n            )\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                  \u2212\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  cos\n                  \u2061\n                  m\n                  \u03b8\n                  +\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  sin\n                  \u2061\n                  m\n                  \u03b8\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n  \nEquivalently, if we write the 2-dimensional vectors as complex numbers \n  \n    \n      \n        \n          z\n          \n            m\n          \n        \n        :=\n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        +\n        i\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n  \n, then RoPE encoding is just multiplication by an angle:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          z\n          \n            m\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          e\n          \n            i\n            m\n            \u03b8\n          \n        \n        \n          z\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n  \nFor a list of \n  \n    \n      \n        2\n        n\n      \n    \n    {\\displaystyle 2n}\n  \n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n  \n    \n      \n        \n          \u03b8\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \u03b8\n          \n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n  \n. Then the RoPE encoding is applied to each pair of coordinates.\nThe benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        \n          \n            )\n          \n        \n        =\n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        +\n        k\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        +\n        k\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n  \n\nfor any integer \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.\n\n\n==== ALiBi ====\nALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    s\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n  \nHere, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a real number (\"scalar\"), and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the linear bias matrix defined by\n  \n    \n      \n        B\n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  2\n                \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u2212\n                  3\n                \n                \n                  \u2212\n                  2\n                \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n                \n                  \u22ef\n                \n              \n              \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22ee\n                \n                \n                  \u22f1\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n  \nin other words, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        j\n        \u2212\n        i\n      \n    \n    {\\displaystyle B_{i,j}=j-i}\n  \n. The idea being that the linear bias matrix is a softened mask. Just as \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n represent full attention paid, and \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n  \n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\nALiBi allows pretraining on short context windows, then finetuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\n\n\n==== Relative Position Encodings ====\nRelative Position Encodings is similar to ALiBi, but more generic:\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is a Toeplitz matrix, that is, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        \n          B\n          \n            \n              i\n              \u2032\n            \n            ,\n            \n              j\n              \u2032\n            \n          \n        \n      \n    \n    {\\displaystyle B_{i,j}=B_{i',j'}}\n  \n whenever \n  \n    \n      \n        i\n        \u2212\n        j\n        =\n        \n          i\n          \u2032\n        \n        \u2212\n        \n          j\n          \u2032\n        \n      \n    \n    {\\displaystyle i-j=i'-j'}\n  \n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".\n\n\n=== Efficient implementation ===\nThe transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.\n\n\n==== FlashAttention ====\nFlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow).\nAn improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\nKey advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).\nBenchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\n\n\n==== Multi-Query Attention ====\nMulti-Query Attention changes the multiheaded attention mechanism. Whereas normally,\n\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n  \nwith Multi-Query Attention, there is just one \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{K},W^{V}}\n  \n, thus:\n\n  \n    \n      \n        \n          MultiQueryAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            \u2208\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n  \n\nThis has a neutral effect on model quality and training speed, but increases inference speed. \nMore generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded atteniton is GQA with the maximal number of groups.\n\n\n==== Caching ====\nWhen an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.\nIf a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.\n\n\n==== Speculative decoding ====\nTransformers are used in large language models for autoregressive sequence generation: generating a stream of text, one token at a time. However, in most settings, decoding from language models is memory-bound, meaning that we have spare compute power available. Speculative decoding uses this spare compute power by computing several tokens in parallel. Similarly to speculative execution in CPUs, future tokens are computed concurrently, by speculating on the value of previous tokens, and are later discarded if it turns out the speculation was incorrect.\nSpecifically, consider a transformer model like GPT-3 with a context window size of 512. To generate an entire context window autoregressively with greedy decoding, it must be run for 512 times, each time generating a token \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            512\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{512}}\n  \n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is indeed the token with the largest log-likelihood in the \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n-th output.\nIn speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose a small model generated four speculative tokens: \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n. These tokens are run through the larger model, and only \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1}}\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{2}}\n  \n are accepted. The same run of the large model already generated a new token \n  \n    \n      \n        \n          x\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle x_{3}}\n  \n to replace \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3}}\n  \n, and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{4}}\n  \n is completely discarded. The process then repeats (starting from the 4th token) until all tokens are generated.\nFor non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.\n\n\n=== Sub-quadratic transformers ===\nTraining transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\n\n\n==== Alternative attention graphs ====\nThe standard attention graph is either all-to-all or causal, both of which scales as \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in a sequence.\nReformer (2020) reduces the computational load from \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n to \n  \n    \n      \n        O\n        (\n        N\n        ln\n        \u2061\n        N\n        )\n      \n    \n    {\\displaystyle O(N\\ln N)}\n  \n by using locality-sensitive hashing and reversible layers. \nSparse attention uses attention graphs that grows slower than \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n. For example, BigBird (2020) uses random small-world networks which grows as \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n  \n.\nOrdinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\n\n\n==== Random Feature Attention ====\nRandom Feature Attention (2021) uses Fourier random features:\n  \n    \n      \n        \u03c6\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              D\n            \n          \n        \n        [\n        cos\n        \u2061\n        \u27e8\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        sin\n        \u2061\n        \u27e8\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        \u22ef\n        cos\n        \u2061\n        \u27e8\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        \u27e9\n        ,\n        sin\n        \u2061\n        \u27e8\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        \u27e9\n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n  \nwhere \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are independent samples from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n. This choice of parameters satisfy \n  \n    \n      \n        \n          E\n        \n        [\n        \u27e8\n        \u03c6\n        (\n        x\n        )\n        ,\n        \u03c6\n        (\n        y\n        )\n        \u27e9\n        ]\n        =\n        \n          e\n          \n            \u2212\n            \n              \n                \n                  \u2016\n                  x\n                  \u2212\n                  y\n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n  \n, or \n  \n    \n      \n        \n          e\n          \n            \u27e8\n            x\n            ,\n            y\n            \u27e9\n            \n              /\n            \n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        =\n        \n          E\n        \n        [\n        \u27e8\n        \n          e\n          \n            \u2016\n            x\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            \u2016\n            y\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        y\n        )\n        \u27e9\n        ]\n        \u2248\n        \u27e8\n        \n          e\n          \n            \u2016\n            x\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            \u2016\n            y\n            \n              \u2016\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              \u03c3\n              \n                2\n              \n            \n          \n        \n        \u03c6\n        (\n        y\n        )\n        \u27e9\n      \n    \n    {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n  \nConsequently, the one-headed attention, with one query, can be written as \n  \n    \n      \n        \n          Attention\n        \n        (\n        q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        \u2248\n        \n          \n            \n              \u03c6\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                e\n                \n                  \u2016\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \u03c6\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n              \n                v\n                \n                  i\n                \n                \n                  T\n                \n              \n            \n            \n              \u03c6\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                \u2211\n                \n                  i\n                \n              \n              \n                e\n                \n                  \u2016\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    \u2016\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \u03c6\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n  \nwhere \n  \n    \n      \n        \u03c3\n        =\n        \n          d\n          \n            K\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle \\sigma =d_{K}^{1/4}}\n  \n. Similarly for multiple queries, and for multiheaded attention.\nThis approximation can be computed in linear time, as we can compute the matrix \n  \n    \n      \n        \u03c6\n        (\n        \n          k\n          \n            i\n          \n        \n        )\n        \n          v\n          \n            i\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n  \n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        \u2248\n        Q\n        (\n        \n          K\n          \n            T\n          \n        \n        V\n        \n          /\n        \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n  \nPerformer (2022) uses the same Random Feature Attention, but \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are first independently sampled from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n, then they are Gram-Schmidt processed.\n\n\n=== Multimodality ===\nTransformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\nMultimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.\nVision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\nConformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\nPerceivers are a variant of Transformers designed for multimodality.\nFor image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.\n\n\n== Applications ==\nThe transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, AlbertAGPT, Claude, BERT, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world or practical applications, including:\n\nmachine translation\ntime series prediction\ndocument summarization\ndocument generation\nnamed entity recognition (NER)\nwriting computer code based on requirements expressed in natural language.\nspeech-to-text\nBeyond traditional NLP, the transformer architecture has had success in other applications, such as:\n\nbiological sequence analysis\nvideo understanding\nprotein folding (such as AlphaFold)\nevaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.\n\n\n== See also ==\nseq2seq \u2013 Family of machine learning approaches\nPerceiver \u2013 Variant of Transformer designed for multimodal data\nVision transformer \u2013 Variant of Transformer designed for vision processing\nLarge language model \u2013 Type of artificial neural network\nBERT (language model) \u2013 Series of language models developed by Google AI\nGenerative pre-trained transformer \u2013 Type of large language model\nT5 (language model) \u2013 Series of large language models developed by Google AI\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==", "link": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)"}, "Automated machine learning": {"title": "Automated machine learning", "content": "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\n\n\n== Comparison to the standard approach ==\nIn a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. \nEach of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\nAutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.\n\n\n== Targets of automation ==\nAutomated machine learning can target various stages of the machine learning process.  Steps to automate are:\n\nData preparation and ingestion (from raw data and miscellaneous formats)\nColumn type detection; e.g., Boolean, discrete numerical, continuous numerical, or text\nColumn intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\nTask detection; e.g., binary classification, regression, clustering, or ranking\nFeature engineering\nFeature selection\nFeature extraction\nMeta-learning and transfer learning\nDetection and handling of skewed data and/or missing values\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\nEnsembling - a form of consensus where using multiple models often gives better results than any single model\nHyperparameter optimization of the learning algorithm and featurization\nNeural architecture search\nPipeline selection under time, memory, and complexity constraints\nSelection of evaluation metrics and validation procedures\nProblem checking\nLeakage detection\nMisconfiguration detection\nAnalysis of obtained results\nCreating user interfaces and visualizations\n\n\n== Challenges and Limitations ==\nThere are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as \"development as a cottage industry\". This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it's the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the machines. However, experts and developers must help create and guide these machines to prepare them for their own learning. To create this system, it requires labor intensive work with knowledge of machine learning algorithms and system design.\nAdditionally, some other challenges include meta-learning challenges and computational resource allocation.\n\n\n== See also ==\nNeural architecture search\nNeuroevolution\nSelf-tuning\nNeural Network Intelligence\nAutoAI\nModelOps\nHyperparameter optimization\n\n\n== References ==\n\n\n== Further reading ==\n\"Open Source AutoML Tools: AutoGluon, TransmogrifAI, Auto-sklearn, and NNI\". Bizety. 2020-06-16.\nFerreira, Lu\u00eds, et al. \"A comparison of AutoML tools for machine learning, deep learning and XGBoost.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021. https://repositorium.sdum.uminho.pt/bitstream/1822/74125/1/automl_ijcnn.pdf\nFeurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., & Hutter, F. (2015). Efficient and robust automated machine learning. Advances in neural information processing systems, 28. https://proceedings.neurips.cc/paper_files/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf", "link": "https://en.wikipedia.org/wiki/Automated_machine_learning"}}, "Data Science": {"Data science": {"title": "Data science", "content": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.\n\n\n== Foundations ==\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human\u2013computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.\n\n\n=== Relationship to statistics ===\nMany statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.\nStanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.\n\n\n== Etymology ==\n\n\n=== Early usage ===\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.\nThe term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.\nDuring the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".\n\n\n=== Modern usage ===\nIn 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".\nThe modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.\nThe professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.\nThere is still no consensus on the definition of data science, and it is considered by some to be a buzzword. Big data is a related marketing term. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.\n\n\n== Data science and data analysis ==\n\nData science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.\nData analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.\nData science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.\nWhile data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.\nDespite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.\nIn summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n\n\n== Cloud computing for data science ==\n\nCloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.\nSome distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.\n\n\n== Ethical consideration in data science ==\nData science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts \nMachine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.\n\n\n== See also ==\nPython (programming language)\nR (programming language)\nData engineering\nBig data\nMachine learning\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Data_science"}, "Data (computer science)": {"title": "Data (computer science)", "content": "In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data. Data requires interpretation to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation. In modern (post-1960) computer systems, all data is digital. \nData exists in three states: data at rest, data in transit and data in use. Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices.\nPhysical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures.\n\n\n== Characteristics ==\nMetadata helps translate data to information. Metadata is data about the data. Metadata may be implied, specified or given.  \nData relating to physical events or processes will have a temporal component. This temporal component may be implied. This is the case when a device such as a temperature logger receives data from a temperature sensor. When the temperature is received it is assumed that the data has a temporal reference of now. So the device records the date, time and temperature together. When the data logger communicates temperatures, it must also report the date and time as metadata for each temperature reading.\nFundamentally, computers follow a sequence of instructions they are given in the form of data. A set of instructions to perform a given task (or tasks) is called a program. A program is data in the form of coded instructions to control the operation of a computer or other machine. In the nominal case, the program, as executed by the computer, will consist of machine code. The elements of storage manipulated by the program, but not actually executed by the central processing unit (CPU), are also data. At its most essential, a single datum is a value stored at a specific location. Therefore, it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data.\nTo store data bytes in a file, they have to be serialized in a file format. Typically, programs are stored in special file types, different from those used for other data. Executable files contain programs; all other files are also data files. However, executable files may also contain data used by the program which is built into the program. In particular, some executable files have a data segment, which nominally contains constants and initial values for variables, both of which can be considered data.\nThe line between program and data can become blurry. An interpreter, for example, is a program. The input data to an interpreter is itself a program, just not one expressed in native machine language. In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program. Metaprogramming similarly involves programs manipulating other programs as data. Programs like compilers, linkers, debuggers, program updaters, virus scanners and such use other programs as their data.\nFor example, a user might first instruct the operating system to load a word processor program from one file, and then use the running program to open and edit a document stored in another file. In this example, the document would be considered data. If the word processor also features a spell checker, then the dictionary (word list) for the spell checker would also be considered data. The algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language.\nIn an alternate usage, binary files (which are not human-readable) are sometimes called data as distinguished from human-readable text. \nThe total amount of digital data in 2007 was estimated to be 281 billion gigabytes (281 exabytes).\n\n\n== Data keys and values, structures and persistence ==\nKeys in data provide the context for values. Regardless of the structure of data, there is always a key component present. Keys in data and data-structures are essential for giving meaning to data values. Without a key that is directly or indirectly associated with a value, or collection of values in a structure, the values become meaningless and cease to be data. That is to say, there has to be a key component linked to a value component in order for it to be considered data.\nData can be represented in computers in multiple ways, as per the following examples:\n\n\n=== RAM ===\nRandom access memory (RAM) holds data that the CPU has direct access to. A CPU may only manipulate data within its processor registers or memory. This is as opposed to data storage, where the CPU must direct the transfer of data between the storage device (disk, tape...) and memory. RAM is an array of linear contiguous locations that a processor may read or write by providing an address for the read or write operation. The processor may operate on any location in memory at any time in any order. In RAM the smallest element of data is the binary bit. The capabilities and limitations of accessing RAM are processor specific. In general main memory is arranged as an array of locations beginning at address 0 (hexadecimal 0). Each location can store usually 8 or 32 bits depending on the computer architecture.\n\n\n=== Keys ===\nData keys need not be a direct hardware address in memory. Indirect, abstract and logical keys codes can be stored in association with values to form a data structure. Data structures have predetermined offsets (or links or paths) from the start of the structure, in which data values are stored. Therefore, the data key consists of the key to the structure plus the offset (or links or paths) into the structure. When such a structure is repeated, storing variations of the data values and the data keys within the same repeating structure, the result can be considered to resemble a table, in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table. In such an organization of data, the data key is usually a value in one (or a composite of the values in several) of the columns.\n\n\n=== Organised recurring data structures ===\nThe tabular view of repeating data structures is only one of many possibilities. Repeating data structures can be organised hierarchically, such that nodes are linked to each other in a cascade of parent-child relationships. Values and potentially more complex data-structures are linked to the nodes. Thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes. This representation can be thought of as an inverted tree. Modern computer operating system file systems are a common example; and XML is another.\n\n\n=== Sorted or ordered data ===\nData has some inherent features when it is sorted on a key. All the values for subsets of the key appear together. When passing sequentially through groups of the data with the same key, or a subset of the key changes, this is referred to in data processing circles as a break, or a control break. It particularly facilitates the aggregation of data values on subsets of a key.\n\n\n=== Peripheral storage ===\nUntil the advent of bulk non-volatile memory like flash, persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives. These devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size. In this case, the seek location on the media, is the data key and the blocks are the data values. Early used raw disk data file-systems or disc operating systems reserved contiguous blocks on the disc drive for data files. In those systems, the files could be filled up, running out of data space before all the data had been written to them. Thus much unused data space was reserved unproductively to ensure adequate free space for each file. Later file-systems introduced partitions. They reserved blocks of disc data space for partitions and used the allocated blocks more economically, by dynamically assigning blocks of a partition to a file as needed. To achieve this, the file system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table. Though this made better use of the disc data space, it resulted in fragmentation of files across the disc, and a concomitant performance overhead due additional seek time to read the data. Modern file systems reorganize fragmented files dynamically to optimize file access times. Further developments in file systems resulted in virtualization of disc drives i.e. where a logical drive can be defined as partitions from a number of physical drives.\n\n\n=== Indexed data ===\nRetrieving a small subset of data from a much larger set may imply inefficiently searching through the data sequentially. Indexes are a way to copy out keys and location addresses from data structures in files, tables and data sets, then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data. In order to do this, the key of the subset of data to be retrieved must be known before retrieval begins. The most popular indexes are the B-tree and the dynamic hash key indexing methods. Indexing is overhead for filing and retrieving data. There are other ways of organizing indexes, e.g. sorting the keys and using a binary search algorithm.\n\n\n=== Abstraction and indirection ===\nObject-oriented programming uses two basic concepts for understanding data and software:\nThe taxonomic rank-structure of classes, which is an example of a hierarchical data structure; and\nat run time, the creation of references to in-memory data-structures of objects that have been instantiated from a class library.\nIt is only after instantiation that an object of a specified class exists. After an object's reference is cleared, the object also ceases to exist. The memory locations where the object's data was stored are garbage and are reclassified as unused memory available for reuse.\n\n\n=== Database data ===\nThe advent of databases introduced a further layer of abstraction for persistent data storage. Databases use metadata, and a structured query language protocol between client and server systems, communicating over a computer network, using a two phase commit logging system to ensure transactional completeness, when saving data.\n\n\n=== Parallel distributed data processing ===\nModern scalable and high-performance data persistence technologies, such as Apache Hadoop, rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network. In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly. This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time.\n\n\n== See also ==\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Data_(computer_science)"}, "Data type": {"title": "Data type", "content": "In computer science and computer programming, a data type (or simply type) is a collection or grouping of data values, usually specified by a set of possible values, a set of allowed operations on these values, and/or a representation of these values as machine types. A data type specification in a program constrains the possible values that an expression, such as a variable or a function call, might take. On literal data, it tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans.\n\n\n== Concept ==\nA data type may be specified for many reasons: similarity, convenience, or to focus the attention. It is frequently a matter of good organization\nthat aids the understanding of complex definitions. Almost all programming languages explicitly include the notion of data type, though the possible data types are often restricted by considerations of simplicity, computability, or regularity. An explicit data type declaration typically allows the compiler to choose an efficient machine representation, but the conceptual organization offered by data types should not be discounted.\nDifferent languages may use different data types or similar types with different semantics. For example, in the Python programming language, int represents an arbitrary-precision integer which has the traditional numeric operations such as addition, subtraction, and multiplication. However, in the Java programming language, the type int represents the set of 32-bit integers ranging in value from \u22122,147,483,648 to 2,147,483,647, with arithmetic operations that wrap on overflow. In Rust this 32-bit integer type is denoted i32 and panics on overflow in debug mode.\nMost programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type.  For example, a programmer might create a new data type named \"complex number\" that would include real and imaginary parts, or a color data type represented by three bytes denoting the amounts each of red, green, and blue, and a string representing the color's name.\nData types are used within type systems, which offer various ways of defining, implementing, and using them. In a type system, a data type represents a constraint placed upon the interpretation of data, describing representation, interpretation and structure of values or objects stored in computer memory. The type system uses data type information to check correctness of computer programs that access or manipulate the data. A compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).\nMost data types in statistics have comparable types in computer programming, and vice versa, as shown in the following table:\n\n\n== Definition ==\nParnas, Shore & Weiss (1976) identified five definitions of a \"type\" that were used\u2014sometimes implicitly\u2014in the literature:\n\nSyntactic\nA type is a purely syntactic label associated with a variable when it is declared. Although useful for advanced type systems such as substructural type systems, such definitions provide no intuitive meaning of the types.\nRepresentation\nA type is defined in terms of a composition of more primitive types\u2014often machine types.\nRepresentation and behaviour\nA type is defined as its representation and a set of operators manipulating these representations.\nValue space\nA type is a set of possible values which a variable can possess. Such definitions make it possible to speak about (disjoint) unions or Cartesian products of types.\nValue space and behaviour\nA type is a set of values which a variable can possess and a set of functions that one can apply to these values.\nThe definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU. Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.\n\n\n== Classification ==\nData types may be categorized according to several factors:\n\nPrimitive data types or built-in data types are types that are built-in to a language implementation. User-defined data types are non-primitive types. For example, Java's numeric types are primitive, while classes are user-defined.\nA value of an atomic type is a single data item that cannot be broken into component parts. A value of a composite type  or aggregate type is a collection of data items that can be accessed individually. For example, an integer is generally considered atomic, although it consists of a sequence of bits, while an array of integers is certainly composite.\nBasic data types or fundamental data types are defined axiomatically from fundamental notions or by enumeration of their elements. Generated data types or derived data types are specified, and partly defined, in terms of other data types. All basic types are atomic. For example, integers are a basic type defined in mathematics, while an array of integers is the result of applying an array type generator to the integer type.\nThe terminology varies - in the literature, primitive, built-in, basic, atomic, and fundamental may be used interchangeably.\n\n\n== Examples ==\n\n\n=== Machine data types ===\nAll data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level. The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits). The unit processed by machine code instructions is called a word (as of 2011, typically 32 or 64 bits).\nMachine data types expose or make available fine-grained control over hardware, but this can also expose implementation details that make code less portable. Hence machine types are mainly used in systems programming or low-level programming languages. In higher-level languages most data types are abstracted in that they do not have a language-defined machine representation. The C programming language, for instance, supplies types such as Booleans, integers, floating-point numbers, etc., but the precise bit representations of these types are implementation-defined. The only C type with a precise machine representation is the char type that represents a byte.\n\n\n=== Boolean type ===\nThe Boolean type represents the values true and false. Although only two values are possible, they are more often represented as a word rather as a single bit as it requires more machine instructions to store and retrieve an individual bit. Many programming languages do not have an explicit Boolean type, instead using an integer type and interpreting (for instance) 0 as false and other values as true.\nBoolean data refers to the logical structure of how the language is interpreted to the machine language. In this case a Boolean 0 refers to the logic False. True is always a non zero, especially a one which is known as Boolean 1.\n\n\n=== Numeric types ===\nAlmost all programming languages supply one or more integer data types. They may either supply a small number of predefined subtypes restricted to certain ranges (such as short and long and their corresponding unsigned variants in C/C++); or allow users to freely define subranges such as 1..12 (e.g. Pascal/Ada). If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.\nFloating point data types represent certain fractional values (rational numbers, mathematically). Although they have predefined limits on both their maximum values and their precision, they are sometimes misleadingly called reals (evocative of mathematical real numbers). They are typically stored internally in the form a \u00d7 2b (where a and b are integers), but displayed in familiar decimal form.\nFixed point data types are convenient for representing monetary values. They are often implemented internally as integers, leading to predefined limits.\nFor independence from architecture details, a Bignum or arbitrary precision numeric type might be supplied. This represents an integer or rational to a precision limited only by the available memory and computational resources on the system. Bignum implementations of arithmetic operations on machine-sized values are significantly slower than the corresponding machine operations.\n\n\n=== Enumerations ===\nThe enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named CLUB, DIAMOND, HEART, SPADE, belonging to an enumerated type named suit.  If a variable V is declared having suit as its data type, one can assign any of those four values to it. Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers.\n\n\n=== String and text types ===\nStrings are a sequence of characters used to store words or plain text, most often textual markup languages representing formatted text. Characters may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc. Characters are drawn from a character set such as ASCII. Character and string types can have different subtypes according to the character encoding. The original 7-bit wide ASCII was found to be limited, and superseded by 8, 16 and 32-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols. Strings may be of either variable length or fixed length, and some programming languages have both types. They may also be subtyped by their maximum size.\nSince most character sets include the digits, it is possible to have a numeric string, such as \"1234\". These numeric strings are usually considered distinct from numeric values such as 1234, although some languages automatically convert between them.\n\n\n=== Union types ===\n\nA union type definition will specify which of a number of permitted subtypes may be stored in its instances, e.g. \"float or long integer\". In contrast with a record, which could be defined to contain a float and an integer, a union may only contain one subtype at a time.\nA tagged union (also called a variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type for enhanced type safety.\n\n\n=== Algebraic data types ===\n\nAn algebraic data type (ADT) is a possibly recursive sum type of product types. A value of an ADT consists of a constructor tag together with zero or more field values, with the number and type of the field values fixed by the constructor. The set of all possible values of an ADT is the set-theoretic disjoint union (sum), of the sets of all possible values of its variants (product of fields). Values of algebraic types are analyzed with pattern matching, which identifies a value's constructor and extracts the fields it contains.\nIf there is only one constructor, then the ADT corresponds to a product type similar to a tuple or record. A constructor with no fields corresponds to the empty product (unit type). If all constructors have no fields then the ADT corresponds to an enumerated type.\nOne common ADT is the option type, defined in Haskell as data Maybe a = Nothing | Just a.\n\n\n=== Data structures ===\nSome types are very useful for storing and retrieving data and are called data structures. Common data structures include:\n\nAn array (also called vector, list, or sequence) stores a number of elements and provides random access to individual elements. The elements of an array are typically (but not in all contexts) required to be of the same type. Arrays may be fixed-length or expandable. Indices into an array are typically required to be integers (if not, one may stress this relaxation by speaking about an associative array) from a specific range (if not all indices in that range correspond to elements, it may be a sparse array).\nRecord (also called tuple or struct) Records are among the simplest data structures. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members.\nAn object contains a number of data fields, like a record, and also offers a number of subroutines for accessing or modifying them, called methods.\nthe singly linked list, which can be used to implement a queue and is defined in Haskell as the ADT data List a = Nil | Cons a (List a), and\nthe binary tree, which allows fast searching, and can be defined in Haskell as the ADT data BTree a = Nil | Node (BTree a) a (BTree a)\n\n\n=== Abstract data types ===\n\nAn abstract data type is a data type that does not specify the concrete representation of the data.  Instead, a formal specification based on the data type's operations is used to describe it. Any implementation of a specification must fulfill the rules given. For example, a stack has push/pop operations that follow a Last-In-First-Out rule, and can be concretely implemented using either a list or an array. Abstract data types are used in formal semantics and program verification and, less strictly, in design.\n\n\n=== Pointers and references ===\n\nThe main non-composite, derived type is the pointer, a data type whose value refers directly to (or \"points to\") another value stored elsewhere in the computer memory using its address. It is a primitive kind of reference. (In everyday terms, a page number in a book could be considered a piece of data that refers to another one). Pointers are often stored in a format similar to an integer; however, attempting to dereference or \"look up\" a pointer whose value was never a valid memory address would cause a program to crash. To ameliorate this potential problem, a pointer type is typically considered distinct from the corresponding integer type, even if the underlying representation is the same.\n\n\n=== Function types ===\n\nFunctional programming languages treat functions as a distinct datatype and allow values of this type to be stored in variables and passed to functions. Some multi-paradigm languages such as JavaScript also have mechanisms for treating functions as data. Most contemporary type systems go beyond JavaScript's simple type \"function object\" and have a family of function types differentiated by argument and return types, such as the type Int -> Bool denoting functions taking an integer and returning a Boolean. In C, a function is not a first-class data type but function pointers can be manipulated by the program. Java and C++ originally did not have function values but have added them in C++11 and Java 8.\n\n\n=== Type constructors ===\n\nA type constructor builds new types from old ones, and can be thought of as an operator taking zero or more types as arguments and producing a type. Product types, function types, power types and list types can be made into type constructors.\n\n\n=== Quantified types ===\nUniversally-quantified and existentially-quantified types are based on predicate logic. Universal quantification is written as \n  \n    \n      \n        \u2200\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\forall x.f(x)}\n  \n or forall x. f x and is the intersection over all types x of the body f x, i.e. the value is of type f x for every x. Existential quantification written as \n  \n    \n      \n        \u2203\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\exists x.f(x)}\n  \n or exists x. f x and is the union over all types x of the body f x, i.e. the value is of type f x for some x.\nIn Haskell, universal quantification is commonly used, but existential types must be encoded by transforming exists a. f a to forall r. (forall a. f a -> r) -> r or a similar type.\n\n\n=== Refinement types ===\n\nA refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type. For instance, the type of natural numbers greater than 5 may be written as \n  \n    \n      \n        {\n        n\n        \u2208\n        \n          N\n        \n        \n        \n          |\n        \n        \n        n\n        >\n        5\n        }\n      \n    \n    {\\displaystyle \\{n\\in \\mathbb {N} \\,|\\,n>5\\}}\n  \n\n\n=== Dependent types ===\n\nA dependent type is a type whose definition depends on a value. Two common examples of dependent types are dependent functions and dependent pairs. The return type of a dependent function may depend on the value (not just type) of one of its arguments. A dependent pair may have a second value of which the type depends on the first value.\n\n\n=== Intersection types ===\n\nAn intersection type is a type containing those values that are members of two specified types. For example, in Java the class Boolean implements both the Serializable and the Comparable interfaces. Therefore, an object of type Boolean is a member of the type Serializable & Comparable. Considering types as sets of values, the intersection type \n  \n    \n      \n        \u03c3\n        \u2229\n        \u03c4\n      \n    \n    {\\displaystyle \\sigma \\cap \\tau }\n  \n is the set-theoretic intersection of \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  \n and \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  \n. It is also possible to define a dependent intersection type, denoted \n  \n    \n      \n        (\n        x\n        :\n        \u03c3\n        )\n        \u2229\n        \u03c4\n      \n    \n    {\\displaystyle (x:\\sigma )\\cap \\tau }\n  \n, where the type \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  \n may depend on the term variable \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n.\n\n\n=== Meta types ===\n\nSome programming languages represent the type information as data, enabling type introspection and reflection. In contrast, higher order type systems, while allowing types to be constructed from other types and passed to functions as values, typically avoid basing computational decisions on them.\n\n\n=== Convenience types ===\nFor convenience, high-level languages and databases may supply ready-made \"real world\" data types, for instance times, dates, and monetary values (currency). These may be built-in to the language or implemented as composite types in a library.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nParnas, David L.; Shore, John E.; Weiss, David (1976). \"Abstract types defined as classes of variables\". Proceedings of the 1976 conference on Data : Abstraction, definition and structure -. pp. 149\u2013154. doi:10.1145/800237.807133. S2CID 14448258.\nCardelli, Luca; Wegner, Peter (December 1985). \"On Understanding Types, Data Abstraction, and Polymorphism\" (PDF). ACM Computing Surveys. 17 (4): 471\u2013523. CiteSeerX 10.1.1.117.695. doi:10.1145/6041.6042. ISSN 0360-0300. S2CID 2921816. Archived (PDF) from the original on 2008-12-03.\nCleaveland, J. Craig (1986). An Introduction to Data Types. Addison-Wesley. ISBN 978-0201119404.\n\n\n== External links ==\n Media related to Data types at Wikimedia Commons", "link": "https://en.wikipedia.org/wiki/Data_type"}, "Data analysis": {"title": "Data analysis", "content": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.\nData mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.\nData integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.\n\n\n== Data analysis process ==\n\nAnalysis refers to dividing a whole into its separate components for individual examination. Data analysis is a process for obtaining raw data, and subsequently converting it into information useful for decision-making by users. Data is collected and analyzed to answer questions, test hypotheses, or disprove theories.\n\nStatistician John Tukey, defined data analysis in 1961, as:\"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases. The CRISP framework, used in data mining, has similar steps.\n\n\n=== Data requirements ===\nThe data is necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analytics (or customers, who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained.  Data may be numerical or categorical (i.e., a text label for numbers).\n\n\n=== Data collection ===\nData is collected from a variety of sources. A list of data sources are available for study & research. The requirements may be communicated by analysts to custodians of the data; such as, Information Technology personnel within an organization. Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. The data may also be collected from sensors in the environment, including traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.\n\n\n=== Data processing ===\n\nData, when initially obtained, must be processed or organized for analysis. For instance, these may involve placing data into rows and columns in a table format (known as structured data) for further analysis, often through the use of spreadsheet or statistical software.\n\n\n=== Data cleaning ===\n\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that the datum are entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example; with financial information, the totals for particular variables may be compared against separately published numbers that are believed to be reliable. Unusual amounts, above or below predetermined thresholds, may also be reviewed.  There are several types of data cleaning, that are dependent upon the type of data in the set; this could be phone numbers, email addresses, employers, or other values. Quantitative data methods for outlier detection, can be used to get rid of data that appears to have a higher likelihood of being input incorrectly. Textual data spell checkers can be used to lessen the amount of mistyped words. However, it is harder to tell if the words themselves are correct.\n\n\n=== Exploratory data analysis ===\nOnce the datasets are cleaned, they can then be analyzed. Analysts may apply a variety of techniques, referred to as exploratory data analysis, to begin understanding the messages contained within the obtained data. The process of data exploration may result in additional data cleaning or additional requests for data; thus, the initialization of the iterative phases mentioned in the lead paragraph of this section. Descriptive statistics, such as, the average or median, can be generated to aid in understanding the data. Data visualization is also a technique used, in which the analyst is able to examine the data in a graphical format in order to obtain additional insights, regarding the messages within the data.\n\n\n=== Modeling and algorithms ===\nMathematical formulas or models (also known as algorithms), may be applied to the data in order to identify relationships among the variables; for example, using correlation or causation. In general terms, models may be developed to evaluate a specific variable based on other variable(s) contained within the dataset, with some residual error depending on the implemented model's accuracy (e.g., Data = Model + Error).\nInferential statistics includes utilizing techniques that measure the relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X), provides an explanation for the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as (Y = aX + b + error), where the model is designed such that (a) and (b) minimize the error when the model predicts Y for a given range of values of X. Analysts may also attempt to build models that are descriptive of the data, in an aim to simplify analysis and communicate results.\n\n\n=== Data product ===\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. For instance, an application that analyzes data about customer purchase history, and uses the results to recommend other purchases the customer might enjoy.\n\n\n=== Communication ===\n\nOnce data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.\nWhen determining how to communicate the results, the analyst may consider implementing a variety of data visualization techniques to help communicate the message more clearly and efficiently to the audience. Data visualization uses information displays (graphics such as, tables and charts) to help communicate key messages contained in the data. Tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers; while charts (e.g., bar charts or line charts), may help explain the quantitative messages contained in the data.\n\n\n== Quantitative messages ==\n\nStephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\n\nTime-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.\nRanking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by salespersons (the category, with each salesperson a categorical subdivision) during a single period.  A bar chart may be used to show the comparison across the salespersons.\nPart-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%).  A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.\nDeviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period.  A bar chart can show the comparison of the actual versus the reference amount.\nFrequency distribution: Shows the number of observations of a particular variable for a given interval, such as the number of years in which the stock market return is between intervals such as 0\u201310%, 11\u201320%, etc. A histogram, a type of bar chart, may be used for this analysis.\nCorrelation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.\nNominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.\nGeographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.\n\n\n== Analyzing quantitative data ==\n\nAuthor Jonathan Koomey has recommended a series of best practices for understanding quantitative data.  These include:\n\nCheck raw data for anomalies prior to performing an analysis;\nRe-perform important calculations, such as verifying columns of data that are formula driven;\nConfirm main totals are the sum of subtotals;\nCheck relationships between numbers that should be related in a predictable way, such as ratios over time;\nNormalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;\nBreak problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.\nFor the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.\n\n The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as \"Mutually Exclusive and Collectively Exhaustive\" or MECE.  For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as the revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).\nAnalysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.\nRegression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.\nNecessary condition analysis (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.\n\n\n== Analytical activities of data users ==\n\nUsers may have particular data points of interest within a data set, as opposed to the general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.\n\n\n== Barriers to effective analysis ==\nBarriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.\n\n\n=== Confusing fact and opinion ===\n\nEffective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011\u20132020 time period would add approximately $3.3 trillion to the national debt. Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.\nAs another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects\". This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\n\n\n=== Cognitive biases ===\nThere are a variety of cognitive biases that can adversely affect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.\nAnalysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.\n\n\n=== Innumeracy ===\nEffective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate.  Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.\nFor example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.\nAnalysts may also analyze data under different assumptions or scenario. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock.  Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.\n\n\n== Other topics ==\n\n\n=== Smart buildings ===\nA data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.\n\n\n=== Analytics and business intelligence ===\n\nAnalytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that uses data to understand and analyze business performance to drive decision-making .\n\n\n=== Education ===\nIn education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators\u2019 data analyses.\n\n\n== Practitioner notes ==\nThis section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.\n\n\n=== Initial data analysis ===\nThe most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:\n\n\n==== Quality of data ====\nThe quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms), normal imputation is needed.\n\nAnalysis of extreme observations: outlying observations in the data are analyzed to see if they seem to disturb the distribution.\nComparison and correction of differences in coding schemes: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.\nTest for common-method variance.\nThe choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.\n\n\n==== Quality of measurements ====\nThe quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.\nThere are two ways to assess measurement quality:\n\nConfirmatory factor analysis\nAnalysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach's \u03b1 of the scales, and the change in the Cronbach's alpha when an item would be deleted from a scale\n\n\n==== Initial transformations ====\nAfter assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.\nPossible transformations of variables are:\n\nSquare root transformation (if the distribution differs moderately from normal)\nLog-transformation (if the distribution differs substantially from normal)\nInverse transformation (if the distribution differs severely from normal)\nMake categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)\n\n\n==== Did the implementation of the study fulfill the intentions of the research design? ====\nOne should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.Other possible data distortions that should be checked are:\n\ndropout (this should be identified during the initial data analysis phase)\nItem non-response (whether this is random or not should be assessed during the initial data analysis phase)\nTreatment quality (using manipulation checks).\n\n\n==== Characteristics of data sample ====\nIn any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.The characteristics of the data sample can be assessed by looking at:\n\nBasic statistics of important variables\nScatter plots\nCorrelations and associations\nCross-tabulations\n\n\n==== Final stage of the initial data analysis ====\nDuring the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:\n\nIn the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?\nIn the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?\nIn the case of outliers: should one use robust analysis techniques?\nIn case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?\nIn the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?\nIn case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?\n\n\n==== Analysis ====\nSeveral analyses can be used during the initial data analysis phase:\n\nUnivariate statistics (single variable)\nBivariate associations (correlations)\nGraphical techniques (scatter plots)\nIt is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:\n\nNominal and ordinal variables\nFrequency counts (numbers and percentages)\nAssociations\ncircumambulations (crosstabulations)\nhierarchical loglinear analysis (restricted to a maximum of 8 variables)\nloglinear analysis (to identify relevant/important variables and possible confounders)\nExact tests or bootstrapping (in case subgroups are small)\nComputation of new variables\nContinuous variables\nDistribution\nStatistics (M, SD, variance, skewness, kurtosis)\nStem-and-leaf displays\nBox plots\n\n\n==== Nonlinear analysis ====\nNonlinear analysis is often necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods.  Nonlinear data analysis is closely related to nonlinear system identification.\n\n\n=== Main data analysis ===\nIn the main analysis phase, analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.\n\n\n==== Exploratory and confirmatory approaches ====\nIn the main analysis phase, either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.\nExploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.\n\n\n==== Stability of results ====\nIt is important to obtain some indication about how generalizable the results are. While this is often difficult to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing that.\n\nCross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.\nSensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.\n\n\n== Free software for data analysis ==\nNotable free software for data analysis include:\n\nDevInfo \u2013 A database system endorsed by the United Nations Development Group for monitoring and analyzing human development.\nELKI \u2013 Data mining framework in Java with data mining oriented visualization functions.\nKNIME \u2013 The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\nOrange \u2013 A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.\nPandas \u2013 Python library for data analysis.\nPAW \u2013 FORTRAN/C data analysis framework developed at CERN.\nR \u2013 A programming language and software environment for statistical computing and graphics.\nROOT \u2013  C++ data analysis framework developed at CERN.\nSciPy \u2013 Python library for scientific computing.\nJulia \u2013 A programming language well-suited for numerical analysis and computational science.\n\n\n== Reproducible analysis ==\nThe typical data analysis workflow involves collecting data, running analyses through various scripts, creating visualizations, and writing reports. However, this workflow presents challenges, including a separation between analysis scripts and data, as well as a gap between analysis and documentation. Often, the correct order of running scripts is only described informally or resides in the data scientist's memory. The potential for losing this information creates issues for reproducibility. To address these challenges, it is essential to have analysis scripts written for automated, reproducible workflows. Additionally, dynamic documentation is crucial, providing reports that are understandable by both machines and humans, ensuring accurate representation of the analysis workflow even as scripts evolve.\n\n\n== International data analysis contests ==\nDifferent companies or organizations hold data analysis contests to encourage researchers to utilize their data or to solve a particular question using data analysis. A few examples of well-known international data analysis contests are as follows: \n\nKaggle competition, which is held by Kaggle.\nLTPP data analysis contest held by FHWA and ASCE.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Bibliography ===\nAd\u00e8r, Herman J. (2008a). \"Chapter 14: Phases and initial steps in data analysis\". In Ad\u00e8r, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 333\u2013356. ISBN 9789079418015. OCLC 905799857.\nAd\u00e8r, Herman J. (2008b). \"Chapter 15: The main analysis phase\". In Ad\u00e8r, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 357\u2013386. ISBN 9789079418015. OCLC 905799857.\nTabachnick, B.G. & Fidell, L.S. (2007). Chapter 4: Cleaning up your act. Screening data prior to analysis. In B.G. Tabachnick & L.S. Fidell (Eds.), Using Multivariate Statistics, Fifth Edition (pp. 60\u2013116). Boston: Pearson Education, Inc. / Allyn and Bacon.\n\n\n== Further reading ==\n\nAd\u00e8r, H.J. & Mellenbergh, G.J. (with contributions by D.J. Hand) (2008). Advising on Research Methods: A Consultant's Companion. Huizen, the Netherlands: Johannes van Kessel Publishing.  ISBN 978-90-79418-01-5\nChambers, John M.; Cleveland, William S.; Kleiner, Beat; Tukey, Paul A. (1983). Graphical Methods for Data Analysis, Wadsworth/Duxbury Press. ISBN 0-534-98052-X\nFandango, Armando (2017). Python Data Analysis, 2nd Edition. Packt Publishers. ISBN 978-1787127487\nJuran, Joseph M.; Godfrey, A. Blanton (1999). Juran's Quality Handbook, 5th Edition. New York: McGraw Hill. ISBN 0-07-034003-X\nLewis-Beck, Michael S. (1995). Data Analysis: an Introduction, Sage Publications Inc, ISBN 0-8039-5772-6\nNIST/SEMATECH (2008) Handbook of Statistical Methods,\nPyzdek, T, (2003). Quality Engineering Handbook, ISBN 0-8247-4614-7\nRichard Veryard (1984). Pragmatic Data Analysis. Oxford : Blackwell Scientific Publications. ISBN 0-632-01311-7\nTabachnick, B.G.; Fidell, L.S. (2007). Using Multivariate Statistics, 5th Edition. Boston: Pearson Education, Inc. / Allyn and Bacon, ISBN 978-0-205-45938-4", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, "Social data science": {"title": "Social data science", "content": "Social data science is an interdisciplinary field that addresses social science problems by applying or designing computational and digital methods. As the name implies, Social Data Science is located primarily within the social science, but it relies on technical advances in fields like data science, network science, and computer science. The data in Social Data Science is always about human beings and derives from social phenomena, and it could be structured data (e.g. surveys) or unstructured data (e.g. digital footprints). The goal of Social Data Science is to yield new knowledge about social networks, human behavior, cultural ideas and political ideologies.\nA social data scientist combines cdomain knowledge and specialized theories from the social sciences with programming, statistical and other data analysis skills.\n\n\n== Methods ==\nSocial data science employs a wide range of quantitative - both established methods in social science as well as new methods developed in computer science and interdisciplinary data science fields such as natural language processing (NLP) and network science.\nSocial Data Science is closely related to Computational Social Science, but also sometimes includes qualitative data, and mixed digital methods.\nCommon social data science methods include:\nQuantitative methods:\n\nMachine learning\nDeep learning\nSocial network analysis\nRandomized controlled trials\nNatural language processing (NLP), especially through text as data.\nsurveys\nQualitative methods:\n\nInterviewing\nObservation\nEthnography\nContent analysis\nDiscourse analysis\nMixed digital methods:\n\nControversy mapping\nSpatial analysis\nQuali-quantitative methods \nComputational ethnography\nOne of the pillars of social data science is in the combination of qualitative and quantitative data to analyze social phenomena and develop computationally grounded theories. For example, by using mixed methods to digitize qualitative data and analyzing it via computational methods, or by qualitatively analyzing and interpreting quantitative data.\n\n\n=== Data ===\nSocial data scientists use both digitized data  (e.g. old books that have been digitized) and natively digital data (e.g. social media posts). Since such data often take the form of found data that were originally produced for other purposes (commercial, governance, etc.) than research, data scraping, cleaning and other forms of preprocessing and data mining occupy a substantial part of a social data scientist's job.\nSources of SDS data include:\n\nText data\nSensor data\nRegister data\nSurvey data\nGeo-location data\nObservational data\n\n\n== Relations to other fields ==\n\n\n=== Social sciences ===\nSocial data science is part of the social sciences along with established disciplines (anthropology, economics, political science, psychology, and sociology) and newer interdisciplinary fields like behavioral science, criminology, international relations, and cognitive science. As such, its fundamental unit of study is social relations, human behavior and cultural ideas, which it investigates by using quantitative and/or qualitative data and methods to develop, test and improve fundamental theories concerning the nature of the human condition. SDS also differs from traditional social science in two ways.\n\nFirst, its primary object is digitized phenomena and data in the widest sense of this word, ranging from digitized text corpora to the footprints gathered by digital platforms and sensors.\nSecondly, more than simply applying existing quantitative and qualitative social science methods, social data science seeks to develop and disrupt these via the import and integration of state of the art of data science techniques \n\n\n=== Data Science ===\nSocial data science is a form of data science in that it applies advanced computational methods and statistics to gain information and insights from data. Social data science researchers often make use of methods developed by data scientists, such as data mining and machine learning, which includes but is not limited to the extraction and processing of information from big data sources. Unlike the broader field of data science, which involves any application and study involving the combination of computational and statistical methods, social data science mainly concerns the scientific study of digital social data and/or digital footprints from human behavior.\n\n\n=== Computational Social Science ===\nLike computational social science, social data science uses data science methods to solve social science problems. This includes the reappropriation and refinement of methods developed by data scientists to better fit the questions and data of the social sciences as well as their specialized domain knowledge and theories. Unlike computational social science, social data science also includes critical studies of how digital platforms and computational processes affect wider society and of how computational and non-computational approaches integrate and combine.\n\n\n=== Digital Methods ===\nWhile most social data science researchers are closely affiliated with or part of computational social science, some qualitative oriented social data scientists are influenced by the fields of digital humanities and digital methods that emerged from science and technology studies (STS). Like digital methods, the aim is here to repurpose the 'methods of the medium' to study digitally-mediated society and to engage in an ongoing discussions about bias in science and society by bringing computational social science and Digital Methods into dialogue. SDS is also related to digital sociology and digital anthropology, but to a higher degree aspires to augment qualitative data and digital methods with state of the art data science techniques.\n\n\n== History of the field ==\nThe origin of term \"social data science\" coincided with the emergence of a number of research centers and degree programs. In 2016, the Copenhagen Center for Social Data Science (SODAS) - the first academic institution using the SDS name - was launched at the University of Copenhagen. The plan for an interdisciplinary center working at the intersection of the social and computational sciences was rooted in the Copenhagen Networks Study  from 2011 to 2016 by researchers from the Technical University of Denmark (DTU) and the University of Copenhagen. The University of Oxford and the University of Copenhagen were among the first research institutions to offer degree programmes in SDS. In 2018, the University of Oxford launched the one-year MSc in Social Data Science,  which was followed by the two-year master's programme at the University of Copenhagen in 2020. Since then, an increasing number of universities have begun to offer graduate programs or specializations in social data science\nSocial data science has emerged after the increasing availability of digitized social data, sometimes referred to as Big Data, and the ability to apply computational methods to this data at a low cost, which has offered novel opportunities to  address questions about social phenomena and human behavior. While the origin of social data can be traced back to 1890s (when some 15 million individual records were processed by the US Census in the form of punch cards), the social data boom in the 21th century is a direct consequence of the increasing availability of consumer data resulting from the advent of e-commerce Subsequent waves of availability of unstructured social data include Amazon.com review system and Wikipedia, and more recently, social media, which has played a key role in the emergence of the digital attention economy and big tech.\n\n\n== Criticism and debates ==\nData scientists have played a vital role in the data revolution, both during the original tech-optimist phase where big data and the Internet was seen as the solution to many societal and scientific problems, and as participants  in the tech-lash that followed in its wake as result of, among other things, the Facebook\u2013Cambridge Analytica data scandal. Social data science researchers and research projects have been especially impactful in debates and criticism revolving around:\n\nSurveillance capitalism\nDigital disinformation\nAlgorithmic bias\nThe replication and validity crisis on the social sciences \nEthics and privacy\nData governance\n\n\n== Impact and examples ==\nSocial data science research is typically published in multidisciplinary journals, including top general journals Science, Nature, and PNAS, as well as notable specialized journals such as:\n\nNature Human Behaviour\nNature Computational Science\nThe Journal of Computational Social Science\nBig Data and Society\nScience Advances\nNature Communications\nScientific Reports\nPLOS ONE\nIn addition, social data science research is published in the top social science field journals including American Sociological Review, Psychological Science, American Economic Review, Current Anthropology.\n\n\n== Education and Research Institutions ==\nThere are multiple specific definitions of social data science, but several institutions around the world currently offer degree and research programs under the rubric of Social Data Science.\n\n\n=== Education ===\nM.Sc. in Social Data Science  - University of Copenhagen\nMSc in Social Data Science  - University of Oxford\nMSc in Social and Economic Data Science (SEDS)  - University of Konstanz\nBSc in Social Data Science  - University of Hong Kong\nP.Grad.Dip in Social Data Science  - University of Dublin\nMSc Applied Social Data Science  - The London School of Economics\nMaster of Science in Social Data Science  - Central European University\nMSc Social Data Science  - University of Essex\nMSc in Techno-Anthropology  - University of Aalborg\nMSc Social Data Science  - University College Dublin\nBSc in Social Data Science  - Witten/Herdecke University\nQuantitative Analysis and Social data Science (QASS)  - KU Leuven\nHuman and Social Data Science MSc  - University of Sussex\n\n\n=== Research ===\nCopenhagen Center for Social Data Science (SODAS)  - University of Copenhagen\nCenter for Social Data Science  - University of Helsinki\nSocial Data Science Lab  - Cardiff University\nSoDa Laboratories  - Monash University\nMannheim Center for Data Science  - University of Mannheim\nSocial & Behavioral Data Science Centre (SoBe DSC)  - University of Amsterdam\nSocial Data Science  - Alan Turing Institute\nSocial Data Science Center  - University of Maryland\nCentre for Social Data Analytics  - Auckland University of Technology\nMASSHINE  \u2013 Aalborg University\n\n\n== Professions and industry ==\nSocial data scientists are in high demand across society, specifically for employers valuing interdisciplinary skills, and can be found working as:\n\n1. Industry Researchers: Typical workplaces: governments, companies and corporations, independent research institutes, foundations, NGOs. Typical titles: researcher, data manager, data steward, data scientist, data engineer, consultant, manager, director, partner, politicians, data analyst, software developer, BI, UX, UI.Researcher\n2. Academic Researchers: Ph.D. Students, Researchers, Postdocs, Professors\n3. Entrepreneurs: Start your own business using social data science methods to solve real-world social problems. Typical titles: CTO, CEO, Chief Data Scientist\n\n\n== Sub Branches ==\nSocial data science is still a new field, with developing branches. Broadly speaking the field can be divided into a range of method-based sub-fields:\n\n\n=== Method-based sub-fields ===\nNetwork Science: Network analysis is often utilized to visualize or study network dynamics in social data science studies. This includes for instance social media networks.\nMixed Digital Methods: In computer-assisted qualitative analysis, the researcher often utilizes computational methods such as natural language processing techniques or topic modelling to explore a corpus of text, such as parliamentary speeches or Twitter data.\nMachine Learning for Causal Inference: The social sciences are often interested in finding causal relationships between variables. This is of special interest to social data science, where multiple fields of research try to establish appropriate policy responses to contemporary societal issues. Often, drawing from research from Judea Pearls directed acyclical graph approach and the Neymann-Rubin Causal model to inform whether there exists a causal relationship between two (or more) variables. Furthermore, incorporating machine learning into causal inference is of great interest.\nNatural Language Processing: Applied natural language processing is the adaptation and repurposing of methods from natural language processing and the application of these methods to questions of social behavior.\n\n\n=== Themes ===\nAlgorithmic Bias and Fairness: Considering how algorithms play a still larger role in humans everyday life, the study of fairness in this context has grown as a field. Especially whether miniorieties are negatively or positively impacted by these algorithms.\nPolarization and Misinformation: Many scholars use enormous amounts of granular data generated by social media and political agents to study social contagion, the spread of misinformation and disinformation. These studies often use text or social media interactions to explore how politicians and/or the public behave and interact in the digital and physical arena.\nClimate Social Data Science: The intersection between climate science, and digital (behavioral) data. This includes climate activism on social media and using digital trace data to investigate how people and societies are impacted by rising temperatures (CITE: Rising Temperature Erode Human Sleep Globally).\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Social_data_science"}, "Biomedical data science": {"title": "Biomedical data science", "content": "Biomedical data science is a multidisciplinary field which leverages large volumes of data to promote biomedical innovation and discovery. Biomedical data science draws from various fields including Biostatistics, Biomedical informatics, and machine learning, with the goal of understanding biological and medical data. It can be viewed as the study and application of data science to solve biomedical problems. Modern biomedical datasets often have specific features which make their analyses difficult, including:\n\nLarge numbers of feature (sometimes billions), typically far larger than the number of samples (typically tens or hundreds)\nNoisy and missing data\nPrivacy concerns (e.g., electronic health record confidentiality)\nRequirement of interpretability from decision makers and regulatory bodies\nMany biomedical data science projects apply machine learning to such datasets. These characteristics, while also present in many data science applications more generally, make biomedical data science a specific field. Examples of biomedical data science research include: \n\nComputational genomics\nComputational imaging\nElectronic health records data mining\nBiomedical network science\n\n\n== Training in Biomedical Data Science ==\nThe National Library of Medicine of the US National Institutes of Health (NIH) identified key biomedical data scientist attributes in an NIH-wide review: general biomedical subject matter knowledge; programming language expertise; predictive analytics, modeling, and machine learning; team science and communication; and responsible data stewardship.\n\n\n=== University Departments and Programs ===\nJohns Hopkins University\u2019s Department of Biomedical Engineering offers biomedical data science training at the undergraduate, master's, and PhD levels. They were the first university to offer programs at both undergraduate and graduate levels.\nDartmouth College's Geisel School of Medicine houses the Department of Biomedical Data Science where Quantitative Biomedical Sciences programs are available at the master's and PhD levels.\nImperial College London\u2019s Faculty of Medicine and Data Science Institute offer an MRes in Biomedical Research (Data Science).\nMount Sinai\u2019s Icahn School of Medicine offers a Master of Science in Biomedical Data Science.\nStanford University\u2019s Department of Biomedical Data Science offers multiple biomedical informatics graduate programs (MS, PhD, and MD/PhD).\nThe University of Exeter\u2019s College of Healthcare and Medicine offers an MSc in Health Data Science.\n\n\n== Biomedical Data Science Research in Academia ==\n\n\n=== Scholarly Journals ===\nThe first journal dedicated to biomedical data science appeared in 2018 \u2013 Annual Review of Biomedical Data Science. \u201cThe Annual Review of Biomedical Data Science provides comprehensive expert reviews in biomedical data science, focusing on advanced methods to store, retrieve, analyze, and organize biomedical data and knowledge. The scope of the journal encompasses informatics, computational, and statistical approaches to biomedical data, including the sub-fields of bioinformatics, computational biology, biomedical informatics, clinical and clinical research informatics, biostatistics, and imaging informatics. The mission of the journal is to identify both emerging and established areas of biomedical data science, and the leaders in these fields.\u201d \nOther journals have a more general scope than biomedical data science, but regularly publish biomedical data science research such as Health Data Science and Nature Machine Intelligence. Data science would not exist without curated datasets and the field has seen the rise of journals that are dedicated to describing and validating such datasets, some of which are useful for biomedical applications, including Scientific Data, Biomedical Data, and Data.\n\n\n== Example ==\nThe Human Genome Project (HGP), which uncovered the DNA sequences that compose human genes, would not have been possible without biomedical data science. Significant computational resources were required to process the data in the HGP, as the human genome contains over 6 billion DNA base pairs. Scientists constructed the genome by piecing together small fragments of DNA, and computing overlaps between these sequences alone required over 10,000 CPU hours. At this massive data scale, scientists relied on advanced algorithms to perform data processing steps such as sequence assembly and sequence alignment for quality control. Some of these algorithms, such as BLAST, are still used in modern bioinformatics. Scientists in the HGP also had to address complexities often associated with biomedical data including noisy data, such as DNA read errors, and privacy rights of the research subjects. The HGP, completed in 2004, has had immense impact both biologically, shedding light on human evolution, and medically, launching the field of bioinformatics and leading to technologies such as genetic screening and gene therapy.\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Biomedical_data_science"}, "Big data": {"title": "Big data", "content": "Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.\"\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17\u00d7260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than \u20ac100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"\n\n\n== Definition ==\nThe term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.  Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.  Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data. Big data \"size\" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.  Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.\n\"Volume\", \"variety\", \"velocity\", and various other \"Vs\" are added by some organizations to describe it, a revision challenged by some industry authorities. The Vs of big data were often referred to as the \"three Vs\", \"four Vs\", and \"five Vs\". They represented the qualities of big data in volume, variety, velocity, veracity, and value. Variability is often included as an additional quality of big data.\nA 2018 definition states \"Big data is where parallel computing tools are needed to handle data\", and notes, \"This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model.\"\nIn a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.\n\n\n=== Big data vs. business intelligence ===\nThe growing maturity of the concept more starkly delineates the difference between \"big data\" and \"business intelligence\":\n\nBusiness intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc.\nBig data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.\n\n\n== Characteristics ==\n\nBig data can be described by the following characteristics:\n\nVolume\nThe quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes.\nVariety\nThe type and nature of the data. Earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. Big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.\nVelocity\nThe speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing.\nVeracity\nThe truthfulness or reliability of the data, which refers to the data quality and the data value. Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it. The data quality of captured data can vary greatly, affecting an accurate analysis.\nValue\nThe worth in information that can be achieved by the processing and analysis of large datasets. Value also can be measured by an assessment of the other qualities of big data. Value may also represent the profitability of information that is retrieved from the analysis of big data.\nVariability\nThe characteristic of the changing formats, structure, or sources of big data. Big data can include structured, unstructured, or combinations of structured and unstructured data. Big data analysis may integrate raw data from multiple sources. The processing of raw data may also involve transformations of unstructured data to structured data.\nOther possible characteristics of big data are:\n\nExhaustive\nWhether the entire system (i.e., \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n  \n=all) is captured or recorded or not. Big data may or may not include all the available data from sources.\nFine-grained and uniquely lexical\nRespectively, the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified.\nRelational\nIf the data collected contains common fields that would enable a conjoining, or meta-analysis, of different data sets.\nExtensional\nIf new fields in each element of the data collected can be added or changed easily.\nScalability\nIf the size of the big data storage system can expand rapidly.\n\n\n== Architecture ==\nBig data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s. For many years, WinterCorp published the largest database report.\nTeradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added unstructured data types including XML, JSON, and Avro.\nIn 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.\nCERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current \"big data\" movement.\nIn 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the \"map\" step). The results are then gathered and delivered (the \"reduce\" step). The framework was very successful, so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named \"Hadoop\". Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds in-memory processing and the ability to set up many operations (not just map followed by reducing).\nMIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled \"Big Data Solution Offering\". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.\nStudies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.\nThe data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.\n\n\n== Technologies ==\nA 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:\n\nTechniques for analyzing data, such as A/B testing, machine learning, and natural language processing\nBig data technologies, like business intelligence, cloud computing, and databases\nVisualization, such as charts, graphs, and other displays of the data\nMultidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.\nAdditional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.\nSome MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.\nDARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called \"Ayasdi\".\nThe practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures\u2014storage area network (SAN) and network-attached storage (NAS)\u2014 is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.\nReal or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good\u2014data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.\n\n\n== Applications ==\n\nBig data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year, about twice as fast as the software business as a whole.\nDeveloped economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data, which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).\nWhile many vendors offer off-the-shelf products for big data, experts promote the development of in-house custom-tailored systems if the company has sufficient technical capabilities.\n\n\n=== Government ===\n\nThe use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation, but comes with flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.\nCivil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.\n\n\n=== International development ===\nResearch on the effective usage of information and communication technologies for development (also known as \"ICT4D\") suggests that big data technology can make important contributions but also present unique challenges to international development. Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management. Additionally, user-generated data offers new opportunities to give the unheard a voice. However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.  The challenge of \"big data for development\" is currently evolving toward the application of this data through machine learning, known as \"artificial intelligence for development (AI4D).\n\n\n==== Benefits ====\nA major practical application of big data for development has been \"fighting poverty with data\". In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata  and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty. Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues  argue that digital trace data has several benefits such as:\n\nThematic coverage: including areas that were previously difficult or impossible to measure\nGeographical coverage: providing sizable and comparable data for almost all countries, including many small countries that usually are not included in international inventories\nLevel of detail: providing fine-grained data with many interrelated variables, and new aspects, like network connections\nTimeliness and timeseries: graphs can be produced within days of being collected\n\n\n==== Challenges ====\nAt the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:\n\nRepresentativeness. While traditional development statistics is mainly concerned with the representativeness of random survey samples, digital trace data is never a random sample.\nGeneralizability. While observational data always represents this source very well, it only represents what it represents, and nothing more. While it is tempting to generalize from specific observations of one platform to broader settings, this is often very deceptive.\nHarmonization. Digital trace data still requires international harmonization of indicators. It adds the challenge of so-called \"data-fusion\", the harmonization of different sources.\nData overload. Analysts and institutions are not used to effectively deal with a large number of variables, which is efficiently done with interactive dashboards. Practitioners still lack a standard workflow that would allow researchers, users and policymakers to efficiently and effectively deal with data.\n\n\n=== Finance ===\nBig Data is being rapidly adopted in Finance to 1) speed up processing and 2) deliver better, more informed inferences, both internally and to the clients of the financial institutions. The financial applications of Big Data range from investing decisions and trading (processing volumes of available price data, limit order books, economic data and more, all at the same time), portfolio management (optimizing over an increasingly large array of financial instruments, potentially selected from different asset classes), risk management (credit rating based on extended information), and any other aspect where the data inputs are large. Big Data has also been a typical concept within the field of alternative financial service. Some of the major areas involve crowd-funding platforms and crypto currency exchanges.\n\n\n=== Healthcare ===\nBig data analytics has been used in healthcare in providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries. Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. \"Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth.\" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.\nBig data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research. Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.\nA related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.  For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily. Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.  These are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.\n\n\n=== Education ===\nA McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including paid programs like The Data Incubator or General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannan is that marketing has several sub domains (e.g., advertising, promotions,\nproduct development, branding) that all use different types of data.\n\n\n=== Media ===\nTo understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.\n\nTargeting of consumers (for advertising by marketers)\nData capture\nData journalism: publishers and journalists use big data tools to provide unique and innovative insights and infographics.\nChannel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.\n\n\n=== Insurance ===\nHealth insurance providers are collecting data on social \"determinants of health\" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.\n\n\n=== Internet of things (IoT) ===\n\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts.\nKevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things\u2014using data they gathered without any help from us\u2014we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\"\n\n\n=== Information technology ===\nEspecially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them. ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data.\n\n\n=== Survey science ===\nCompared to survey-based data collection, big data has low cost per data point, applies analysis techniques via machine learning and data mining, and includes diverse and new data sources, e.g., registers, social media, apps, and other forms digital data. Since 2018, survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality. There have been three Big Data Meets Survey Science (BigSurv) conferences in 2018, 2020 (virtual), 2023, and as of 2023 one conference forthcoming in 2025, a special issue in the Social Science Computer Review, a special issue in Journal of the Royal Statistical Society, and a special issue in EP J Data Science, and a book called Big Data Meets Social Sciences edited by Craig Hill and five other Fellows of the American Statistical Association. In 2021, the founding members of BigSurv received the Warren J. Mitofsky Innovators Award from the American Association for Public Opinion Research.\n\n\n=== Marketing ===\nBig data is notable in marketing due to the constant \u201cdatafication\u201d of everyday consumers of the internet, in which all forms of data are tracked. The datafication of consumers can be defined as  quantifying many of or all human behaviors for the purpose of marketing. The increasingly digital world of rapid datafication makes this idea relevant to marketing because the amount of data constantly grows exponentially. It is predicted to increase from 44 to 163 zettabytes within the span of five years. The size of big data can often be difficult to navigate for marketers. As a result, adopters of big data may find themselves at a disadvantage. Algorithmic findings can be difficult to achieve with such large datasets. Big data in marketing is a highly lucrative tool that can be used for large corporations, its value being as a result of the possibility of predicting significant trends, interests, or statistical outcomes in a consumer-based manner.\nThere are three significant factors in the use of big data in marketing:\n\nBig data provides customer behavior pattern spotting for marketers, since all human actions are being quantified into readable numbers for marketers to analyze and use for their research. In addition, big data can also be seen as a customized product recommendation tool. Specifically, since big data is effective in analyzing customers\u2019 purchase behaviors and browsing patterns, this technology can assist companies in promoting specific personalized products to specific customers.\nReal-time market responsiveness is important for marketers because of the ability to shift marketing efforts and correct to current trends, which is helpful in maintaining relevance to consumers. This can supply corporations with the information necessary to predict the wants and needs of consumers in advance.\nData-driven market ambidexterity are being highly fueled by big data. New models and algorithms are being developed to make significant predictions about certain economic and social situations.\n\n\n== Case studies ==\n\n\n=== Government ===\n\n\n==== China ====\nThe Integrated Joint Operations Platform (IJOP, \u4e00\u4f53\u5316\u8054\u5408\u4f5c\u6218\u5e73\u53f0) is used by the government to monitor the population, particularly Uyghurs. Biometrics, including DNA samples, are gathered through a program of free physicals.\nBy 2020, China plans to give all its citizens a personal \"social credit\" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology.\n\n\n==== India ====\nBig data analysis was tried out for the BJP to win the 2014 Indian General Election.\nThe Indian government uses numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.\n\n\n==== Israel ====\nPersonalized diabetic treatments can be created through GlucoMe's big data solution.\n\n\n==== United Kingdom ====\nExamples of uses of big data in public services:\n\nData on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify and examine the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.\nJoining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as Meals on Wheels. The connection of data allowed the local authority to avoid any weather-related delay.\n\n\n==== United States ====\nIn 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government. The initiative is composed of 84 different big data programs spread across six departments.\nBig data analysis played a large role in Barack Obama's successful 2012 re-election campaign.\nThe United States Federal Government owns four of the ten most powerful supercomputers in the world.\nThe Utah Data Center has been constructed by the United States National Security Agency. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabytes. This has posed security concerns regarding the anonymity of the data collected.\n\n\n=== Retail ===\nWalmart handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data\u2014the equivalent of 167 times the information contained in all the books in the US Library of Congress.\nWindermere Real Estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.\nFICO Card Detection System protects accounts worldwide.\nOmnichannel retailing leverages online big data to improve offline experiences.\n\n\n=== Science ===\nThe Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.99995% of these streams, there are 1,000 collisions of interest per second.\nAs a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.\nIf all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication. To put the number in perspective, this is equivalent to 500 quintillion (5\u00d71020) bytes per day, almost 200 times more than all the other sources combined in the world.\nThe Square Kilometre Array is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day. It is considered one of the most ambitious scientific projects ever undertaken.\nWhen the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.\nDecoding the human genome originally took 10 years to process; now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times less expensive than the reduction in cost predicted by Moore's law.\nThe NASA Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.\nGoogle's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any \"friction points\", or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.\n23andme's DNA database contains the genetic information of over 1,000,000 people worldwide. The company explores selling the \"anonymous aggregated genetic data\" to other researchers and pharmaceutical companies for research purposes if patients give their consent. Ahmad Hariri, professor of psychology and neuroscience at Duke University who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists. A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.\nComputational fluid dynamics (CFD) and hydrodynamic turbulence research generate massive data sets. The Johns Hopkins Turbulence Databases (JHTDB) contains over 350 terabytes of spatiotemporal fields from Direct Numerical simulations of various turbulent flows. Such data have been difficult to share using traditional methods such as downloading flat simulation output files. The data within JHTDB can be accessed using \"virtual sensors\" with various access modes ranging from direct web-browser queries, access through Matlab, Python, Fortran and C programs executing on clients' platforms, to cut out services to download raw data. The data have been used in over 150 scientific publications.\n\n\n=== Sports ===\nBig data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.\nFuture performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.\nIn Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.\nBased on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.\n\n\n=== Technology ===\nAs of 2013, eBay.com uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.\nAmazon.com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.\nFacebook handles 50 billion photos from its user base. As of June 2017, Facebook reached 2 billion monthly active users.\nGoogle was handling roughly 100 billion searches per month as of August 2012.\n\n\n=== COVID-19 ===\nDuring the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.\nGovernments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.\n\n\n== Research activities ==\nEncrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.\nIn March 2012, The White House announced a national \"Big Data Initiative\" that consisted of six federal departments and agencies committing more than $200 million to big data research projects.\nThe initiative included a National Science Foundation \"Expeditions in Computing\" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.\nThe White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.\nThe U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.\nThe European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.\nThe British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.\nAt the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.\nComputational social sciences \u2013 Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the \"future orientation index\". They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.\nTobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.\nBig data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.\n\n\n=== Sampling big data ===\nA research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient. Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.\n\n\n== Critique ==\nCritiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.\n\n\n=== Critiques of the big data paradigm ===\n\"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data.\"  In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by \"big judgment\", according to an article in the Harvard Business Review.\nMuch in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggest to use \"abductive reasoning as a first step in the research process in order to bring context to consumers' digital traces and make new theories emerge\".  Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (e.g. contingency tables) typically employed with smaller data sets.\nIn health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.  A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation. In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction (\"Glory of Science and Philosophy scandal\", C. D. Broad, 1926) are to be considered.\nPrivacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.\nBarocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.\n\n\n=== Critiques of the \"V\" model ===\nThe \"V\" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:\n\nData completeness: understanding of the non-obvious from data\nData correlation, causation, and predictability: causality as not essential requirement to achieve predictability\nExplainability and interpretability: humans desire to understand and accept what they understand, where algorithms do not cope with this\nLevel of automated decision-making: algorithms that support automated decision making and algorithmic self-learning\n\n\n=== Critiques of novelty ===\nLarge data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial \"big data\". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.\n\n\n=== Critiques of big data execution ===\nUlf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a \"fad\" in scientific research. Researcher Danah Boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data. This approach may lead to results that have a bias in one way or another. Integration across heterogeneous data resources\u2014some that might be considered big data and others not\u2014presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.\nIn the provocative article \"Critical Questions for Big Data\", the authors title big data a part of mythology: \"large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy\". Users of big data are often \"lost in the sheer volume of numbers\", and \"working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth\". Recent developments in BI domain, such as pro-active reporting especially target improvements in the usability of big data, through automated filtering of non-useful data and correlations. Big structures are full of spurious correlations either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers \"speak for themselves\" and revolutionize scientific method, is questioned. Catherine Tucker has pointed to \"hype\" around big data, writing \"By itself, big data is unlikely to be valuable.\" The article explains: \"The many contexts where data is cheap relative to the cost of retaining talent to process it, suggests that processing skills are more important than data itself in creating value for a firm.\"\nBig data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.\nBig data is a buzzword and a \"vague term\", but at the same time an \"obsession\" with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.\nBig data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate\u2014which is based on big data statistical analysis of text\u2014does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.\nOn the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.\nIoannidis argued that \"most published research findings are false\" due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a \"significant\" result being false grows fast \u2013 even more so, when only positive results are published.\nFurthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. presidential election with varying degrees of success.\n\n\n=== Critiques of big data policing and surveillance ===\nBig data has been used in policing and surveillance by institutions like law enforcement and corporations. Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways:\n\nPlacing people under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm\nIncreasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system\nEncouraging members of society to abandon interactions with institutions that would create a digital trace, thus creating obstacles to social inclusion\nIf these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nHilbert, M (2016), \"Big Data for Development: A Review of Promises and Challenges\", Development Policy Review, 34 (1): 135\u201374, doi:10.1111/dpr.12142; free access, Archived 21 April 2021 at the Wayback Machine\nSnijders, C.; Matzat, U.; Reips, U.-D. (2012). \"'Big Data': Big gaps of knowledge in the field of Internet\". International Journal of Internet Science. 7: 1\u20135. Archived from the original on 23 November 2019. Retrieved 13 April 2013.\nYanase, J; Triantaphyllou, E (2019). \"A Systematic Survey of Computer-Aided Diagnosis in Medicine: Past and Present Developments\". Expert Systems with Applications. 138: 112821. doi:10.1016/j.eswa.2019.112821. S2CID 199019309.\n\n\n== Further reading ==\n\nPeter Kinnaird; Inbal Talgam-Cohen, eds. (2012). \"Big Data\". XRDS: Crossroads, The ACM Magazine for Students. Vol. 19, no. 1. Association for Computing Machinery. ISSN 1528-4980. OCLC 779657714.\nJure Leskovec; Anand Rajaraman; Jeffrey D. Ullman (2014). Mining of massive datasets. Cambridge University Press. ISBN 978-1-10707723-2. OCLC 888463433.\nViktor Mayer-Sch\u00f6nberger; Kenneth Cukier (2013). Big Data: A Revolution that Will Transform how We Live, Work, and Think. Houghton Mifflin Harcourt. ISBN 978-1-29990302-9. OCLC 828620988.\nPress, Gil (9 May 2013). \"A Very Short History of Big Data\". forbes.com. Jersey City, NJ. Retrieved 17 September 2016.\nStephens-Davidowitz, Seth (2017). Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are. Dey Street Books. ISBN 978-0-06239085-1.\n\"Big Data: The Management Revolution\". Harvard Business Review. October 2012.\nO'Neil, Cathy (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books. ISBN 978-0-55341883-5.\n\n\n== External links ==\n Media related to Big data at Wikimedia Commons\n The dictionary definition of big data at Wiktionary", "link": "https://en.wikipedia.org/wiki/Big_data"}, "Master in Data Science": {"title": "Master in Data Science", "content": "A Master of Science in Data Science is an interdisciplinary degree program designed to provide studies in scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured, similar to data mining.\n\n\n== Overview ==\nAs an area of expertise and field, data science is defined as a \"concept to unify statistics, data analysis and their related quantitative and qualitative methods\" in order to \"understand and analyze actual phenomena\" with data. It employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information science, and computer science, in particular from the subdomains of machine learning, statistical classification, cluster analysis, data mining, databases, and visualization.\nThe degree is relatively new, with graduate schools, business schools, and data science centers often housing the programs. Data science degree programs have emerged to address the growing and unique need for data scientists who can provide insight into multiple organizational issues and interests across several disciplines.\nWhen Harvard Business Review called data scientist \"The Sexiest Job of the 21st Century\" the term became a buzzword, and is now often applied to business analytics, or even arbitrary use of data, or used as a term for statistics. While many university programs now offer a data science degree, there exists no consensus on a definition or curriculum contents.\n\n\n== Master in Data Science programs ==\n\n\n=== Australia ===\nJames Cook University\nMacquarie University\nMonash University\nQueensland University of Technology\nUniversity of Melbourne\nUniversity of New England (Australia)\nUniversity of New South Wales\nUniversity of Newcastle (Australia)\nUniversity of Queensland\nUniversity of Sydney\nUniversity of Technology Sydney\nUniversity of Western Australia\nWestern Sydney University\n\n\n=== United States ===\nBoston University\nBrown University\nCabrini University\nCarnegie Mellon University\nCity University of Seattle\nColumbia University\nCornell University\nDuke University\nDrexel University\nHarvard University\nIllinois Institute of Technology\nIndiana University, Bloomington\nJohns Hopkins University\nLehigh University\nLewis University\nMerrimack College\nNew Jersey Institute of Technology\nNew York University\nNorth Carolina State University\nNorthwestern University\nRutgers University\nSaint Peter's University\nSouthern Methodist University\nStanford University\nSyracuse University\nTufts University\nUniversity of California, Berkeley\nUniversity of California, San Diego\nUniversity of Colorado Boulder\nUniversity of Connecticut\nUniversity of Illinois Urbana\u2013Champaign\nUniversity of Michigan\nUniversity of Minnesota - Twin Cities\nUniversity of St. Thomas\nUniversity of Pennsylvania\nUniversity of Rochester\nUniversity of Southern California\nUniversity of Virginia\nUniversity of Wisconsin\nUtica College\n\n\n=== Canada ===\nMcGill University\nRyerson University\nUniversity of Alberta\nUniversity of British Columbia\nUniversity of Calgary\nUniversity of the Fraser Valley\n\n\n=== United Kingdom ===\nCity University London\nCoventry University\nDe Montfort University\nGoldsmiths, University of London\nImperial College London\nNewcastle University\nRobert Gordon University\nRoyal Holloway, University of London\nUniversity College London\nUniversity of Bristol\nUniversity of Dundee\nUniversity of Durham\nUniversity of East Anglia\nUniversity of East London\nUniversity of Essex\nUniversity of Greenwich\nUniversity of Liverpool\nUniversity of Manchester\nUniversity of St. Andrews\nUniversity of Warwick\n\n\n=== Ireland ===\nDublin Institute of Technology\nInstitute of Technology, Blanchardstown\nIT Carlow\nUniversity College Cork\n\n\n=== India ===\nIIT\nIndian Institute of Science\nNational Institutes of Technology (India)\nBrainware University\nChrist University\nVellore Institute of Technology\nKalinga Institute of Industrial Technology\nBITS Pilani\nJawaharlal Nehru Technological University\n\n\n=== Germany ===\nFernUniversit\u00e4t in Hagen\nFrankfurt School of Finance & Management\nFriedrich\u2013Alexander University Erlangen\u2013N\u00fcrnberg (FAU)\nRWTH Aachen University\n\n\n=== France ===\nT\u00e9l\u00e9com Paris Tech\nT\u00e9l\u00e9com SudParis\nESSEC Business School\n\n\n=== Finland ===\nUniversity of Helsinki\nAalto University\n\n\n=== Denmark ===\nAalborg University\n\n\n=== New Zealand ===\nUniversity of Auckland\nUniversity of Canterbury\nUniversity of Otago\n\n\n=== Hong Kong ===\nCity University of Hong Kong\nChinese University of Hong Kong\nUniversity of Hong Kong\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Master_in_Data_Science"}, "Computer science": {"title": "Computer science", "content": "Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). \nAlgorithms and data structures are central to computer science.\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human\u2013computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\n\n\n== History ==\n\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".\n\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff\u2013Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\n\n\n== Etymology and scope ==\n\nAlthough first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM,\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM\u2014turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), inform\u00e1tica (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (\u03c0\u03bb\u03b7\u03c1\u03bf\u03c6\u03bf\u03c1\u03b9\u03ba\u03ae, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"\nA folkloric quotation, often attributed to\u2014but almost certainly not first formulated by\u2014Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt G\u00f6del, Alan Turing, John von Neumann, R\u00f3zsa P\u00e9ter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.\nThe relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.\nThe academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n\n\n== Philosophy ==\n\n\n=== Epistemology of computer science ===\nDespite the word science in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.\nProponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.\n\n\n=== Paradigms of computer science ===\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.\n\n\n== Fields ==\n\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.\nCSAB, formerly called Computing Sciences Accreditation Board\u2014which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)\u2014identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human\u2013computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\n\n\n=== Theoretical computer science ===\n\nTheoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n\n\n==== Theory of computation ====\n\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\n\n\n==== Information and coding theory ====\n\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n\n\n==== Data structures and algorithms ====\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\n\n\n==== Programming language theory and formal methods ====\n\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n\n\n=== Applied computer science ===\n\n\n==== Computer graphics and visualization ====\n\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n\n\n==== Image and sound processing ====\n\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier \u2013 whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science.\n\n\n==== Computational science, finance and engineering ====\n\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.\n\n\n==== Social computing and human\u2013computer interaction ====\n\nSocial computing is an area that is concerned with the intersection of social behavior and computational systems. Human\u2013computer interaction research develops theories, principles, and guidelines for user interface designers.\n\n\n==== Software engineering ====\n\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software\u2014it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\n\n\n==== Artificial intelligence ====\n\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\n\n\n=== Computer systems ===\n\n\n==== Computer architecture and microarchitecture ====\n\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959.\n\n\n==== Concurrent, parallel and distributed computing ====\n\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.\n\n\n==== Computer networks ====\n\nThis branch of computer science aims to manage networks between computers worldwide.\n\n\n==== Computer security and cryptography ====\n\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\n\n\n==== Databases and data mining ====\n\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\n\n\n== Discoveries ==\nThe philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:\n\nGottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent \"anything\".\nAll the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on/off\", \"magnetized/de-magnetized\", \"high-voltage/low-voltage\", etc.).\n\nAlan Turing's insight: there are only five actions that a computer has to perform in order to do \"anything\".\nEvery algorithm can be expressed in a language for a computer consisting of only five basic instructions:\nmove left one location;\nmove right one location;\nread symbol at current location;\nprint 0 at current location;\nprint 1 at current location.\n\nCorrado B\u00f6hm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do \"anything\".\nOnly three rules are needed to combine any set of basic instructions into more complex ones:\nsequence: first do this, then do that;\n selection: IF such-and-such is the case, THEN do this, ELSE do that;\nrepetition: WHILE such-and-such is the case, DO this.\nThe three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).\n\n\n== Programming paradigms ==\n\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n\nFunctional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.\nImperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\nObject-oriented programming, a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.\nService-oriented programming, a programming paradigm that uses \"services\" as the unit of computer work, to design and implement integrated business applications and mission critical software programs.\nMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\n\n\n== Research ==\n\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nDBLP Computer Science Bibliography\nAssociation for Computing Machinery\nInstitute of Electrical and Electronics Engineers", "link": "https://en.wikipedia.org/wiki/Computer_science"}}, "Big Data": {"Big data": {"title": "Big data", "content": "Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.\"\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17\u00d7260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than \u20ac100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"\n\n\n== Definition ==\nThe term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.  Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.  Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data. Big data \"size\" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.  Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.\n\"Volume\", \"variety\", \"velocity\", and various other \"Vs\" are added by some organizations to describe it, a revision challenged by some industry authorities. The Vs of big data were often referred to as the \"three Vs\", \"four Vs\", and \"five Vs\". They represented the qualities of big data in volume, variety, velocity, veracity, and value. Variability is often included as an additional quality of big data.\nA 2018 definition states \"Big data is where parallel computing tools are needed to handle data\", and notes, \"This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model.\"\nIn a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.\n\n\n=== Big data vs. business intelligence ===\nThe growing maturity of the concept more starkly delineates the difference between \"big data\" and \"business intelligence\":\n\nBusiness intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc.\nBig data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.\n\n\n== Characteristics ==\n\nBig data can be described by the following characteristics:\n\nVolume\nThe quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes.\nVariety\nThe type and nature of the data. Earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. Big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.\nVelocity\nThe speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing.\nVeracity\nThe truthfulness or reliability of the data, which refers to the data quality and the data value. Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it. The data quality of captured data can vary greatly, affecting an accurate analysis.\nValue\nThe worth in information that can be achieved by the processing and analysis of large datasets. Value also can be measured by an assessment of the other qualities of big data. Value may also represent the profitability of information that is retrieved from the analysis of big data.\nVariability\nThe characteristic of the changing formats, structure, or sources of big data. Big data can include structured, unstructured, or combinations of structured and unstructured data. Big data analysis may integrate raw data from multiple sources. The processing of raw data may also involve transformations of unstructured data to structured data.\nOther possible characteristics of big data are:\n\nExhaustive\nWhether the entire system (i.e., \n  \n    \n      \n        n\n      \n    \n    {\\textstyle n}\n  \n=all) is captured or recorded or not. Big data may or may not include all the available data from sources.\nFine-grained and uniquely lexical\nRespectively, the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified.\nRelational\nIf the data collected contains common fields that would enable a conjoining, or meta-analysis, of different data sets.\nExtensional\nIf new fields in each element of the data collected can be added or changed easily.\nScalability\nIf the size of the big data storage system can expand rapidly.\n\n\n== Architecture ==\nBig data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s. For many years, WinterCorp published the largest database report.\nTeradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added unstructured data types including XML, JSON, and Avro.\nIn 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.\nCERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current \"big data\" movement.\nIn 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the \"map\" step). The results are then gathered and delivered (the \"reduce\" step). The framework was very successful, so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named \"Hadoop\". Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds in-memory processing and the ability to set up many operations (not just map followed by reducing).\nMIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled \"Big Data Solution Offering\". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.\nStudies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.\nThe data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.\n\n\n== Technologies ==\nA 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:\n\nTechniques for analyzing data, such as A/B testing, machine learning, and natural language processing\nBig data technologies, like business intelligence, cloud computing, and databases\nVisualization, such as charts, graphs, and other displays of the data\nMultidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.\nAdditional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.\nSome MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.\nDARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called \"Ayasdi\".\nThe practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures\u2014storage area network (SAN) and network-attached storage (NAS)\u2014 is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.\nReal or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good\u2014data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.\n\n\n== Applications ==\n\nBig data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year, about twice as fast as the software business as a whole.\nDeveloped economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data, which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).\nWhile many vendors offer off-the-shelf products for big data, experts promote the development of in-house custom-tailored systems if the company has sufficient technical capabilities.\n\n\n=== Government ===\n\nThe use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation, but comes with flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.\nCivil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.\n\n\n=== International development ===\nResearch on the effective usage of information and communication technologies for development (also known as \"ICT4D\") suggests that big data technology can make important contributions but also present unique challenges to international development. Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management. Additionally, user-generated data offers new opportunities to give the unheard a voice. However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.  The challenge of \"big data for development\" is currently evolving toward the application of this data through machine learning, known as \"artificial intelligence for development (AI4D).\n\n\n==== Benefits ====\nA major practical application of big data for development has been \"fighting poverty with data\". In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata  and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty. Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues  argue that digital trace data has several benefits such as:\n\nThematic coverage: including areas that were previously difficult or impossible to measure\nGeographical coverage: providing sizable and comparable data for almost all countries, including many small countries that usually are not included in international inventories\nLevel of detail: providing fine-grained data with many interrelated variables, and new aspects, like network connections\nTimeliness and timeseries: graphs can be produced within days of being collected\n\n\n==== Challenges ====\nAt the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:\n\nRepresentativeness. While traditional development statistics is mainly concerned with the representativeness of random survey samples, digital trace data is never a random sample.\nGeneralizability. While observational data always represents this source very well, it only represents what it represents, and nothing more. While it is tempting to generalize from specific observations of one platform to broader settings, this is often very deceptive.\nHarmonization. Digital trace data still requires international harmonization of indicators. It adds the challenge of so-called \"data-fusion\", the harmonization of different sources.\nData overload. Analysts and institutions are not used to effectively deal with a large number of variables, which is efficiently done with interactive dashboards. Practitioners still lack a standard workflow that would allow researchers, users and policymakers to efficiently and effectively deal with data.\n\n\n=== Finance ===\nBig Data is being rapidly adopted in Finance to 1) speed up processing and 2) deliver better, more informed inferences, both internally and to the clients of the financial institutions. The financial applications of Big Data range from investing decisions and trading (processing volumes of available price data, limit order books, economic data and more, all at the same time), portfolio management (optimizing over an increasingly large array of financial instruments, potentially selected from different asset classes), risk management (credit rating based on extended information), and any other aspect where the data inputs are large. Big Data has also been a typical concept within the field of alternative financial service. Some of the major areas involve crowd-funding platforms and crypto currency exchanges.\n\n\n=== Healthcare ===\nBig data analytics has been used in healthcare in providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries. Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. \"Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth.\" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.\nBig data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research. Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.\nA related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.  For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily. Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.  These are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.\n\n\n=== Education ===\nA McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including paid programs like The Data Incubator or General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannan is that marketing has several sub domains (e.g., advertising, promotions,\nproduct development, branding) that all use different types of data.\n\n\n=== Media ===\nTo understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.\n\nTargeting of consumers (for advertising by marketers)\nData capture\nData journalism: publishers and journalists use big data tools to provide unique and innovative insights and infographics.\nChannel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.\n\n\n=== Insurance ===\nHealth insurance providers are collecting data on social \"determinants of health\" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.\n\n\n=== Internet of things (IoT) ===\n\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts.\nKevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things\u2014using data they gathered without any help from us\u2014we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\"\n\n\n=== Information technology ===\nEspecially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them. ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data.\n\n\n=== Survey science ===\nCompared to survey-based data collection, big data has low cost per data point, applies analysis techniques via machine learning and data mining, and includes diverse and new data sources, e.g., registers, social media, apps, and other forms digital data. Since 2018, survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality. There have been three Big Data Meets Survey Science (BigSurv) conferences in 2018, 2020 (virtual), 2023, and as of 2023 one conference forthcoming in 2025, a special issue in the Social Science Computer Review, a special issue in Journal of the Royal Statistical Society, and a special issue in EP J Data Science, and a book called Big Data Meets Social Sciences edited by Craig Hill and five other Fellows of the American Statistical Association. In 2021, the founding members of BigSurv received the Warren J. Mitofsky Innovators Award from the American Association for Public Opinion Research.\n\n\n=== Marketing ===\nBig data is notable in marketing due to the constant \u201cdatafication\u201d of everyday consumers of the internet, in which all forms of data are tracked. The datafication of consumers can be defined as  quantifying many of or all human behaviors for the purpose of marketing. The increasingly digital world of rapid datafication makes this idea relevant to marketing because the amount of data constantly grows exponentially. It is predicted to increase from 44 to 163 zettabytes within the span of five years. The size of big data can often be difficult to navigate for marketers. As a result, adopters of big data may find themselves at a disadvantage. Algorithmic findings can be difficult to achieve with such large datasets. Big data in marketing is a highly lucrative tool that can be used for large corporations, its value being as a result of the possibility of predicting significant trends, interests, or statistical outcomes in a consumer-based manner.\nThere are three significant factors in the use of big data in marketing:\n\nBig data provides customer behavior pattern spotting for marketers, since all human actions are being quantified into readable numbers for marketers to analyze and use for their research. In addition, big data can also be seen as a customized product recommendation tool. Specifically, since big data is effective in analyzing customers\u2019 purchase behaviors and browsing patterns, this technology can assist companies in promoting specific personalized products to specific customers.\nReal-time market responsiveness is important for marketers because of the ability to shift marketing efforts and correct to current trends, which is helpful in maintaining relevance to consumers. This can supply corporations with the information necessary to predict the wants and needs of consumers in advance.\nData-driven market ambidexterity are being highly fueled by big data. New models and algorithms are being developed to make significant predictions about certain economic and social situations.\n\n\n== Case studies ==\n\n\n=== Government ===\n\n\n==== China ====\nThe Integrated Joint Operations Platform (IJOP, \u4e00\u4f53\u5316\u8054\u5408\u4f5c\u6218\u5e73\u53f0) is used by the government to monitor the population, particularly Uyghurs. Biometrics, including DNA samples, are gathered through a program of free physicals.\nBy 2020, China plans to give all its citizens a personal \"social credit\" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology.\n\n\n==== India ====\nBig data analysis was tried out for the BJP to win the 2014 Indian General Election.\nThe Indian government uses numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.\n\n\n==== Israel ====\nPersonalized diabetic treatments can be created through GlucoMe's big data solution.\n\n\n==== United Kingdom ====\nExamples of uses of big data in public services:\n\nData on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify and examine the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.\nJoining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as Meals on Wheels. The connection of data allowed the local authority to avoid any weather-related delay.\n\n\n==== United States ====\nIn 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government. The initiative is composed of 84 different big data programs spread across six departments.\nBig data analysis played a large role in Barack Obama's successful 2012 re-election campaign.\nThe United States Federal Government owns four of the ten most powerful supercomputers in the world.\nThe Utah Data Center has been constructed by the United States National Security Agency. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabytes. This has posed security concerns regarding the anonymity of the data collected.\n\n\n=== Retail ===\nWalmart handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data\u2014the equivalent of 167 times the information contained in all the books in the US Library of Congress.\nWindermere Real Estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.\nFICO Card Detection System protects accounts worldwide.\nOmnichannel retailing leverages online big data to improve offline experiences.\n\n\n=== Science ===\nThe Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.99995% of these streams, there are 1,000 collisions of interest per second.\nAs a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.\nIf all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication. To put the number in perspective, this is equivalent to 500 quintillion (5\u00d71020) bytes per day, almost 200 times more than all the other sources combined in the world.\nThe Square Kilometre Array is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day. It is considered one of the most ambitious scientific projects ever undertaken.\nWhen the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.\nDecoding the human genome originally took 10 years to process; now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times less expensive than the reduction in cost predicted by Moore's law.\nThe NASA Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.\nGoogle's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any \"friction points\", or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.\n23andme's DNA database contains the genetic information of over 1,000,000 people worldwide. The company explores selling the \"anonymous aggregated genetic data\" to other researchers and pharmaceutical companies for research purposes if patients give their consent. Ahmad Hariri, professor of psychology and neuroscience at Duke University who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists. A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.\nComputational fluid dynamics (CFD) and hydrodynamic turbulence research generate massive data sets. The Johns Hopkins Turbulence Databases (JHTDB) contains over 350 terabytes of spatiotemporal fields from Direct Numerical simulations of various turbulent flows. Such data have been difficult to share using traditional methods such as downloading flat simulation output files. The data within JHTDB can be accessed using \"virtual sensors\" with various access modes ranging from direct web-browser queries, access through Matlab, Python, Fortran and C programs executing on clients' platforms, to cut out services to download raw data. The data have been used in over 150 scientific publications.\n\n\n=== Sports ===\nBig data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.\nFuture performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.\nIn Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.\nBased on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.\n\n\n=== Technology ===\nAs of 2013, eBay.com uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.\nAmazon.com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.\nFacebook handles 50 billion photos from its user base. As of June 2017, Facebook reached 2 billion monthly active users.\nGoogle was handling roughly 100 billion searches per month as of August 2012.\n\n\n=== COVID-19 ===\nDuring the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.\nGovernments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.\n\n\n== Research activities ==\nEncrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.\nIn March 2012, The White House announced a national \"Big Data Initiative\" that consisted of six federal departments and agencies committing more than $200 million to big data research projects.\nThe initiative included a National Science Foundation \"Expeditions in Computing\" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.\nThe White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.\nThe U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.\nThe European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.\nThe British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.\nAt the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.\nComputational social sciences \u2013 Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the \"future orientation index\". They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.\nTobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.\nBig data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.\n\n\n=== Sampling big data ===\nA research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient. Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.\n\n\n== Critique ==\nCritiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.\n\n\n=== Critiques of the big data paradigm ===\n\"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data.\"  In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by \"big judgment\", according to an article in the Harvard Business Review.\nMuch in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggest to use \"abductive reasoning as a first step in the research process in order to bring context to consumers' digital traces and make new theories emerge\".  Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (e.g. contingency tables) typically employed with smaller data sets.\nIn health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.  A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation. In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction (\"Glory of Science and Philosophy scandal\", C. D. Broad, 1926) are to be considered.\nPrivacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.\nBarocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.\n\n\n=== Critiques of the \"V\" model ===\nThe \"V\" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:\n\nData completeness: understanding of the non-obvious from data\nData correlation, causation, and predictability: causality as not essential requirement to achieve predictability\nExplainability and interpretability: humans desire to understand and accept what they understand, where algorithms do not cope with this\nLevel of automated decision-making: algorithms that support automated decision making and algorithmic self-learning\n\n\n=== Critiques of novelty ===\nLarge data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial \"big data\". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.\n\n\n=== Critiques of big data execution ===\nUlf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a \"fad\" in scientific research. Researcher Danah Boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data. This approach may lead to results that have a bias in one way or another. Integration across heterogeneous data resources\u2014some that might be considered big data and others not\u2014presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.\nIn the provocative article \"Critical Questions for Big Data\", the authors title big data a part of mythology: \"large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy\". Users of big data are often \"lost in the sheer volume of numbers\", and \"working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth\". Recent developments in BI domain, such as pro-active reporting especially target improvements in the usability of big data, through automated filtering of non-useful data and correlations. Big structures are full of spurious correlations either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers \"speak for themselves\" and revolutionize scientific method, is questioned. Catherine Tucker has pointed to \"hype\" around big data, writing \"By itself, big data is unlikely to be valuable.\" The article explains: \"The many contexts where data is cheap relative to the cost of retaining talent to process it, suggests that processing skills are more important than data itself in creating value for a firm.\"\nBig data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.\nBig data is a buzzword and a \"vague term\", but at the same time an \"obsession\" with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.\nBig data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate\u2014which is based on big data statistical analysis of text\u2014does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.\nOn the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.\nIoannidis argued that \"most published research findings are false\" due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a \"significant\" result being false grows fast \u2013 even more so, when only positive results are published.\nFurthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. presidential election with varying degrees of success.\n\n\n=== Critiques of big data policing and surveillance ===\nBig data has been used in policing and surveillance by institutions like law enforcement and corporations. Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways:\n\nPlacing people under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm\nIncreasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system\nEncouraging members of society to abandon interactions with institutions that would create a digital trace, thus creating obstacles to social inclusion\nIf these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nHilbert, M (2016), \"Big Data for Development: A Review of Promises and Challenges\", Development Policy Review, 34 (1): 135\u201374, doi:10.1111/dpr.12142; free access, Archived 21 April 2021 at the Wayback Machine\nSnijders, C.; Matzat, U.; Reips, U.-D. (2012). \"'Big Data': Big gaps of knowledge in the field of Internet\". International Journal of Internet Science. 7: 1\u20135. Archived from the original on 23 November 2019. Retrieved 13 April 2013.\nYanase, J; Triantaphyllou, E (2019). \"A Systematic Survey of Computer-Aided Diagnosis in Medicine: Past and Present Developments\". Expert Systems with Applications. 138: 112821. doi:10.1016/j.eswa.2019.112821. S2CID 199019309.\n\n\n== Further reading ==\n\nPeter Kinnaird; Inbal Talgam-Cohen, eds. (2012). \"Big Data\". XRDS: Crossroads, The ACM Magazine for Students. Vol. 19, no. 1. Association for Computing Machinery. ISSN 1528-4980. OCLC 779657714.\nJure Leskovec; Anand Rajaraman; Jeffrey D. Ullman (2014). Mining of massive datasets. Cambridge University Press. ISBN 978-1-10707723-2. OCLC 888463433.\nViktor Mayer-Sch\u00f6nberger; Kenneth Cukier (2013). Big Data: A Revolution that Will Transform how We Live, Work, and Think. Houghton Mifflin Harcourt. ISBN 978-1-29990302-9. OCLC 828620988.\nPress, Gil (9 May 2013). \"A Very Short History of Big Data\". forbes.com. Jersey City, NJ. Retrieved 17 September 2016.\nStephens-Davidowitz, Seth (2017). Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are. Dey Street Books. ISBN 978-0-06239085-1.\n\"Big Data: The Management Revolution\". Harvard Business Review. October 2012.\nO'Neil, Cathy (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books. ISBN 978-0-55341883-5.\n\n\n== External links ==\n Media related to Big data at Wikimedia Commons\n The dictionary definition of big data at Wiktionary", "link": "https://en.wikipedia.org/wiki/Big_data"}, "Big Data (band)": {"title": "Big Data (band)", "content": "Big Data is an American electronic music project created by producer Alan Wilkis. Big Data is best known for the single \"Dangerous\", featuring Joywave, which reached number one on the Billboard Alternative Songs chart in August 2014, and was certified gold by the RIAA in May 2015.\nBig Data's first EP, 1.0, was released on October 1, 2013, on Wilkis's own Wilcassettes label and features the songs \"The Stroke of Return\", \"Dangerous\", \"Big Dater\", and \"Bombs over Brooklyn\". In early December 2013, they also released a remix EP, 1.5, which included eight remixes of the song \"Dangerous\", including one by Joywave. Another remix EP, 1.6, was released in late September 2014, and included seven remixes of \"Dangerous\".\nBig Data's first studio album, 2.0, was released on March 20, 2015. Their second album, 3.0, was released on July 26, 2019.\n\n\n== Discography ==\n\n\n=== Studio albums ===\n\n\n=== Singles ===\n\n\n==== As lead artist ====\n\n\n==== Promotional singles ====\n\n\n=== Extended plays ===\n\n\n== References ==\n\n\n== External links ==\nOfficial website", "link": "https://en.wikipedia.org/wiki/Big_Data_(band)"}, "Data science": {"title": "Data science", "content": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.\n\n\n== Foundations ==\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human\u2013computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.\n\n\n=== Relationship to statistics ===\nMany statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.\nStanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.\n\n\n== Etymology ==\n\n\n=== Early usage ===\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.\nThe term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.\nDuring the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".\n\n\n=== Modern usage ===\nIn 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".\nThe modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.\nThe professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.\nThere is still no consensus on the definition of data science, and it is considered by some to be a buzzword. Big data is a related marketing term. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.\n\n\n== Data science and data analysis ==\n\nData science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.\nData analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.\nData science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.\nWhile data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.\nDespite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.\nIn summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n\n\n== Cloud computing for data science ==\n\nCloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.\nSome distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.\n\n\n== Ethical consideration in data science ==\nData science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts \nMachine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.\n\n\n== See also ==\nPython (programming language)\nR (programming language)\nData engineering\nBig data\nMachine learning\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Data_science"}, "Big data ethics": {"title": "Big data ethics", "content": "Big data ethics, also known simply as data ethics, refers to systemizing, defending, and recommending concepts of right and wrong conduct in relation to data, in particular personal data. Since the dawn of the Internet the sheer quantity and quality of data has dramatically increased and is continuing to do so exponentially. Big data describes this large amount of data that is so voluminous and complex that traditional data processing application software is inadequate to deal with them. Recent innovations in medical research and healthcare, such as high-throughput genome sequencing, high-resolution imaging, electronic medical patient records and a plethora of internet-connected health devices have triggered a data deluge that will reach the exabyte range in the near future. Data ethics is of increasing relevance as the quantity of data increases because of the scale of the impact.\nBig data ethics are different from information ethics because the focus of information ethics is more concerned with issues of intellectual property and concerns relating to librarians, archivists, and information professionals, while big data ethics is more concerned with collectors and disseminators of structured or unstructured data such as data brokers, governments, and large corporations.  However, since artificial intelligence or machine learning systems are regularly built using big data sets, the discussions surrounding data ethics are often intertwined with those in the ethics of artificial intelligence. More recently, issues of big data ethics have also been researched in relation with other areas of technology and science ethics, including ethics in mathematics and engineering ethics, as many areas of applied mathematics and engineering use increasingly large data sets.\n\n\n== Principles ==\nData ethics is concerned with the following principles:\n\nOwnership \u2013 Individuals own their personal data.\nTransaction transparency \u2013 If an individual's personal data is used, they should have transparent access to the algorithm design used to generate aggregate data sets.\nConsent \u2013 If an individual or legal entity would like to use personal data, one needs informed and explicitly expressed consent of what personal data moves to whom, when, and for what purpose from the owner of the data.\nPrivacy \u2013 If data transactions occur all reasonable effort needs to be made to preserve privacy.\nCurrency \u2013 Individuals should be aware of financial transactions resulting from the use of their personal data and the scale of these transactions.\nOpenness \u2013 Aggregate data sets should be freely available.\n\n\n=== Ownership ===\nOwnership of data involves determining rights and duties over property, such as the ability to exercise individual control over (including limit the sharing of) personal data comprising one's digital identity. The question of data ownership arises when someone records observations on an individual person. The observer and the observed both state a claim to the data. Questions also arise as to the responsibilities that the observer and the observed have in relation to each other. These questions have become increasingly relevant with the Internet magnifying the scale and systematization of observing people and their thoughts. The question of personal data ownership relates to questions of corporate ownership and intellectual property.\nIn the European Union, some people argue that the General Data Protection Regulation indicates that individuals own their personal data, although this is contested.\n\n\n=== Transaction transparency ===\nConcerns have been raised around how biases can be integrated into algorithm design resulting in systematic oppression.\nIn terms of governance, big data ethics is concerned with which types of inferences and predictions should be made using big data technologies such as algorithms.\nAnticipatory governance is the practice of using predictive analytics to assess possible future behaviors. This has ethical implications because it affords the ability to target particular groups and places which can encourage prejudice and discrimination For example, predictive policing highlights certain groups or neighborhoods which should be watched more closely than others which leads to more sanctions in these areas, and closer surveillance for those who fit the same profiles as those who are sanctioned.\nThe term \"control creep\" refers to data that has been generated with a particular purpose in mind but which is repurposed. This practice is seen with airline industry data which has been repurposed for profiling and managing security risks at airports.\n\n\n=== Privacy ===\nPrivacy has been presented as a limitation to data usage which could also be considered unethical. For example, the sharing of healthcare data can shed light on the causes of diseases, the effects of treatments, an can allow for tailored analyses based on individuals' needs. This is of ethical significance in the big data ethics field because while many value privacy, the affordances of data sharing are also quite valuable, although they may contradict one's conception of privacy. Attitudes against data sharing may be based in a perceived loss of control over data and a fear of the exploitation of personal data. However, it is possible to extract the value of data without compromising privacy.\nSome scholars such as Jonathan H. King and Neil M. Richards are redefining the traditional meaning of privacy, and others to question whether or not privacy still exists. In a 2014 article for the Wake Forest Law Review, King and Richard argue that privacy in the digital age can be understood not in terms of secrecy but in term of regulations which govern and control the use of personal information. In the European Union, the right to be forgotten entitles EU countries to force the removal or de-linking of personal data from databases at an individual's request if the information is deemed irrelevant or out of date. According to Andrew Hoskins, this law demonstrates the moral panic of EU members over the perceived loss of privacy and the ability to govern personal data in the digital age. In the United States, citizens have the right to delete voluntarily submitted data. This is very different from the right to be forgotten because much of the data produced using big data technologies and platforms are not voluntarily submitted. While traditional notions of privacy are under scrutiny, different legal frameworks related to privacy in the EU and US demonstrate how countries are grappling with these concerns in the context of big data. For example, the \"right to be forgotten\" in the EU and the right to delete voluntarily submitted data in the US illustrate the varying approaches to privacy regulation in the digital age.\n\n\n==== How much data is worth ====\nThe difference in value between the services facilitated by tech companies and the equity value of these tech companies is the difference in the exchange rate offered to the citizen and the \"market rate\" of the value of their data. Scientifically there are many holes in this rudimentary calculation: the financial figures of tax-evading companies are unreliable, either revenue or profit could be more appropriate, how a user is defined, a large number of individuals are needed for the data to be valuable, possible tiered prices for different people in different countries, etc. Although these calculations are crude, they serve to make the monetary value of data more tangible. Another approach is to find the data trading rates in the black market. RSA publishes a yearly cybersecurity shopping list that takes this approach.\nThis raises the economic question of whether free tech services in exchange for personal data is a worthwhile implicit exchange for the consumer. In the personal data trading model, rather than companies selling data, an owner can sell their personal data and keep the profit.\n\n\n=== Openness ===\nThe idea of open data is centered around the argument that data should be freely available and should not have restrictions that would prohibit its use, such as copyright laws. As of 2014 many governments had begun to move towards publishing open datasets for the purpose of transparency and accountability. This movement has gained traction via \"open data activists\" who have called for governments to make datasets available to allow citizens to themselves extract meaning from the data and perform checks and balances themselves. King and Richards have argued that this call for transparency includes a tension between openness and secrecy.\nActivists and scholars have also argued that because this open-sourced model of data evaluation is based on voluntary participation, the availability of open datasets has a democratizing effect on a society, allowing any citizen to participate. To some, the availability of certain types of data is seen as a right and an essential part of a citizen's agency.\nOpen Knowledge Foundation (OKF) lists several dataset types it argues should be provided by governments for them to be truly open. OKF has a tool called the Global Open Data Index (GODI), a crowd-sourced survey for measuring the openness of governments, based on its Open Definition. GODI aims to be a tool for providing feedback to governments about the quality of their open datasets.\nWillingness to share data varies from person to person. Preliminary studies have been conducted into the determinants of the willingness to share data. For example, some have suggested that baby boomers are less willing to share data than millennials.\n\n\n== See also ==\nDynamic consent\n\n\n== Footnotes ==\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Big_data_ethics"}, "List of big data companies": {"title": "List of big data companies", "content": "This is an alphabetical list of notable IT companies using the marketing term big data:\n\n\n== A ==\nAlpine Data Labs, an analytics interface working with Apache Hadoop and big data\nAzure Data Lake is a highly scalable data storage and analytics service. The service is hosted in Azure, Microsoft's public cloud\n\n\n== B ==\nBig Data Partnership, a professional services company based in London\nBig Data Scoring, a cloud-based service that lets consumer lenders improve loan quality and acceptance rates through the use of big data\nBigPanda, a technology company headquartered in Mountain View, California\nBright Computing,  developer of software for deploying and managing high-performance (HPC) clusters, big data clusters, and OpenStack in data centers and in the cloud\n\n\n== C ==\nClarivate Analytics, a global company that owns and operates a collection of subscription-based services focused largely on analytics\nCloudera, an American-based software company that provides Apache Hadoop-based software, support and services, and training to business customers\nCompuverde, an IT company with a focus on big data storage\nCVidya, a provider of big data analytics products for communications and digital service providers\nCybatar Cloud, a cloud-based system for managing, assigning, tracking and monitoring on-demand goods and service delivery tasks and agents.\n\n\n== D ==\nDatabricks, a company founded by the creators of Apache Spark\nDataiku, a French computer software company\nDataStax\nDomo\n\n\n== F ==\nFluentd\n\n\n== G ==\nGreenplum\nGroundhog Technologies\n\n\n== H ==\nHack/reduce\nHazelcast\nHortonworks\nHPCC Systems\n\n\n== I ==\nIBM\nImply Corporation\n\n\n== M ==\nMapR\nMarkLogic\nMedio\nMedopad\n\n\n== N ==\nNetApp\n\n\n== O ==\nOracle Cloud Platform\n\n\n== P ==\nPalantir Technologies\nPentaho, a data integration and business analytics company with an enterprise-class, open source-based platform for big data deployments\nPitney Bowes\nPlatfora\n\n\n== Q ==\nQumulo\n\n\n== R ==\nRocket Fuel Inc.\n\n\n== S ==\nSAP SE, offers the SAP Data Hub to connect data bases and other products through acquisition of Altiscale\nSalesforceIQ\nScyllaDB, developer of a database for big data\nSense Networks\nShanghai Data Exchange\nSK Telecom, developer of big data analytics platform Metatron Discovery\nSojern\nSplunk\nSumo Logic\n\n\n== T ==\nTeradata\nThetaRay\nTubeMogul\n\n\n== V ==\nVoloMetrix\n\n\n== Z ==\nZaloni, deployment and vendor agnostic data lake management platform\nZoomdata\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/List_of_big_data_companies"}, "Data lake": {"title": "Data lake", "content": "A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics, and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs), and binary data (images, audio, video). A data lake can be established \"on premises\" (within an organization's data centers) or \"in the cloud\" (using cloud services).\n\n\n== Background ==\nJames Dixon, then chief technology officer at Pentaho, coined the term by 2011 to contrast it with data mart, which is a smaller repository of interesting attributes derived from raw data. In promoting data lakes, he argued that data marts have several inherent problems, such as information siloing. PricewaterhouseCoopers (PwC) said that data lakes could \"put an end to data silos\". In their study on data lakes they noted that enterprises were \"starting to extract and place data for analytics into a single, Hadoop-based repository.\"\n\n\n== Examples ==\nMany companies use cloud storage services such as Google Cloud Storage and  Amazon S3 or a distributed file system such as  Apache Hadoop distributed file system (HDFS). There is a gradual academic interest in the concept of data lakes.  For example, Personal DataLake at Cardiff University is a new type of data lake which aims at managing big data of individual users by providing a single point of collecting, organizing, and sharing personal data. \nEarly data lakes, such as Hadoop 1.0, had limited capabilities because it only supported batch-oriented processing (Map Reduce). Interacting with it required expertise in Java, map reduce and higher-level tools like Apache Pig, Apache Spark and Apache Hive (which were also originally batch-oriented).\n\n\n== Criticism ==\nPoorly-managed data lakes have been facetiously called data swamps.\nIn June 2015, David Needle characterized \"so-called data lakes\" as \"one of the more controversial ways to manage big data\". PwC was also careful to note in their research that not all data lake initiatives are successful. They quote Sean Martin, CTO of Cambridge Semantics:\n\nWe see customers creating big data graveyards, dumping everything into Hadoop distributed file system (HDFS) and hoping to do something with it down the road. But then they just lose track of what\u2019s there. The main challenge is not creating a data lake, but taking advantage of the opportunities it presents.\nThey describe companies that build successful data lakes as gradually maturing their lake as they figure out which data and metadata are important to the organization. \nAnother criticism is that the term \"data lake\" is not useful because it is used in so many different ways.  It may be used to refer to, for example: any tools or data management practices that are not data warehouses; a particular technology for implementation; a raw data reservoir; a hub for ETL offload; or a central hub for self-service analytics. \nWhile critiques of data lakes are warranted, in many cases they apply to other data projects as well. For example, the definition of \u201cdata warehouse\u201d is also changeable, and not all data warehouse efforts have been successful. In response to various critiques, McKinsey noted that the data lake should be viewed as a service model for delivering business value within the enterprise, not a technology outcome.\n\n\n== Data lakehouses ==\nData lakehouses are a hybrid approach that can ingest a variety of raw data formats like a data lake, yet provide ACID transactions and enforce data quality like a data warehouse. A data lakehouse architecture attempts to address several criticisms of data lakes by adding data warehouse capabilities such as transaction support, schema enforcement, governance, and support for diverse workloads. According to Oracle, data lakehouses combine the \"flexible storage of unstructured data from a data lake and the management features and tools from data warehouses\".\n\n\n== See also ==\nAzure Data Lake\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Data_lake"}, "Data management": {"title": "Data management", "content": "Data management comprises all disciplines related to handling data as a valuable resource, it is  the practice of managing an organization's data so it can be analyzed for decision making.\n\n\n== Concept ==\n\nThe concept of data management arose in the 1980s as technology moved from sequential processing  (first punched cards, then magnetic tape) to random access storage.\nSince it was now possible to store a discrete fact and quickly access it using random access disk technology, those suggesting that data management was more important than business process management used arguments such as \"a customer's home address is stored in 75 (or some other large number) places in our computer systems.\" However, during this period, random access processing was not competitively fast, so those suggesting \"process management\" was more important than \"data management\" used batch processing time as their primary argument.\nAs application software evolved into real-time, interactive usage, it became obvious that both management processes were important. If the data was not well defined, the data would be mis-used in applications. If the process wasn't well defined, it was impossible to meet user needs.\n\n\n== Patterns ==\nFollowings are common data management patterns:\n\nCache-aside\ncommand query responsibility segregation (CQRS)\nEvent sourcing\nIndex table\nMaterialized view\nSharding\nValet key\nStatic content hosting\n\n\n== Topics ==\nTopics in data management include:\n\n\n== Usage ==\n\nIn modern management usage, the term data is increasingly replaced by information or even knowledge in a non-technical context. Thus data management has become information management or knowledge management. This trend obscures the raw data processing and renders interpretation implicit. The distinction between data and derived value is illustrated by the information ladder.\nHowever, data has staged a comeback with the popularisation of the term big data, which refers to the collection and analyses of massive sets of data. While big data is a recent phenomenon, the requirement for data to aid decision-making traces back to the early 1970s with the emergence of decision support systems (DSS). These systems can be considered as the initial iteration of data management for decision support.\nSeveral organisations have established data management centers (DMC) for their operations.\n\n\n== Data sources ==\nMarketers and marketing organizations have been using data collection and analysis to refine their operations for the last few decades. Marketing departments in organizations and marketing companies conduct data collection and analysis by collecting data from different data sources and analyzing them to come up with insightful data they can use for strategic decision-making (Baier et al., 2012). In the modern business environment, data has evolved into a crucial asset for businesses since businesses use data as a strategic asset that is used regularly to create a competitive advantage and improve customer experiences. Among the most significant forms of data is customer information which is a critical asset used to assess customer behavior and trends and use it for developing new strategies for improving customer experience (Ahmed, 2004). However, data has to be of high quality to be used as a business asset for creating a competitive advantage. Therefore, data governance is a critical element of data collection and analysis since it determines the quality of data while integrity constraints guarantee the reliability of information collected from data sources. Various technologies including Big Data are used by businesses and organizations to allow users to search for specific information from raw data by grouping it based on the preferred criteria marketing departments in organizations could apply for developing targeted marketing strategies (Ahmed, 2004). As technology evolves, new forms of data are being introduced for analysis and classification purposes in marketing organizations and businesses. The introduction of new gadgets such as Smartphones and new-generation PCs has also introduced new data sources from which organizations can collect, analyze and classify data when developing marketing strategies. Retail businesses are the business category that uses customer data from smart devices and websites to understand how their current and targeted customers perceive their services before using the information to make improvements and increase customer satisfaction (Cerchiello and Guidici, 2012). Analyzing customer data is crucial for businesses since it allows marketing teams to understand customer behavior and trends which makes a considerable difference during the development of new marketing campaigns and strategies. Retailers who use customer data from various sources gain an advantage in the market since they can develop data-informed strategies for attracting and retaining customers in the overly competitive business environment. Based on the information on the benefits of data collection and analysis, the following hypotheses are proposed: The sources of data used as the foundation of data collection and analysis have a considerable impact on the data analysis tools used for analyzing and categorizing data.\n\n\n== Data analysis tools ==\nOrganizations use various data analysis tools for discovering unknown information and insights from huge databases; this allows organizations to discover new patterns that were not known to them or extract buried information before using it to come up with new patterns and relationships (Ahmed, 2004). There are 2 main categories of data analysis tools, data mining tools and data profiling tools. Also, most commercial data analysis tools are used by organizations for extracting, transforming and loading ETL for data warehouses in a manner that ensures no element is left out during the process (Turban et al., 2008). Thus the data analysis tools are used for supporting the 3 Vs in Big Data: volume, variety and velocity. Factor velocity emerged in the 1980s as one of the most important procedures in data analysis tools which was widely used by organizations for market research. The tools used to select core variables from the data that was collected from various sources and analyzed it; if the amount of data used to be too huge for humans to understand via manual observation, factor analysis would be introduced to distinguish between qualitative and quantitative data (Stewart, 1981). Organizations collect data from numerous sources including websites, emails and customer devices before conducting data analysis. Collecting data from numerous sources and analyzing it using different data analysis tools has its advantages, including overcoming the risk of method bias; using data from different sources and analyzing it using multiple analysis methods guarantees businesses and organizations robust and reliable findings they can use in decision making.  On the other hand, researchers use modern technologies to analyze and group data collected from respondents in the form of images, audio and video files by applying algorithms and other analysis software Berry et al., 1997). Researchers and marketers can then use the information obtained from the new generation analysis tools and methods for forecasting, decision support and making estimations for decision making. For instance, information from different data sources on demand forecasts can help a retail business determine the amount of stock required in an upcoming season depending on data from previous seasons. The analysis can allow organizations to make data-informed decisions to gain competitive advantage in an era where all businesses and organizations are capitalizing on emerging technologies and business intelligence tools to gain competitive edges. While there are numerous analysis tools in the market, Big Data analytics is the most common and advanced technology that has led to the following hypothesis: Data analytic tools used to analyze data collected from numerous data sources determine the quality and reliability of data analysis.\n\n\n== Data security and data privacy ==\nWhile organizations need to use quality data collection and analysis tools to guarantee the quality and reliability of the customer data they collect, they must implement security and privacy strategies to protect the data and customer information from privacy leaks (Van Till, 2013). A study conducted by PWC indicated that more than two-thirds of retail customers prefer purchasing products and services from businesses that have data protection and privacy plans for protecting customer information. Also, the study indicated that customers trust businesses that can prove they cannot use customer data for any other purposes other than marketing. As technology and the Internet continue improving, the success of businesses using it as a platform for marketing their products will depend on how effectively they can gain and maintain the trust of customers and users. Therefore, businesses will have to introduce and implement effective data protection and privacy strategies to protect business data and customer privacy. Although developing trust between customers and businesses affects the customers\u2019 purchasing intentions, it also has a considerable impact on long-term purchasing behaviors including how frequently customers purchase which could impact the profitability of a business in the long run. Thus, the above information leads to the following hypothesis: Implementing data security and privacy plans has a positive impact on economic and financial outcomes.\n\n\n== Financial and economic outcomes ==\nStudies indicate that customer transactions account for a 40% increase in the data collected annually, which means that financial data has a considerable impact on business decisions. Therefore, modern organizations are using big data analytics to identify 5 to 10 new data sources that can help them collect and analyze data for improved decision-making. Jonsen (2013) explains that organizations using average analytics technologies are 20% more likely to gain higher returns compared to their competitors who have not introduced any analytics capabilities in their operations. Also, IRI reported that the retail industry could experience an increase of more than $10 billion each year resulting from the implementation of modern analytics technologies. Therefore, the following hypothesis can be proposed: Economic and financial outcomes can impact how organizations use data analytics tools.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n Media related to Data management at Wikimedia Commons", "link": "https://en.wikipedia.org/wiki/Data_management"}, "Data analysis": {"title": "Data analysis", "content": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.\nData mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.\nData integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.\n\n\n== Data analysis process ==\n\nAnalysis refers to dividing a whole into its separate components for individual examination. Data analysis is a process for obtaining raw data, and subsequently converting it into information useful for decision-making by users. Data is collected and analyzed to answer questions, test hypotheses, or disprove theories.\n\nStatistician John Tukey, defined data analysis in 1961, as:\"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases. The CRISP framework, used in data mining, has similar steps.\n\n\n=== Data requirements ===\nThe data is necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analytics (or customers, who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained.  Data may be numerical or categorical (i.e., a text label for numbers).\n\n\n=== Data collection ===\nData is collected from a variety of sources. A list of data sources are available for study & research. The requirements may be communicated by analysts to custodians of the data; such as, Information Technology personnel within an organization. Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. The data may also be collected from sensors in the environment, including traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.\n\n\n=== Data processing ===\n\nData, when initially obtained, must be processed or organized for analysis. For instance, these may involve placing data into rows and columns in a table format (known as structured data) for further analysis, often through the use of spreadsheet or statistical software.\n\n\n=== Data cleaning ===\n\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that the datum are entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example; with financial information, the totals for particular variables may be compared against separately published numbers that are believed to be reliable. Unusual amounts, above or below predetermined thresholds, may also be reviewed.  There are several types of data cleaning, that are dependent upon the type of data in the set; this could be phone numbers, email addresses, employers, or other values. Quantitative data methods for outlier detection, can be used to get rid of data that appears to have a higher likelihood of being input incorrectly. Textual data spell checkers can be used to lessen the amount of mistyped words. However, it is harder to tell if the words themselves are correct.\n\n\n=== Exploratory data analysis ===\nOnce the datasets are cleaned, they can then be analyzed. Analysts may apply a variety of techniques, referred to as exploratory data analysis, to begin understanding the messages contained within the obtained data. The process of data exploration may result in additional data cleaning or additional requests for data; thus, the initialization of the iterative phases mentioned in the lead paragraph of this section. Descriptive statistics, such as, the average or median, can be generated to aid in understanding the data. Data visualization is also a technique used, in which the analyst is able to examine the data in a graphical format in order to obtain additional insights, regarding the messages within the data.\n\n\n=== Modeling and algorithms ===\nMathematical formulas or models (also known as algorithms), may be applied to the data in order to identify relationships among the variables; for example, using correlation or causation. In general terms, models may be developed to evaluate a specific variable based on other variable(s) contained within the dataset, with some residual error depending on the implemented model's accuracy (e.g., Data = Model + Error).\nInferential statistics includes utilizing techniques that measure the relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X), provides an explanation for the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as (Y = aX + b + error), where the model is designed such that (a) and (b) minimize the error when the model predicts Y for a given range of values of X. Analysts may also attempt to build models that are descriptive of the data, in an aim to simplify analysis and communicate results.\n\n\n=== Data product ===\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. For instance, an application that analyzes data about customer purchase history, and uses the results to recommend other purchases the customer might enjoy.\n\n\n=== Communication ===\n\nOnce data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.\nWhen determining how to communicate the results, the analyst may consider implementing a variety of data visualization techniques to help communicate the message more clearly and efficiently to the audience. Data visualization uses information displays (graphics such as, tables and charts) to help communicate key messages contained in the data. Tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers; while charts (e.g., bar charts or line charts), may help explain the quantitative messages contained in the data.\n\n\n== Quantitative messages ==\n\nStephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\n\nTime-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.\nRanking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by salespersons (the category, with each salesperson a categorical subdivision) during a single period.  A bar chart may be used to show the comparison across the salespersons.\nPart-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%).  A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.\nDeviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period.  A bar chart can show the comparison of the actual versus the reference amount.\nFrequency distribution: Shows the number of observations of a particular variable for a given interval, such as the number of years in which the stock market return is between intervals such as 0\u201310%, 11\u201320%, etc. A histogram, a type of bar chart, may be used for this analysis.\nCorrelation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.\nNominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.\nGeographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.\n\n\n== Analyzing quantitative data ==\n\nAuthor Jonathan Koomey has recommended a series of best practices for understanding quantitative data.  These include:\n\nCheck raw data for anomalies prior to performing an analysis;\nRe-perform important calculations, such as verifying columns of data that are formula driven;\nConfirm main totals are the sum of subtotals;\nCheck relationships between numbers that should be related in a predictable way, such as ratios over time;\nNormalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;\nBreak problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.\nFor the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.\n\n The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as \"Mutually Exclusive and Collectively Exhaustive\" or MECE.  For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as the revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).\nAnalysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.\nRegression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.\nNecessary condition analysis (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.\n\n\n== Analytical activities of data users ==\n\nUsers may have particular data points of interest within a data set, as opposed to the general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.\n\n\n== Barriers to effective analysis ==\nBarriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.\n\n\n=== Confusing fact and opinion ===\n\nEffective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011\u20132020 time period would add approximately $3.3 trillion to the national debt. Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.\nAs another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects\". This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\n\n\n=== Cognitive biases ===\nThere are a variety of cognitive biases that can adversely affect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.\nAnalysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.\n\n\n=== Innumeracy ===\nEffective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate.  Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.\nFor example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.\nAnalysts may also analyze data under different assumptions or scenario. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock.  Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.\n\n\n== Other topics ==\n\n\n=== Smart buildings ===\nA data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.\n\n\n=== Analytics and business intelligence ===\n\nAnalytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that uses data to understand and analyze business performance to drive decision-making .\n\n\n=== Education ===\nIn education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators\u2019 data analyses.\n\n\n== Practitioner notes ==\nThis section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.\n\n\n=== Initial data analysis ===\nThe most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:\n\n\n==== Quality of data ====\nThe quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms), normal imputation is needed.\n\nAnalysis of extreme observations: outlying observations in the data are analyzed to see if they seem to disturb the distribution.\nComparison and correction of differences in coding schemes: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.\nTest for common-method variance.\nThe choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.\n\n\n==== Quality of measurements ====\nThe quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.\nThere are two ways to assess measurement quality:\n\nConfirmatory factor analysis\nAnalysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach's \u03b1 of the scales, and the change in the Cronbach's alpha when an item would be deleted from a scale\n\n\n==== Initial transformations ====\nAfter assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.\nPossible transformations of variables are:\n\nSquare root transformation (if the distribution differs moderately from normal)\nLog-transformation (if the distribution differs substantially from normal)\nInverse transformation (if the distribution differs severely from normal)\nMake categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)\n\n\n==== Did the implementation of the study fulfill the intentions of the research design? ====\nOne should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.Other possible data distortions that should be checked are:\n\ndropout (this should be identified during the initial data analysis phase)\nItem non-response (whether this is random or not should be assessed during the initial data analysis phase)\nTreatment quality (using manipulation checks).\n\n\n==== Characteristics of data sample ====\nIn any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.The characteristics of the data sample can be assessed by looking at:\n\nBasic statistics of important variables\nScatter plots\nCorrelations and associations\nCross-tabulations\n\n\n==== Final stage of the initial data analysis ====\nDuring the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:\n\nIn the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?\nIn the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?\nIn the case of outliers: should one use robust analysis techniques?\nIn case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?\nIn the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?\nIn case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?\n\n\n==== Analysis ====\nSeveral analyses can be used during the initial data analysis phase:\n\nUnivariate statistics (single variable)\nBivariate associations (correlations)\nGraphical techniques (scatter plots)\nIt is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:\n\nNominal and ordinal variables\nFrequency counts (numbers and percentages)\nAssociations\ncircumambulations (crosstabulations)\nhierarchical loglinear analysis (restricted to a maximum of 8 variables)\nloglinear analysis (to identify relevant/important variables and possible confounders)\nExact tests or bootstrapping (in case subgroups are small)\nComputation of new variables\nContinuous variables\nDistribution\nStatistics (M, SD, variance, skewness, kurtosis)\nStem-and-leaf displays\nBox plots\n\n\n==== Nonlinear analysis ====\nNonlinear analysis is often necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods.  Nonlinear data analysis is closely related to nonlinear system identification.\n\n\n=== Main data analysis ===\nIn the main analysis phase, analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.\n\n\n==== Exploratory and confirmatory approaches ====\nIn the main analysis phase, either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.\nExploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.\n\n\n==== Stability of results ====\nIt is important to obtain some indication about how generalizable the results are. While this is often difficult to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing that.\n\nCross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.\nSensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.\n\n\n== Free software for data analysis ==\nNotable free software for data analysis include:\n\nDevInfo \u2013 A database system endorsed by the United Nations Development Group for monitoring and analyzing human development.\nELKI \u2013 Data mining framework in Java with data mining oriented visualization functions.\nKNIME \u2013 The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\nOrange \u2013 A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.\nPandas \u2013 Python library for data analysis.\nPAW \u2013 FORTRAN/C data analysis framework developed at CERN.\nR \u2013 A programming language and software environment for statistical computing and graphics.\nROOT \u2013  C++ data analysis framework developed at CERN.\nSciPy \u2013 Python library for scientific computing.\nJulia \u2013 A programming language well-suited for numerical analysis and computational science.\n\n\n== Reproducible analysis ==\nThe typical data analysis workflow involves collecting data, running analyses through various scripts, creating visualizations, and writing reports. However, this workflow presents challenges, including a separation between analysis scripts and data, as well as a gap between analysis and documentation. Often, the correct order of running scripts is only described informally or resides in the data scientist's memory. The potential for losing this information creates issues for reproducibility. To address these challenges, it is essential to have analysis scripts written for automated, reproducible workflows. Additionally, dynamic documentation is crucial, providing reports that are understandable by both machines and humans, ensuring accurate representation of the analysis workflow even as scripts evolve.\n\n\n== International data analysis contests ==\nDifferent companies or organizations hold data analysis contests to encourage researchers to utilize their data or to solve a particular question using data analysis. A few examples of well-known international data analysis contests are as follows: \n\nKaggle competition, which is held by Kaggle.\nLTPP data analysis contest held by FHWA and ASCE.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Bibliography ===\nAd\u00e8r, Herman J. (2008a). \"Chapter 14: Phases and initial steps in data analysis\". In Ad\u00e8r, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 333\u2013356. ISBN 9789079418015. OCLC 905799857.\nAd\u00e8r, Herman J. (2008b). \"Chapter 15: The main analysis phase\". In Ad\u00e8r, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 357\u2013386. ISBN 9789079418015. OCLC 905799857.\nTabachnick, B.G. & Fidell, L.S. (2007). Chapter 4: Cleaning up your act. Screening data prior to analysis. In B.G. Tabachnick & L.S. Fidell (Eds.), Using Multivariate Statistics, Fifth Edition (pp. 60\u2013116). Boston: Pearson Education, Inc. / Allyn and Bacon.\n\n\n== Further reading ==\n\nAd\u00e8r, H.J. & Mellenbergh, G.J. (with contributions by D.J. Hand) (2008). Advising on Research Methods: A Consultant's Companion. Huizen, the Netherlands: Johannes van Kessel Publishing.  ISBN 978-90-79418-01-5\nChambers, John M.; Cleveland, William S.; Kleiner, Beat; Tukey, Paul A. (1983). Graphical Methods for Data Analysis, Wadsworth/Duxbury Press. ISBN 0-534-98052-X\nFandango, Armando (2017). Python Data Analysis, 2nd Edition. Packt Publishers. ISBN 978-1787127487\nJuran, Joseph M.; Godfrey, A. Blanton (1999). Juran's Quality Handbook, 5th Edition. New York: McGraw Hill. ISBN 0-07-034003-X\nLewis-Beck, Michael S. (1995). Data Analysis: an Introduction, Sage Publications Inc, ISBN 0-8039-5772-6\nNIST/SEMATECH (2008) Handbook of Statistical Methods,\nPyzdek, T, (2003). Quality Engineering Handbook, ISBN 0-8247-4614-7\nRichard Veryard (1984). Pragmatic Data Analysis. Oxford : Blackwell Scientific Publications. ISBN 0-632-01311-7\nTabachnick, B.G.; Fidell, L.S. (2007). Using Multivariate Statistics, 5th Edition. Boston: Pearson Education, Inc. / Allyn and Bacon, ISBN 978-0-205-45938-4", "link": "https://en.wikipedia.org/wiki/Data_analysis"}}, "Cloud Computing": {"Cloud computing": {"title": "Cloud computing", "content": "Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each of which is a data center. Cloud computing relies on sharing of resources to achieve coherence and typically uses a pay-as-you-go model, which can help in reducing capital expenses but may also lead to unexpected operating expenses for users.\n\n\n== Definition ==\nA European Commission communication issued in 2012 argued that the breadth of scope offered by cloud computing made a general definition \"elusive\", whereas the United States National Institute of Standards and Technology's 2011 definition of cloud computing identified \"five essential characteristics\":\n\nOn-demand self-service. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.\nBroad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).\nResource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. \nRapid elasticity. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.\nMeasured service. Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service, although for some organisations the revenue impact of high usage may affect profitability, compared to an option of sunk capital costs.\n\n\n== History ==\n\nCloud computing has a rich history which extends back to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This was a time of exploration and experimentation with ways to make large-scale computing power available to more users through time-sharing, optimizing the infrastructure, platform, and applications, and increasing efficiency for end users.\nThe \"cloud\" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of \"places\" that mobile agents in the Telescript environment could \"go\". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with \"cloud computing-enabled applications\". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers.\nIn the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 the beta version of Google Docs was released, Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2), in 2008 NASA's development of the first open-source software for deploying private and hybrid clouds.\nThe following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities.\nSince the global pandemic of 2020, cloud technology has surged in popularity due to the level of data security it offers and the flexibility of working options it provides for all employees, notably remote workers.\n\n\n== Value proposition ==\nAdvocates of public and hybrid clouds claim that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand, providing burst computing capability: high computing power at certain periods of peak demand.\nAdditional value propositions of cloud computing include:\n\n\n== Challenges and limitations ==\n\nOne of the main challenges of cloud computing, in comparison to more traditional on-premises computing, is data security and privacy. Cloud users entrust their sensitive data to third-party providers, who may not have adequate measures to protect it from unauthorized access, breaches, or leaks. Cloud users also face compliance risks if they have to adhere to certain regulations or standards regarding data protection, such as GDPR or HIPAA.\nAnother challenge of cloud computing is reduced visibility and control. Cloud users may not have full insight into how their cloud resources are managed, configured, or optimized by their providers. They may also have limited ability to customize or modify their cloud services according to their specific needs or preferences. Complete understanding of all technology may be impossible, especially given the scale, complexity, and deliberate opacity of contemporary systems; however, there is a need for understanding complex technologies and their interconnections to have power and agency within them. The metaphor of the cloud can be seen as problematic as cloud computing retains the aura of something noumenal and numinous; it is something experienced without precisely understanding what it is or how it works.\nAdditionally, cloud migration is a significant challenge. This process involves transferring data, applications, or workloads from one cloud environment to another, or from on-premises infrastructure to the cloud. Cloud migration can be complicated, time-consuming, and expensive, particularly when there are compatibility issues between different cloud platforms or architectures. If not carefully planned and executed, cloud migration can lead to downtime, reduced performance, or even data loss.\n\n\n=== Security and privacy ===\n\nCloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information. Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored. Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access. Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity. The systems work by creating and describing identities, recording activities, and getting rid of unused identities.\nAccording to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure\u2014which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. \"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into\". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack\u2014a process he called \"hyperjacking\". Some examples of this include the Dropbox security breach, and iCloud 2014 leak. Dropbox had been breached in October 2014, having over seven million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).\nThere is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership. Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services. Some small businesses that do not have expertise in IT security could find that it is more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes do not read the many pages of the terms of service agreement, and just click \"Accept\" without reading). This is important now that cloud computing is common and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Assistant). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.\nThe attacks that can be made on cloud computing systems include man-in-the middle attacks, phishing attacks, authentication attacks, and malware attacks. One of the largest threats is considered to be malware attacks, such as Trojan horses. Recent research conducted in 2022 has revealed that the Trojan horse injection method is a serious problem with harmful impacts on cloud computing systems.\n\n\n== Service models ==\n\nThe service-oriented architecture (SOA) promotes the idea of \"Everything as a Service\" (EaaS or XaaS, or simply aAsS). This concept is operationalized in cloud computing through several service models as defined by the National Institute of Standards and Technology (NIST). The three standard service models are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). They are commonly depicted as layers in a stack, providing different levels of abstraction. However, these layers are not necessarily interdependent. For instance, SaaS can be delivered on bare metal, bypassing PaaS and IaaS, and a program can run directly on IaaS without being packaged as SaaS.\n\n\n=== Infrastructure as a service (IaaS) ===\n\nInfrastructure as a service (IaaS) refers to online services that provide high-level APIs used to abstract various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A hypervisor runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. The use of containers offers higher performance than virtualization because there is no hypervisor overhead. IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.\n\nThe NIST's definition of cloud computing describes IaaS as \"where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).\"\nIaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the number of resources allocated and consumed.\n\n\n=== Platform as a service (PaaS) ===\n\nThe NIST's definition of cloud computing defines Platform as a Service as:\n\nThe capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment.\nPaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including an operating system, programming-language execution environment, database, and the web server. Application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers. With some PaaS, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually.\nSome integration and data management providers also use specialized applications of PaaS as delivery models for data. Examples include iPaaS (Integration Platform as a Service) and dPaaS (Data Platform as a Service). iPaaS enables customers to develop, execute and govern integration flows. Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware. dPaaS delivers integration\u2014and data-management\u2014products as a fully managed service. Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of programs by building data applications for the customer. dPaaS users access data through data-visualization tools.\n\n\n=== Software as a service (SaaS) ===\n\nThe NIST's definition of cloud computing defines Software as a Service as:\n\nThe capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings.\nIn the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as \"on-demand software\" and is usually priced on a pay-per-use basis or using a subscription fee. In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability\u2014which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand. Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.\nThe pricing model for SaaS applications is typically a monthly or yearly flat fee per user, so prices become scalable and adjustable if users are added or removed at any point. It may also be free. Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result, there could be unauthorized access to the data. Examples of applications offered as SaaS are games and productivity software like Google Docs and Office Online. SaaS applications may be integrated with cloud storage or File hosting services, which is the case with Google Docs being integrated with Google Drive, and Office Online being integrated with OneDrive.\n\n\n=== \"Backend\" as a service (BaaS) ===\n\nIn the \"backend\" as a service (m) model, also known as \"mobile backend as a service\" (MBaaS), web app and mobile app developers are provided with a way to link their applications to cloud storage and cloud computing services with application programming interfaces (APIs) exposed to their applications and custom software development kits (SDKs). Services include user management, push notifications, integration with social networking services and more. This is a relatively recent model in cloud computing, with most BaaS startups dating from 2011 or later but trends indicate that these services are gaining significant mainstream traction with enterprise consumers.\n\n\n=== Serverless computing or Function-as-a-Service (FaaS) ===\n\nServerless computing is a cloud computing code execution model in which the cloud provider fully manages starting and stopping virtual machines as necessary to serve requests. Requests are billed by an abstract measure of the resources required to satisfy the request, rather than per virtual machine per hour. Despite the name, serverless computing does not actually involve running code without servers. The business or person using the system does not have to purchase, rent or provide servers or virtual machines for the back-end code to run on.\nFunction as a Service (FaaS) is a remote procedure call hosted as a service, leveraging serverless computing to deploy individual functions in the cloud that run in response to specific events. Some consider FaaS to fall under the umbrella of serverless computing, while others use the terms interchangeably.\n\n\n== Deployment models ==\n\nThe deployment of services to the cloud is referred to as cloud migration. \nReverse cloud migration, also known as cloud repatriation, refers to moving cloud-based workloads back to on-premises infrastructures including enterprise data centers, colocation providers, and managed service providers. Cloud repatriation occurs due to security concerns, costs, performance issues, compatibility problems, and uptime concerns.\n\n\n=== Private ===\nPrivate cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally. Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users \"still have to buy, build, and manage them\" and thus do not benefit from less hands-on management, essentially \"[lacking] the economic model that makes cloud computing such an intriguing concept\".\n\n\n=== Public ===\n\nCloud services are considered \"public\" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge. Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications.\nSeveral factors like the functionality of the solutions, cost, integrational and organizational aspects as well as safety & security are influencing the decision of enterprises and organizations to choose a public cloud or on-premises solution.\n\n\n=== Hybrid ===\n\nHybrid cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources, that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources. Gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers. A hybrid cloud service crosses isolation and provider boundaries so that it cannot be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.\nVaried use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service. This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.\nAnother example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud. This capability enables hybrid clouds to employ cloud bursting for scaling across clouds. Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and \"bursts\" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed. Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.\n\n\n=== Others ===\n\n\n==== Community ====\nCommunity cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether it is managed internally or by a third-party, and hosted internally or externally, the costs are distributed among fewer users compared to a public cloud (but more than a private cloud). As a result, only a portion of the potential cost savings of cloud computing is achieved.\n\n\n==== Distributed ====\nA cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.\n\nPublic-resource computing \u2013 This type of distributed cloud results from an expansive definition of cloud computing, because they are more akin to distributed computing than cloud computing. Nonetheless, it is considered a sub-class of cloud computing.\nVolunteer cloud \u2013 Volunteer cloud computing is characterized as the intersection of public-resource computing and cloud computing, where a cloud computing infrastructure is built using volunteered resources. Many challenges arise from this type of infrastructure, because of the volatility of the resources used to build it and the dynamic environment it operates in. It can also be called peer-to-peer clouds, or ad-hoc clouds. An interesting effort in such direction is Cloud@Home, it aims to implement a cloud computing infrastructure using volunteered resources providing a business-model to incentivize contributions through financial restitution.\n\n\n==== Multi ====\n\nMulticloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).\n\n\n==== Poly ====\nPoly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more than could be done with a single provider.\n\n\n==== Big data ====\nThe issues of transferring large amounts of data to the cloud as well as data security once the data is in the cloud initially hampered adoption of cloud for big data, but now that much data originates in the cloud and with the advent of bare-metal servers, the cloud has become a solution for use cases including business analytics and geospatial analysis.\n\n\n==== HPC ====\nHPC cloud refers to the use of cloud computing services and infrastructure to execute high-performance computing (HPC) applications. These applications consume a considerable amount of computing power and memory and are traditionally executed on clusters of computers. In 2016 a handful of companies, including R-HPC, Amazon Web Services, Univa, Silicon Graphics International, Sabalcore, Gomput, and Penguin Computing offered a high-performance computing cloud. The Penguin On Demand (POD) cloud was one of the first non-virtualized remote HPC services offered on a pay-as-you-go basis. Penguin Computing launched its HPC cloud in 2016 as an alternative to Amazon's EC2 Elastic Compute Cloud, which uses virtualized computing nodes.\n\n\n== Architecture ==\n\nCloud architecture, the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.\n\n\n=== Cloud engineering ===\nCloud engineering is the application of engineering disciplines of cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization and governance in conceiving, developing, operating and maintaining cloud computing systems. It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information technology engineering, security, platform, risk, and quality engineering.\n\n\n== Market ==\nAccording to International Data Corporation (IDC), global spending on cloud computing services has reached $706 billion and is expected to reach $1.3 trillion by 2025. Gartner estimated that global public cloud services end-user spending would reach $600 billion by 2023. According to a McKinsey & Company report, cloud cost-optimization levers and value-oriented business use cases foresee more than $1 trillion in run-rate EBITDA across Fortune 500 companies as up for grabs in 2030. In 2022, more than $1.3 trillion in enterprise IT spending was at stake from the shift to the cloud, growing to almost $1.8 trillion in 2025, according to Gartner.\nThe European Commission's 2012 Communication identified several issues which were impeding the development of the cloud computing market::\u200aSection 3\u200a\n\nfragmentation of the digital single market across the EU\nconcerns about contracts including reservations about data access and ownership, data portability, and change control\nvariations in standards applicable to cloud computing\nThe Communication set out a series of \"digital agenda actions\" which the Commission proposed to undertake in order to support the development of a fair and effective market for cloud computing services.:\u200aPages 6-14\u200a\n\n\n== List of public clouds ==\nAdobe Creative Cloud\nAmazon Web Services\nGoogle Cloud\nIBM Cloud\nMicrosoft Azure\nOpenStack\nOracle Cloud\nPanorama9\n\n\n== Similar concepts ==\nThe goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.\nCloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.\nCloud computing shares characteristics with:\n\nClient\u2013server model \u2013 Client\u2013server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients).\nComputer bureau \u2013 A service bureau providing computer services, particularly from the 1960s to 1980s.\nGrid computing \u2013 A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks.\nFog computing \u2013 Distributed computing paradigm that provides data, compute, storage and application services closer to the client or near-user edge devices, such as network routers. Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client-side (e.g. mobile devices), instead of sending data to a remote location for processing.\nUtility computing \u2013 The \"packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity.\"\nPeer-to-peer \u2013 A distributed architecture without the need for central coordination. Participants are both suppliers and consumers of resources (in contrast to the traditional client-server model).\nCloud sandbox \u2013 A live, isolated computer environment in which a program, code or file can run without affecting the application in which it runs.\n\n\n== Best practices ==\nAccording to Yan Cui, ephemeral resources should be kept together to achieve a high cohesion. However, shared resources that have a long spin-up time (e.g. AWS RDS cluster) and landing zone should have their own separate repository, deployment pipeline and stack. \n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nMillard, Christopher (2013). Cloud Computing Law. Oxford University Press. ISBN 978-0-19-967168-7.\nWeisser, Alexander (2020). International Taxation of Cloud Computing. Editions Juridiques Libres, ISBN 978-2-88954-030-3.\nSingh, Jatinder; Powles, Julia; Pasquier, Thomas; Bacon, Jean (July 2015). \"Data Flow Management and Compliance in Cloud Computing\". IEEE Cloud Computing. 2 (4): 24\u201332. doi:10.1109/MCC.2015.69. S2CID 9812531.\nArmbrust, Michael; Stoica, Ion; Zaharia, Matei; Fox, Armando; Griffith, Rean; Joseph, Anthony D.; Katz, Randy; Konwinski, Andy; Lee, Gunho; Patterson, David; Rabkin, Ariel (1 April 2010). \"A view of cloud computing\". Communications of the ACM. 53 (4): 50. doi:10.1145/1721654.1721672. S2CID 1673644.\nHu, Tung-Hui (2015). A Prehistory of the Cloud. MIT Press. ISBN 978-0-262-02951-3.\nMell, P. (2011, September). The NIST Definition of Cloud Computing. Retrieved November 1, 2015, from National Institute of Standards and Technology website\n\n Media related to Cloud computing at Wikimedia Commons", "link": "https://en.wikipedia.org/wiki/Cloud_computing"}, "Cloud computing security": {"title": "Cloud computing security", "content": "Cloud computing security or, more simply, cloud security, refers to a broad set of policies, technologies, applications, and controls utilized to protect virtualized IP, data, applications, services, and the associated infrastructure of cloud computing. It is a sub-domain of computer security, network security, and, more broadly, information security.\n\n\n== Security issues associated with the cloud ==\nCloud computing and storage provide users with the capabilities to store and process their data in third-party data centers. Organizations use the cloud in a variety of different service models (with acronyms such as SaaS, PaaS, and IaaS) and deployment models (private, public, hybrid, and community).\nSecurity concerns associated with cloud computing are typically categorized in two ways: as security issues faced by cloud providers (organizations providing software-, platform-, or infrastructure-as-a-service via the cloud) and security issues faced by their customers (companies or organizations who host applications or store data on the cloud). The responsibility is shared, however, and is often detailed in a cloud provider's \"shared security responsibility model\" or \"shared responsibility model.\" The provider must ensure that their infrastructure is secure and that their clients\u2019 data and applications are protected, while the user must take measures to fortify their application and use strong passwords and authentication measures.\nWhen an organization elects to store data or host applications on the public cloud, it loses its ability to have physical access to the servers hosting its information. As a result, potentially sensitive data is at risk from insider attacks. According to a 2010 Cloud Security Alliance report, insider attacks are one of the top seven biggest threats in cloud computing. Therefore, cloud service providers must ensure that thorough background checks are conducted for employees who have physical access to the servers in the data center. Additionally, data centers are recommended to be frequently monitored for suspicious activity.\nIn order to conserve resources, cut costs, and maintain efficiency, cloud service providers often store more than one customer's data on the same server. As a result, there is a chance that one user's private data can be viewed by other users (possibly even competitors). To handle such sensitive situations, cloud service providers should ensure proper data isolation and logical storage segregation.\nThe extensive use of virtualization in implementing cloud infrastructure brings unique security concerns for customers or tenants of a public cloud service.  Virtualization alters the relationship between the OS and underlying hardware \u2013 be it computing, storage or even networking. This introduces an additional layer \u2013 virtualization \u2013 that itself must be properly configured, managed and secured. Specific concerns include the potential to compromise the virtualization software, or \"hypervisor\". While these concerns are largely theoretical, they do exist. For example, a breach in the administrator workstation with the management software of the virtualization software can cause the whole data center to go down or be reconfigured to an attacker's liking.\n\n\n== Cloud security controls ==\nCloud security architecture is effective only if the correct defensive implementations are in place. An efficient cloud security architecture should recognize the issues that will arise with security management and follow all of the best practices, procedures, and guidelines to ensure a secure cloud environment. Security management addresses these issues with security controls. These controls protect cloud environments and are put in place to safeguard any weaknesses in the system and reduce the effect of an attack. While there are many types of controls behind a cloud security architecture, they can usually be found in one of the following categories:\n\nDeterrent controls\nThese controls are administrative mechanisms intended to reduce attacks on a cloud system and are utilized to ensure compliance with external controls. Much like a warning sign on a fence or a property, deterrent controls typically reduce the threat level by informing potential attackers that there will be adverse consequences for them if they proceed. (Some consider them a subset of preventive controls.) Examples of such controls could be considered as policies, procedures, standards, guidelines, laws, and regulations that guide an organization towards security. Although most malicious actors ignore such deterrent controls, such controls are intended to ward off those who are inexperienced or curious about compromising the IT infrastructure of an organization.\nPreventive controls\nThe main objective of preventive controls is to strengthen the system against incidents, generally by reducing if not actually eliminating vulnerabilities, as well as preventing unauthorized intruders from accessing or entering the system. This could be achieved by either adding software or feature implementations (such as firewall protection, endpoint protection, and multi-factor authentication), or removing unneeded functionalities so that the attack surface is minimized (as in unikernel applications). Additionally, educating individuals through security awareness training and exercises is included in such controls due to human error being the weakest point of security. Strong authentication of cloud users, for instance, makes it less likely that unauthorized users can access cloud systems, and more likely that cloud users are positively identified. All in all, preventative controls affect the likelihood of a loss event occurring and are intended to prevent or eliminate the systems\u2019 exposure to malicious action.\nDetective controls\nDetective controls are intended to detect and react appropriately to any incidents that occur. In the event of an attack, a detective control will signal the preventative or corrective controls to address the issue. Detective security controls function not only when such an activity is in progress and after it has occurred. System and network security monitoring, including intrusion detection and prevention arrangements, are typically employed to detect attacks on cloud systems and the supporting communications infrastructure. Most organizations acquire or create a dedicated security operations center (SOC), where dedicated members continuously monitor the organization\u2019s IT infrastructure through logs and Security Information and Event Management (SIEM) software. SIEMs are security solutions that help organizations and security teams analyze \u201clog data in real-time for swift detection of security incidents.\u201d SIEMS are not the only examples of detective controls. There are also Physical security controls, Intrusion detection systems, and anti-virus/anti-malware tools, which all have different functions centered around the exact purpose of detecting security compromises within an IT infrastructure.\nCorrective controls\nCorrective controls reduce the consequences of an incident, generally by limiting the damage. Such controls include technical, physical, and administrative measures that occur during or after an incident to restore the systems or resources to their previous state after a security incident. There are plenty of examples of corrective controls, both physical and technical. For instance, re-issuing an access card or repairing physical damage can be considered corrective controls. However, technical controls such as terminating a process and administrative controls such as implementing an incident response plan could also be considered corrective controls. Corrective controls are focused on recovering and repairing any damage caused by a security incident or unauthorized activity. The value is needed to change the function of security.\n\n\n== Dimensions of cloud security ==\nCloud security engineering is characterized by the security layers, plan, design, programming, and best practices that exist inside a cloud security arrangement. Cloud security engineering requires the composed and visual model (design and UI) to be characterized by the tasks inside the Cloud. This cloud security engineering process includes such things as access to the executives, techniques, and controls to ensure applications and information. It also includes ways to deal with and keep up with permeability, consistency, danger stance, and by and large security. Processes for imparting security standards into cloud administrations and activities assume an approach that fulfills consistent guidelines and essential framework security parts.\nFor interest in Cloud advancements to be viable, companies should recognize the various parts of the Cloud and how they remain to impact and help them. These interests may include investments in cloud computing and security, for example. This of course leads to leads to driving push for the Cloud advancements to succeed.\nThough the idea of cloud computing isn't new, associations are increasingly enforcing it because of its flexible scalability, relative trustability, and cost frugality of services. However, despite its rapid-fire relinquishment in some sectors and disciplines, it's apparent from exploration and statistics that security-related pitfalls are the most conspicuous hedge to its wide relinquishment.\nIt is generally recommended that information security controls be selected and implemented according to and in proportion to the risks, typically by assessing the threats, vulnerabilities and impacts. Cloud security concerns can be grouped in various ways; Gartner named seven while the Cloud Security Alliance identified twelve areas of concern. Cloud access security brokers (CASBs) are software that sits between cloud users and cloud applications to provide visibility into cloud application usage, data protection and governance to monitor all activity and enforce security policies.\n\n\n== Security and privacy ==\nAny service without a \"hardened\" environment is considered a \"soft\" target. Virtual servers should be protected just like a physical server against data leakage, malware, and exploited vulnerabilities. \"Data loss or leakage represents 24.6% and cloud related malware 3.4% of threats causing cloud outages\u201d.\n\n\n=== Identity management ===\nEvery enterprise will have its own identity management system to control access to information and computing resources. Cloud providers either integrate the customer's identity management system into their own infrastructure, using federation or SSO technology or a biometric-based identification system, or provide an identity management system of their own. CloudID, for instance, provides privacy-preserving cloud-based and cross-enterprise biometric identification. It links the confidential information of the users to their biometrics and stores it in an encrypted fashion. Making use of a searchable encryption technique, biometric identification is performed in the encrypted domain to make sure that the cloud provider or potential attackers do not gain access to any sensitive data or even the contents of the individual queries.\n\n\n=== Physical security ===\nCloud service providers physically secure the IT hardware (servers, routers, cables etc.) against unauthorized access, interference, theft, fires, floods etc. and ensure that essential supplies (such as electricity) are sufficiently robust to minimize the possibility of disruption.  This is normally achieved by serving cloud applications from professionally specified, designed, constructed, managed, monitored and maintained data centers.\n\n\n=== Personnel security ===\nVarious information security concerns relating to the IT and other professionals associated with cloud services are typically handled through pre-, para- and post-employment activities such as security screening potential recruits, security awareness and training programs, and proactive.\n\n\n=== Privacy ===\nProviders ensure that all critical data (credit card numbers, for example) are masked or encrypted and that only authorized users have access to data in its entirety. Moreover, digital identities and credentials must be protected as should any data that the provider collects or produces about customer activity in the cloud.\n\n\n=== Penetration testing ===\nPenetration testing is the process of performing offensive security tests on a system, service, or computer network to find security weaknesses in it. Since the cloud is a shared environment with other customers or tenants, following penetration testing rules of engagement step-by-step is a mandatory requirement. Scanning and penetration testing from inside or outside the cloud should be authorized by the cloud provider. Violation of acceptable use policies can lead to termination of the service.\n\n\n=== Cloud vulnerability and penetration testing ===\nScanning the cloud from outside and inside using free or commercial products is crucial because without a hardened environment your service is considered a soft target. Virtual servers should be hardened just like a physical server against data leakage, malware, and exploited vulnerabilities. \"Data loss or leakage represents 24.6% and cloud-related malware 3.4% of threats causing cloud outages\u201d\nScanning and penetration testing from inside or outside the cloud must be authorized by the cloud provider. Since the cloud is a shared environment with other customers or tenants, following penetration testing rules of engagement step-by-step is a mandatory requirement. Violation of acceptable use policies can lead to the termination of the service. Some key terminology to grasp when discussing penetration testing is the difference between application and network layer testing. Understanding what is asked of you as the tester is sometimes the most important step in the process. The network-layer testing refers to testing that includes internal/external connections as well as the interconnected systems throughout the local network. Oftentimes, social engineering attacks are carried out, as the most vulnerable link in security is often the employee.\nWhite-box testing\nTesting under the condition that the \u201cattacker\u201d has full knowledge of the internal network, its design, and implementation.\nGrey-box testing\nTesting under the condition that the \u201cattacker\u201d has partial knowledge of the internal network, its design, and implementation.\nBlack-box testing\nTesting under the condition that the \u201cattacker\u201d has no prior knowledge of the internal network, its design, and implementation.\n\n\n== Data security ==\nThere are numerous security threats associated with cloud data services. This includes traditional threats and non-traditional threats. Traditional threats include: network eavesdropping, illegal invasion, and denial of service attacks, but also specific cloud computing threats, such as side channel attacks, virtualization vulnerabilities, and abuse of cloud services. In order to mitigate these threats security controls often rely on monitoring the three areas of the CIA triad. The CIA Triad refers to confidentiality (including access controllability which can be further understood from the following.), integrity and availability.\nMany effective security measures cover several or all of the three categories. Encryption for example can be used to prevent unauthorized access, and also ensure integrity of the data). Backups on the other hand generally cover integrity and availability and firewalls only cover confidentiality and access controllability.\n\n\n=== Confidentiality ===\nData confidentiality is the property in that data contents are not made available or disclosed to illegal users. Outsourced data is stored in a cloud and out of the owners' direct control. Only authorized users can access the sensitive data while others, including CSPs, should not gain any information about the data. Meanwhile, data owners expect to fully utilize cloud data services, e.g., data search, data computation, and data sharing, without the leakage of the data contents to CSPs or other adversaries. Confidentiality refers to how data must be kept strictly confidential to the owner of said data\nAn example of security control that covers confidentiality is encryption so that only authorized users can access the data. Symmetric or asymmetric key paradigm can be used for encryption.\n\n\n=== Access controllability ===\nAccess controllability means that a data owner can perform the selective restriction of access to their data outsourced to the cloud. Legal users can be authorized by the owner to access the data, while others can not access it without permission. Further, it is desirable to enforce fine-grained access control to the outsourced data, i.e., different users should be granted different access privileges with regard to different data pieces. The access authorization must be controlled only by the owner in untrusted cloud environments.\nAccess control can also be referred to as availability. While unauthorized access should be strictly prohibited, access for administrative or even consumer uses should be allowed but monitored as well. Availability and Access control ensure that the proper amount of permissions is granted to the correct persons.\n\n\n=== Integrity ===\nData integrity demands maintaining and assuring the accuracy and completeness of data. A data owner always expects that her or his data in a cloud can be stored correctly and trustworthy. It means that the data should not be illegally tampered with, improperly modified, deliberately deleted, or maliciously fabricated. If any undesirable operations corrupt or delete the data, the owner should be able to detect the corruption or loss. Further, when a portion of the outsourced data is corrupted or lost, it can still be retrieved by the data users. Effective integrity security controls go beyond protection from malicious actors and protect data from unintentional alterations as well.\nAn example of security control that covers integrity is automated backups of information.\n\n\n== Risks and vulnerabilities of Cloud Computing ==\nWhile cloud computing is on the cutting edge of information technology there are risks and vulnerabilities to consider before investing fully in it. Security controls and services do exist for the cloud but as with any security system they are not guaranteed to succeed. Furthermore, some risks extend beyond asset security and may involve issues in productivity and even privacy as well.\n\n\n=== Privacy Concerns ===\nCloud computing is still an emerging technology and thus is developing in relatively new technological structures. As a result, all cloud services must undertake Privacy Impact Assessments or PIAs before releasing their platform. Consumers as well that intend to use clouds to store their customer's data must also be aware of the vulnerabilities of having non-physical storage for private information.\n\n\n=== Unauthorized Access to Management interface ===\nDue to the autonomous nature of the cloud, consumers are often given management interfaces to monitor their databases. By having controls in such a congregated location and by having the interface be easily accessible for convenience for users, there is a possibility that a single actor could gain access to the cloud's management interface; giving them a great deal of control and power over the database.\n\n\n=== Data Recovery Vulnerabilities ===\nThe cloud's capabilities with allocating resources as needed often result in resources in memory and otherwise being recycled to another user at a later event. For these memory or storage resources, it could be possible for current users to access information left by previous ones.\n\n\n=== Internet Vulnerabilities ===\nThe cloud requires an internet connection and therefore internet protocols to access. Therefore, it is open to many internet protocol vulnerabilities such as man-in-the-middle attacks. Furthermore, by having a heavy reliance on internet connectivity, if the connection fails consumers will be completely cut off from any cloud resources.\n\n\n=== Encryption Vulnerabilities ===\nCryptography is an ever-growing field and technology. What was secure 10 years ago may be considered a significant security risk by today's standards. As technology continues to advance and older technologies grow old, new methods of breaking encryptions will emerge as well as fatal flaws in older encryption methods. Cloud providers must keep up to date with their encryption as the data they typically contain is especially valuable.\n\n\n=== Legal issues ===\nPrivacy legislation often varies from country to country. By having information stored via the cloud it is difficult to determine under which jurisdictions the data falls under. Transborder clouds are especially popular given that the largest companies transcend several countries. Other legal dilemmas from the ambiguity of the cloud refer to how there is a difference in privacy regulation between information shared between and information shared inside of organizations.\n\n\n=== Attacks ===\nThere are several different types of attacks on cloud computing, one that is still very much untapped is infrastructure compromise. Though not completely known it is listed as the attack with the highest amount of payoff. What makes this so dangerous is that the person carrying out the attack is able to gain a level of privilege of having essentially root access to the machine. It is very hard to defend against attacks like these because they are so unpredictable and unknown, attacks of this type are also called zero day exploits because they are difficult to defend against since the vulnerabilities were previously unknown and unchecked until the attack has already occurred.\nDoS attacks aim to have systems be unavailable to their users. Since cloud computing software is used by large numbers of people, resolving these attacks is increasingly difficult. Now with cloud computing on the rise, this has left new opportunities for attacks because of the virtualization of data centers and cloud services being utilized more.\nWith the global pandemic that started early in 2020 taking effect, there was a massive shift to remote work, because of this companies became more reliant on the cloud. This massive shift has not gone unnoticed, especially by cybercriminals and bad actors, many of which saw the opportunity to attack the cloud because of this new remote work environment. Companies have to constantly remind their employees to keep constant vigilance especially remotely. Constantly keeping up to date with the latest security measures and policies, mishaps in communication are some of the things that these cybercriminals are looking for and will prey upon.\nMoving work to the household was critical for workers to be able to continue, but as the move to remote work happened, several security issues arose quickly. The need for data privacy, using applications, personal devices, and the internet all came to the forefront. The pandemic has had large amounts of data being generated especially in the healthcare sector. Big data is accrued for the healthcare sector now more than ever due to the growing coronavirus pandemic. The cloud has to be able to organize and share the data with its users securely. Quality of data looks for four things: accuracy, redundancy, completeness and consistency.\nUsers had to think about the fact that massive amounts of data are being shared globally. Different countries have certain laws and regulations that have to be adhered to. Differences in policy and jurisdiction give rise to the risk involved with the cloud. Workers are using their personal devices more now that they are working from home. Criminals see this increase as an opportunity to exploit people, software is developed to infect people's devices and gain access to their cloud. The current pandemic has put people in a situation where they are incredibly vulnerable and susceptible to attacks. The change to remote work was so sudden that many companies simply were unprepared to deal with the tasks and subsequent workload they have found themselves deeply entrenched in. Tighter security measures have to be put in place to ease that newfound tension within organizations.\nThe attacks that can be made on cloud computing systems include man-in-the middle attacks, phishing attacks, authentication attacks, and malware attacks. One of the largest threats is considered to be malware attacks, such as Trojan horses.\nRecent research conducted in 2022 has revealed that the Trojan horse injection method is a serious problem with harmful impacts on cloud computing systems. A Trojan attack on cloud systems tries to insert an application or service into the system that can impact the cloud services by changing or stopping the functionalities. When the cloud system identifies the attacks as legitimate, the service or application is performed which can damage and infect the cloud system.\n\n\n== Encryption ==\nSome advanced encryption algorithms which have been applied to cloud computing increase the protection of privacy. In a practice called crypto-shredding, the keys can simply be deleted when there is no more use of the data.\n\n\n=== Attribute-based encryption (ABE) ===\nAttribute-based encryption is a type of public-key encryption in which the secret key of a user and the ciphertext are dependent upon attributes (e.g. the country in which he lives, or the kind of subscription he has). In such a system, the decryption of a ciphertext is possible only if the set of attributes of the user key matches the attributes of the ciphertext.\nSome of the strengths of Attribute-based encryption are that it attempts to solve issues that exist in current public-key infrastructure(PKI) and identity-based encryption(IBE) implementations. By relying on attributes ABE circumvents needing to share keys directly, as with PKI, as well as having to know the identity of the receiver, as with IBE.\nThese benefits come at a cost as ABE suffers from the decryption key re-distribution problem. Since decryption keys in ABE only contain information regarding access structure or the attributes of the user it is hard to verify the user's actual identity. Thus malicious users can intentionally leak their attribute information so that unauthorized users can imitate and gain access.\n\n\n==== Ciphertext-policy ABE (CP-ABE) ====\nCiphertext-policy ABE (CP-ABE) is a type of public-key encryption. In the CP-ABE, the encryptor controls the access strategy. The main research work of CP-ABE is focused on the design of the access structure. A Ciphertext-policy attribute-based encryption scheme consists of four algorithms: Setup, Encrypt, KeyGen, and Decrypt. The Setup algorithm takes security parameters and an attribute universe description as input and outputs public parameters and a master key. The encryption algorithm takes data as input. It then encrypts it to produce ciphertext that only a user that possesses a set of attributes that satisfies the access structure will decrypt the message. The KeyGen algorithm then takes the master key and the user's attributes to develop a private key. Finally, the Decrypt algorithm takes the public parameters, the ciphertext, the private key, and user attributes as input. With this information, the algorithm first checks if the users\u2019 attributes satisfy the access structure and then decrypts the ciphertext to return the data.\n\n\n==== Key-policy ABE (KP-ABE) ====\nKey-policy Attribute-Based Encryption, or KP-ABE, is an important type of Attribute-Based Encryption. KP-ABE allows senders to encrypt their messages under a set of attributes, much like any Attribute Based Encryption system. For each encryption, private user keys are then generated which contain decryption algorithms for deciphering the message and these private user keys grant users access to specific messages that they correspond to. In a KP-ABE system, ciphertexts, or the encrypted messages, are tagged by the creators with a set of attributes, while the user's private keys are issued that specify which type of ciphertexts the key can decrypt. The private keys control which ciphertexts a user is able to decrypt. In KP-ABE, the attribute sets are used to describe the encrypted texts and the private keys are associated to the specified policy that users will have for the decryption of the ciphertexts. A drawback to KP-ABE is that in KP-ABE the encryptor does not control who has access to the encrypted data, except through descriptive attributes, which creates a reliance on the key-issuer granting and denying access to users. Hence, the creation of other ABE systems such as Ciphertext-Policy Attribute-Based Encryption.\n\n\n=== Fully homomorphic encryption (FHE) ===\nFully Homomorphic Encryption is a cryptosystem that supports arbitrary computation on ciphertext and also allows computing sum and product for the encrypted data without decryption. Another interesting feature of Fully Homomorphic Encryption or FHE for short is that it allows operations to be executed without the need for a secret key. FHE has been linked not only to cloud computing but to electronic voting as well. Fully Homomorphic Encryption has been especially helpful with the development of cloud computing and computing technologies. However, as these systems are developing the need for cloud security has also increased. FHE aims to secure data transmission as well as cloud computing storage with its encryption algorithms. Its goal is to be a much more secure and efficient method of encryption on a larger scale to handle the massive capabilities of the cloud.\n\n\n=== Searchable encryption (SE) ===\nSearchable encryption is a cryptographic system that offers secure search functions over encrypted data. SE schemes can be classified into two categories: SE based on secret-key (or symmetric-key) cryptography, and SE based on public-key cryptography. In order to improve search efficiency, symmetric-key SE generally builds keyword indexes to answer user queries. This has the obvious disadvantage of providing multimodal access routes for unauthorized data retrieval, bypassing the encryption algorithm by subjecting the framework to alternative parameters within the shared cloud environment.\n\n\n== Compliance ==\nNumerous laws and regulations pertaining to the storage and use of data.  In the US these include privacy or data protection laws, Payment Card Industry Data Security Standard (PCI DSS), the Health Insurance Portability and Accountability Act (HIPAA), the Sarbanes-Oxley Act, the Federal Information Security Management Act of 2002 (FISMA), and Children's Online Privacy Protection Act of 1998, among others.  Similar standards exist in other jurisdictions, e.g. Singapore's Multi-Tier Cloud Security Standard.\nSimilar laws may apply in different legal jurisdictions and may differ quite markedly from those enforced in the US.  Cloud service users may often need to be aware of the legal and regulatory differences between the jurisdictions.  For example, data stored by a cloud service provider may be located in, say, Singapore and mirrored in the US.\nMany of these regulations mandate particular controls (such as strong access controls and audit trails) and require regular reporting. Cloud customers must ensure that their cloud providers adequately fulfill such requirements as appropriate, enabling them to comply with their obligations since, to a large extent, they remain accountable.\n\nBusiness continuity and data recovery\nCloud providers have business continuity and data recovery plans in place to ensure that service can be maintained in case of a disaster or an emergency and that any data loss will be recovered. These plans may be shared with and reviewed by their customers, ideally dovetailing with the customers' own continuity arrangements. Joint continuity exercises may be appropriate, simulating a major Internet or electricity supply failure for instance.\nLog and audit trail\nIn addition to producing logs and audit trails, cloud providers work with their customers to ensure that these logs and audit trails are properly secured, maintained for as long as the customer requires, and are accessible for the purposes of forensic investigation (e.g., eDiscovery).\nUnique compliance requirements\nIn addition to the requirements to which customers are subject, the data centers used by cloud providers may also be subject to compliance requirements. Using a cloud service provider (CSP) can lead to additional security concerns around data jurisdiction since customer or tenant data may not remain on the same system, in the same data center, or even within the same provider's cloud.\nThe European Union\u2019s GDPR has introduced new compliance requirements for customer data.\n\n\n== Legal and contractual issues ==\n\nAside from the security and compliance issues enumerated above, cloud providers and their customers will negotiate terms around liability (stipulating how incidents involving data loss or compromise will be resolved, for example), intellectual property, and end-of-service (when data and applications are ultimately returned to the customer). In addition, there are considerations for acquiring data from the cloud that may be involved in litigation. These issues are discussed in service-level agreements (SLA).\n\n\n=== Public records ===\nLegal issues may also include records-keeping requirements in the public sector, where many agencies are required by law to retain and make available electronic records in a specific fashion. This may be determined by legislation, or law may require agencies to conform to the rules and practices set by a records-keeping agency. Public agencies using cloud computing and storage must take these concerns into account.\n\n\n== See also ==\nComputer security\nCommon Vulnerabilities and Exposures\n\n\n== References ==\n\n\n== Further reading ==\nMowbray, Miranda (15 April 2009). \"The Fog over the Grimpen Mire: Cloud Computing and the Law\". SCRIPT-ed. 6 (1): 132\u2013146. doi:10.2966/scrip.060109.132.\nMather, Tim; Kumaraswamy, Subra; Latif, Shahed (2009). Cloud Security and Privacy: An Enterprise Perspective on Risks and Compliance. O'Reilly Media, Inc. ISBN 9780596802769.\nWinkler, Vic (2011). Securing the Cloud: Cloud Computer Security Techniques and Tactics. Elsevier. ISBN 9781597495929.\nOttenheimer, Davi (2012). Securing the Virtual Environment: How to Defend the Enterprise Against Attack. Wiley. ISBN 9781118155486.\nBS ISO/IEC 27017: \"Information technology. Security techniques. Code of practice for information security controls based on ISO/IEC 27002 for cloud services.\" (2015)\nBS ISO/IEC 27018: \"Information technology. Security techniques. Code of practice for protection of personally identifiable information (PII) in public clouds acting as PII processors.\" (2014)\nBS ISO/IEC 27036-4: \"Information technology. Security techniques. Information security for supplier relationships. Guidelines for security of cloud services\" (2016)\n\n\n== External links ==\nCloud Security Alliance\nCheck Point Cloud Security\nCloud Security Solutions\nWhy cloud security requires multiple layers\nThe Beginner's Guide to Cloud Security\nDoD Cloud Computing Security Requirements Guide (CC SRG)\n\n\n=== Archive ===\nArchived 2018-10-21 at the Wayback Machine", "link": "https://en.wikipedia.org/wiki/Cloud_computing_security"}, "History of cloud computing": {"title": "History of cloud computing", "content": "The concept of the cloud computing as a platform for distributed computing traces its roots back to 1993. At that time, Apple spin-off General Magic and AT&T utilized the term in the context of their Telescript and Personal Link technologies.\nIn an April 1994 feature by Wired, titled \"Bill and Andy's Excellent Adventure II\", Andy Hertzfeld elaborated on Telescript, General Magic's distributed programming language. He described the expansive potential of the cloud:\n\nThe beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service. No one had conceived that before. The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties.\n\n\n== Early history ==\nIn 1963, the Defense Advanced Research Projects Agency (DARPA) funded Project MAC, the first computer time-sharing system. During the 1960s, the initial concepts of time-sharing became popularized via Remote Job Entry (RJE); this terminology was mostly associated with large vendors such as IBM and DEC. Full-time-sharing solutions were available by the early 1970s on such platforms as Multics (on GE hardware), Cambridge CTSS, and the earliest UNIX ports (on DEC hardware). Yet, the \"data center\" model where users submitted jobs to operators to run on IBM mainframes was overwhelmingly predominant.\nIn the late 1980s, the invention of the world wide web led to internet expansion and on-premises data centers. In the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering virtual private network (VPN) services with comparable quality of service, but at a lower cost. By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively. They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for. Cloud computing extended this boundary to cover all servers as well as the network infrastructure. As computers became more diffused, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing. They experimented with algorithms to optimize the infrastructure, platform, and applications, to prioritize tasks to be executed by CPUs, and to increase efficiency for end users. At the same time, Application Service Providers became popular, and later evolved into Software as a Service (SaaS). In 1999, Medidata launched Rave, the first electronic data capture software for clinical data.  \nThe use of the cloud metaphor for virtualized services dates at least to General Magic in 1994, where it was used to describe the universe of \"places\" that mobile agents in the Telescript environment could go. As described by Andy Hertzfeld:\n\n\"The beauty of Telescript,\" says Andy, \"is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service.\"\nThe use of the cloud metaphor is credited to General Magic communications employee David Hoffman, based on long-standing use in networking and telecom. In addition to use by General Magic itself, it was also used in promoting AT&T's associated Personal Link Services.\n\n\n== 2000s ==\nIn 2002, Amazon established its subsidiary Amazon Web Services, which allows developers to build applications independently.\nIn 2006, Amazon introduced Simple Storage Service (S3) in March and Elastic Compute Cloud (EC2) in August. These services were among the first to use server virtualization to provide IaaS on a pay-as-you-go basis. In the same year, Google launched Google Docs, a SaaS model to edit and save documents online.  \nIn 2007, Netflix launches its online video streaming service, the first SaaS streaming site.  Also, IBM and Google partnered with universities-- University of Washington, Carnegie Mellon University, MIT, Stanford, University of Maryland, and UC Berkeley-- to create a research server farm.  This would later become the Cluster Exploratory program when the National Science Foundation funded the project in early 2008. \nIn April of 2008, Google released the beta version of Google App Engine, a PaaS that provides a fully managed infrastructure and platform for users to create web applications. In mid-2018, Gartner noted the potential for cloud computing to reshape the relationship between IT service consumers, users, and providers.\nNASA's Nebula becomes the first open-source software for deploying private and hybrid clouds in early 2009.  Later in the same year, The French government announced the Androm\u00e8de Project to establish a national cloud computing service. The government committed \u20ac285 million to the initiative. The initiative ultimately failed, leading to the shutdown of Cloudwatt on 1 February 2020. \n\n\n== 2010s ==\nIn February 2010, Microsoft launched Microsoft Azure in February, following its announcement in October 2008. Five months later, Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. This project aimed to facilitate organizations in offering cloud-computing services on standard hardware. The early codebase was sourced from NASA's Nebula platform and Rackspace's Cloud Files platform.\nIn March of 2011, IBM introduced the IBM SmartCloud framework, designed to support the Smarter Planet initiative. Later that year, the US government established the Federal Risk Management Program, FedRAMP, becoming the first government-wide cloud services accreditation program with standardized risk assessment methodologies for cloud products and services. Later on October 12, iCloud was launched, allowing users to store personal information across multiple devices and share with other users.\nIn June 2012, On June 7, Oracle announced the Oracle Cloud. In May, Google Compute Engine was released in preview and subsequently rolled out into General Availability in December 2013. Also in 2013, Docker launched as a PaaS model to host containers in the cloud for software development.\nIn December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer datacenters, co-location spaces, or on-premises facilities.\n\n\n== 2020s ==\nSince the global pandemic of 2020, cloud technology jumped ahead in popularity due to the level of security of data and the flexibility of working options for all employees, notably remote workers. For example, Zoom grew over 160% in 2020 alone. Security and privacy are still a major concern due to security breaches and one of the main focuses of research. CloudChain, a cloud-oriented blockchain system is designed to increase the layers of security.\nCurrently, global spending on cloud computing services has reached $706 billion and the International Data Corporation predicts it to reach $1.3 trillion by 2025.\n\n\n== References ==\n\n\n== Further reading ==\nYeo, ShinJoung. (2023) Behind the Search Box: Google and the Global Internet Industry (U of Illinois Press, 2023) ISBN 10:0252087127 online", "link": "https://en.wikipedia.org/wiki/History_of_cloud_computing"}, "Cloud computing research": {"title": "Cloud computing research", "content": "Many universities, vendors, institutes and government organizations are investing in cloud computing research:\n\nIn October 2007, the Academic Cloud Computing Initiative (ACCI) was announced as a multi-university project designed to enhance students' technical knowledge to address the challenges of cloud computing.\nIn April 2009, UC Santa Barbara released the first open source platform-as-a-service, AppScale, which is capable of running Google App Engine applications at scale on a multitude of infrastructures.\nIn April 2009, the St Andrews Cloud Computing Co-laboratory was launched, focusing on research in the important new area of cloud computing. Unique in the UK, StACC aims to become an international centre of excellence for research and teaching in cloud computing and provides advice and information to businesses interested in cloud-based services.\nIn October 2010, the TClouds (Trustworthy Clouds) project was started, funded by the European Commission's 7th Framework Programme. The project's goal is to research and inspect the legal foundation and architectural design to build a resilient and trustworthy cloud-of-cloud infrastructure on top of that. The project also develops a prototype to demonstrate its results.\nIn January 2011, the IRMOS EU-funded project developed a real-time cloud platform, enabling interactive applications to be executed in cloud infrastructures.\nIn February 2011, Enterprise Ireland and the Irish Industrial Development Authority launched the Irish Centre for Cloud Computing and Commerce to deliver industry-led research on cloud architectures, quality of service, security and business and legal issues.\nIn July 2011, the High Performance Computing Cloud (HPCCLoud) project was kicked off, aiming at finding out the possibilities of enhancing performance on cloud environments while running the scientific applications \u2013 development of HPCCLoud Performance Analysis Toolkit which was funded by CIM-Returning Experts Programme \u2013 under the coordination of Prof. Dr. Shajulin Benedict.\nIn June 2011, the Telecommunications Industry Association developed a Cloud Computing White Paper, to analyze the integration challenges and opportunities between cloud services and traditional U.S. telecommunications standards.\nIn December 2011, the VISION Cloud EU-funded project proposed an architecture along with an implementation of a cloud environment for data-intensive services aiming to provide a virtualized Cloud Storage infrastructure.\nIn October 2012, the Centre For Development of Advanced Computing  released an  open source, complete cloud service, software suite called \"Meghdoot\".\nIn October 2012, the ECO2Clouds EU-funded project was launched to analyze the environmental impact of applications on the cloud and to optimize their deployment and scheduling based on a monitoring infrastructure based on BonFIRE proving ecometrics \nIn February 2013, the BonFIRE project launched a multi-site cloud experimentation and testing facility. The facility provides transparent access to cloud resources, with the control and observability necessary to engineer future cloud technologies, in a way that is not restricted, for example, by current business models.\nIn October 2013, the CACTOS project (short for Content-Aware Cloud Simulation and Optimisation) was launched to address the specific problems data centre operators face due to the exploding heterogeneity of the underlying hardware.\nIn February 2015, CloudLightning, a European Commission-funded Horizon 2020 project, was launched to address energy efficiency and high performance by developing a self-organising, self-optimising heterogeneous cloud computing service delivery model. Its initial application domains: genome processing, oil and gas exploration, and ray tracing.\nIn January 2017, RECAP, an EU-funded Horizon 2020 project, was launched to advance cloud and edge computing technology. It develops mechanisms for reliable capacity provisioning to make application placement, infrastructure management, and capacity provisioning autonomous, predictable and optimized.\n\n\n== European research ==\nIn 2012 the European Commission has issued an analysis of the relevance of the open research issues for commercial stabilisation  in which various experts from industry and academia identify in particular the following major concerns:\n\nopen interoperation across (proprietary) cloud solutions at IaaS, PaaS and SaaS levels\nmanaging multitenancy at large scale and in heterogeneous environments\ndynamic and seamless elasticity from inhouse clouds to public clouds for unusual (scale, complexity) and/or infrequent requirements\ndata management in a cloud environment, taking the technical and legal constraints into consideration\nThese findings have been refined into a research roadmap proposed by the Cloud Computing Expert Group on Research in December 2012  which tries to lay out a timeline for the identified research topics according to their commercial relevance. With the 8th Framework Programmes for Research and Technological Development, the European Commission is trying to support the according research work along the lines of the Europe 2020 strategy.\n\n\n== See also ==\nCloud computing\nCloud storage\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Cloud_computing_research"}, "Cloud Computing (horse)": {"title": "Cloud Computing (horse)", "content": "Cloud Computing (foaled April 29, 2014) is an American Thoroughbred racehorse who won the 2017 Preakness Stakes in only his fourth start.\n\n\n== Background ==\n\nCloud Computing was bred in Kentucky by Hill 'n' Dale Equine Holdings and Stretch Run Ventures. He is from the first foal crop sired by Maclean's Music, a son of Distorted Humor. Maclean's Music was an impressive winner by 7+1\u20442 lengths in his only start, a 2011 race for three-year-old maiden horses. and based upon the performances of his first crop of foals, exceeded expectations as a sire. Cloud Computing's dam is Quick Temper, a multiple stakes-placed daughter of A.P. Indy.\nCloud Computing was purchased as a yearling at the 2015 Keeneland Sales for $200,000 by Seth Klarman, the owner of Klaravich Stables, and William Lawrence. Introduced by mutual friends in 2004, the two typically buy about 50\u201360 horses a year. Both hedge fund managers, they chose his name based upon their pattern of using terms from the finance industry to name their horses, other examples being graded stakes winners Takeover Target and Currency Swap.\nHe is trained by Chad C. Brown.\n\n\n== Racing career ==\n\nCloud Computing did not make his racing debut until February 11, 2017, when, as a three-year-old, he won a maiden special weight at Aqueduct Racetrack. He then finished second to J Boys Echo in the Gotham Stakes and third behind winner Irish War Cry in the Wood Memorial. He earned enough points from these races to qualify on the 2017 Road to the Kentucky Derby.  However, his connections elected to bypass the race, instead starting their Champagne Stakes winner, Practical Joke, who finished fifth.\nCloud Computing was one of the more highly regarded \"new shooters\" for the Preakness Stakes, a race which is typically won by horses who had earlier raced in the Kentucky Derby. The two favorites in the Preakness, Always Dreaming and Classic Empire, had finished first and fourth respectively in the Derby. These two went to an early lead and set a solid pace while Cloud Computing rated a few lengths behind in third. Around the final turn, Classic Empire surged to the front and Always Dreaming dropped back. In mid-stretch, Classic Empire had a three-length lead and looked the likely winner before Cloud Computing angled out from traffic and started closing ground rapidly. Classic Empire tried to rally but could not hold off Cloud Computing, who won by a head.\n\nCloud Computing became just the fourth horse in the last 34 years to win the Preakness after not having raced in the Derby. The last horse to do so was the filly Rachel Alexandra in 2009. It was the first win of a Triple Crown race for his trainer Chad Brown and the second for jockey Javier Castellano, who was riding the colt for the first time.\n\"I'm not going to dispute the fact that I brought in a fresh horse as part of our strategy\", said Brown. \"Classic Empire and Always Dreaming are two outstanding horses and our strategy was, if we were going to ever beat them, let's take them on two weeks' rest when we have six, and it worked.\"\n\"It's incredibly special\", said Klarman. \"He's a great horse. I have the best trainer and the best jockey going for me. I never imagined it, but I'm thrilled.\" Klarman had grown up in Baltimore just a few blocks away from Pimlico and attended many runnings of the Preakness including Secretariat's win in 1973.\nCloud Computing was then given some time off before finishing fifth in the Jim Dandy Stakes at Saratoga on July 29. He followed up with a ninth-place finish in the Travers Stakes on August 26.\n\n\n== Breeding career ==\nCloud Computing retired to stud in 2019 at the Pin Oak Lane Farm in New Freedom, Pennsylvania for a fee of $3,500. His best racer is I'm Very Busy, a 2020 colt out of Two Kisses that won the Grade II Muniz Memorial Classic Stakes in 2024.\n\n\n== Statistics ==\n\n\n== Pedigree ==\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Cloud_Computing_(horse)"}, "Mobile cloud computing": {"title": "Mobile cloud computing", "content": "Mobile Cloud Computing (MCC) is the combination of cloud computing and mobile computing to bring rich computational resources to mobile users, network operators, as well as cloud computing providers. The ultimate goal of MCC is to enable execution of rich mobile applications on a plethora of mobile devices, with a rich user experience. MCC provides business opportunities for mobile network operators as well as cloud providers. More comprehensively, MCC can be defined as \"a rich mobile computing technology that leverages unified elastic resources of varied clouds and network technologies toward unrestricted functionality, storage, and mobility to serve a multitude of mobile devices anywhere, anytime through the channel of Ethernet or Internet regardless of heterogeneous environments and platforms based on the pay-as-you-use principle.\"\n\n\n== Architecture ==\n\nMCC uses computational augmentation approaches (computations are executed remotely instead of on the device) by which resource-constraint mobile devices can utilize computational resources of varied cloud-based resources. In MCC, there are four types of cloud-based resources, namely distant immobile clouds, proximate immobile computing entities, proximate mobile computing entities, and hybrid (combination of the other three model). Giant clouds such as Amazon EC2 are in the distant immobile groups whereas cloudlet or surrogates are member of proximate immobile computing entities. Smartphones, tablets, handheld devices, and wearable computing devices are part of the third group of cloud-based resources which is proximate mobile computing entities.\nVodafone, Orange and Verizon have started to offer cloud computing services for companies.\n\n\n== Challenges ==\nIn the MCC landscape, an amalgam of mobile computing, cloud computing, and communication networks (to augment smartphones) creates several complex challenges such as Mobile Computation Offloading, Seamless Connectivity, Long WAN Latency, Mobility Management, Context-Processing, Energy Constraint, Vendor/data Lock-in, Security and Privacy, Elasticity that hinder MCC success and adoption.\n\n\n=== Open research issues ===\nAlthough significant research and development in MCC is available in the literature, efforts in the following domains is still lacking:\n\nArchitectural issues: A reference architecture for heterogeneous MCC environment is a crucial requirement for unleashing the power of mobile computing towards unrestricted ubiquitous computing.\nEnergy-efficient transmission: MCC requires frequent transmissions between cloud platform and mobile devices, due to the stochastic nature of wireless networks, the transmission protocol should be carefully designed.\nContext-awareness issues: Context-aware and socially-aware computing are inseparable traits of contemporary handheld computers. To achieve the vision of mobile computing among heterogeneous converged networks and computing devices, designing resource-efficient environment-aware applications is an essential need.\nLive VM migration issues: Executing resource-intensive mobile application via Virtual Machine (VM) migration-based application offloading involves encapsulation of application in VM instance and migrating it to the cloud, which is a challenging task due to additional overhead of deploying and managing VM on mobile devices.\nMobile communication congestion issues: Mobile data traffic is tremendously hiking by ever increasing mobile user demands for exploiting cloud resources which impact on mobile network operators and demand future efforts to enable smooth communication between mobile and cloud endpoints.\nTrust, security, and privacy issues: Trust is an essential factor for the success of the burgeoning MCC paradigm. It is because the data along with code/component/application/complete VM is offloaded to the cloud for execution. Moreover, just like software and mobile application piracy, the MCC application development models are also affected by the piracy issue. Pirax is known to be the first specialized framework for controlling application piracy in MCC requirements\n\n\n== MCC research groups and activities ==\nSeveral academic and industrial research groups in MCC have been emerging since last few years. Some of the MCC research groups in academia with large number of researchers and publications include:\n\nMDC, Mobile and Distributed Computing research group is at Faculty of Computer and Information Science, King Saud University. MDC research group focuses on architectures, platforms, and protocols for mobile and distributed computing. The group has developed algorithms, tools, and technologies which offer energy efficient, fault tolerant, scalable, secure, and high performance computing on mobile devices.\nMobCC lab, Faculty of Computer Science and Information Technology, University Malaya. The lab was established in 2010 under the High Impact Research Grant, Ministry of Higher Education, Malaysia. It has 17 researchers and has track of 22 published articles in international conference and peer reviewed CS journals.\nICCLAB, Z\u00fcrich University of Applied Sciences has a segment working on MCC. The InIT Cloud Computing Lab is a research lab within the Institute of Applied Information Technology (InIT) of Z\u00fcrich University of Applied Sciences (ZHAW). It covers topic areas across the entire cloud computing technology stack.\nMobile & Cloud Lab, Institute of Computer Science, University of Tartu. Mobile & Cloud Lab conducts research and teaching in the mobile computing and cloud computing domains. The research topics of the group include cloud computing, mobile application development, mobile cloud, mobile web services and migrating scientific computing and enterprise applications to the cloud.\nSmartLab, Data Management Systems Laboratory, Department of Computer Science, University of Cyprus. SmartLab is a first-of-a-kind open cloud of smartphones that enables a new line of systems-oriented mobile computing research.\nMobile Cloud Networking: Mobile Cloud Networking (MCN) was an EU FP7 Large-scale Integrating Project (IP, 15m Euro) funded by the European Commission. The MCN project was launched in November 2012 for the period of 36 month. The project was coordinated by SAP Research and the ICCLab at the Zurich University of Applied Science. In total 19 partners from industry and academia established the first vision of Mobile Cloud Computing. The project was primarily motivated by an ongoing transformation that drives the convergence between the Mobile Communications and Cloud Computing industry enabled by the Internet and is considered the first pioneer in the area of Network Function Virtualization.\n\n\n== See also ==\nCloudlet\nCloud computing\nCloud collaboration\nMobile collaboration\nCrowd computing\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Mobile_cloud_computing"}, "IBM Cloud": {"title": "IBM Cloud", "content": "IBM Cloud (formerly known as Bluemix) is a set of cloud computing services for business offered by the information technology company IBM.\n\n\n== Services ==\nAs of 2021, IBM Cloud contains more than 170 services including compute, storage, networking, database, analytics, machine learning, and developer tools.\n\n\n== History ==\n\n\n=== SoftLayer ===\n\nSoftLayer Technologies, Inc. (now IBM Cloud) was a dedicated server, managed hosting, and cloud computing provider, founded in 2005 and acquired by IBM in 2013. SoftLayer initially specialized in hosting workloads for gaming companies and startups, but shifted focus to enterprise workloads after its acquisition.\nSoftLayer had bare-metal compute offerings before other large cloud providers such as Amazon Web Services.\nSoftLayer has hosted workloads for companies such as The Hartford, WhatsApp, Whirlpool, Daimler, and Macy's.\n\n\n==== Timeline ====\nYear 2005: SoftLayer was established in 2005 by Lance Crosby and several of his ex-coworkers.\nYear 2010 - August: GI Partners acquired a majority equity stake in SoftLayer in August 2010.\nYear 2010 - November: In November of that year it merged the company with The Planet Internet Services, SoftLayer's biggest competitor, and consolidated the customer base under the SoftLayer brand.\nYear 2011 - Q1: In Q1 2011, the company reported hosting more than 81,000 servers for more than 26,000 customers in locations throughout the United States.\nYear 2011 - July: In July 2011, the company announced plans for international expansion to Amsterdam and Singapore to add to the existing network of North American-based data centers in Dallas (Texas), San Jose (California), Seattle (Washington), Houston (Texas) and Washington, D.C. Most of these data centers were leased via Digital Realty.\nYear 2013 June 4: On June 4, 2013, IBM announced its acquisition of SoftLayer under undisclosed financial terms, in a deal that according to Reuters could have fetched more than $2 billion, to form an IBM Cloud Services Division. At the time of acquisition, SoftLayer was described as the biggest privately held cloud infrastructure provider (IaaS) in the world.\nYear 2015 - May: As of May 2015, the company has 23 data centers in 11 different countries.\nYear 2018: By 2018, SoftLayer was renamed to IBM Cloud.\n\n\n=== Initial launch of Bluemix (2013\u20132016) ===\nIn June 2013, IBM acquired SoftLayer, a public cloud platform, to serve as the foundation for its IaaS offering. Bluemix was announced for public beta in February 2014 after having been developed since early 2013. Bluemix was based on the open source Cloud Foundry project and ran on SoftLayer infrastructure. IBM announced the general availability of the Bluemix Platform-as-a-Service (PaaS) offering in July 2014.\nBy April 2015, Bluemix included a suite of over 100 cloud-based development tools \"including social, mobile, security, analytics, database, and IoT (internet of things). Bluemix had grown to 83,000 users in India with growth of approximately 10,000 users each month.\nA year after announcement, Bluemix had made little headway in the cloud-computing platform space relative to its competition, and remained substantially behind market leaders Microsoft Azure and Amazon AWS. By August 2016, little had changed in market acceptance of the Bluemix offering. In February 2016, IBM Bluemix includes IBM's Function as a Service (FaaS) system, or Serverless computing offering, that is built using open source from the Apache OpenWhisk incubator project largely credited to IBM for seeding. This system, equivalent to Amazon Lambda, Microsoft Azure Functions, Oracle Cloud Fn or Google Cloud Functions, allows calling of a specific function in response to an event without requiring any resource management from the developer.\n\n\n=== Re-brand to IBM Cloud (since 2017) ===\nIn May 2017 IBM released Kubernetes support as the IBM Bluemix Container Service, later renamed to the IBM Cloud Kubernetes Service (IKS). IKS was built using the open source Kubernetes project. This system, equivalent to Amazon Web Services EKS, Microsoft Azure AKS, or Google Cloud GKE, aims to provide a platform for automating deployment, scaling, and operations of application containers across clusters of hosts. In October 2017, IBM announced that they would rebrand their cloud as IBM Cloud brand, merging all components, thus retiring the Bluemix and Softlayer brands. In March 2018, IBM launched an industry first managed Kubernetes service on bare metal. In August 2019, 3 weeks after the close of Red Hat acquisition, IBM launched a managed Red Hat OpenShift on IBM Cloud.\nIn November 2019, IBM has announced that it had designed the world's first financial services-ready public cloud and that Bank of America was its first committed collaborator and anchor customer, joined shortly thereafter in 2020 by BNP Paribas as its first European anchor client. IBM announced in April 2021 the general availability of IBM Cloud for Financial Services, including support for Red Hat OpenShift and other cloud-native technologies. In July 2021, it was announced that SAP is onboarding two of its finance and data management solutions to IBM Cloud for Financial Services. In September 2021, it was CaixaBank's turn to boost digital capabilities with IBM Cloud for Financial Services and onboarding to new IBM Cloud Multizone Region in Spain.\n\n\n=== Customer base ===\nIn 2019, IBM partnered with the United States Tennis Association (USTA) to provide new AI-powered tools for the US Open.\nIn May 2020, IBM announced agreements with six European companies, including Osram and Cr\u00e9dit Mutuel, that use IBM Cloud to access advanced technologies such as AI, blockchain and analytics.\n\n\n== Reviews ==\nIBM Cloud continued to be considered a leader in bare-metal in 2020, and distinguished itself by providing over 11 million possible custom configurations with the latest Power, Intel, and AMD CPUs and Nvidia GPUs.\n\n\n== Environmental impact ==\nIn 2021, IBM announced it would achieve net zero greenhouse gas emissions by 2030.\n\n\n== References ==\n\n\n== External links ==\nOfficial website", "link": "https://en.wikipedia.org/wiki/IBM_Cloud"}, "Amazon Elastic Compute Cloud": {"title": "Amazon Elastic Compute Cloud", "content": "Amazon Elastic Compute Cloud (EC2) is a part of Amazon's cloud-computing platform, Amazon Web Services (AWS), that allows users to rent virtual computers on which to run their own computer applications. EC2 encourages scalable deployment of applications by providing a web service through which a user can boot an Amazon Machine Image (AMI) to configure a virtual machine, which Amazon calls an \"instance\", containing any software desired. A user can create, launch, and terminate server-instances as needed, paying by the second for active servers \u2013 hence the term \"elastic\". EC2 provides users with control over the geographical location of instances that allows for latency optimization and high levels of redundancy. In November 2010, Amazon switched its own retail website platform to EC2 and AWS.\n\n\n== History ==\n\nAmazon announced a limited public beta test of EC2 on August 25, 2006, offering access on a first-come, first-served basis.\nAmazon added two new instance types (Large and Extra-Large) on October 16, 2007. On May 29, 2008, two more types were added, High-CPU Medium and High-CPU Extra Large. There were twelve types of instances available.\nAmazon added three new features on March 27, 2008, static IP addresses, availability zones, and user selectable kernels. On August 20, 2008, Amazon added Elastic Block Store (EBS)\nThis provides persistent storage, a feature that had been lacking since the service was introduced.\nAmazon EC2 went into full production when it dropped the beta label on October 23, 2008. On the same day, Amazon announced the following features:\n\na service level agreement for EC2,\nMicrosoft Windows in beta form on EC2,\nMicrosoft SQL Server in beta form on EC2,\nplans for an AWS management console, and\nplans for load balancing, autoscaling, and cloud monitoring services.\nThese features were subsequently added on May 18, 2009.\nAmazon EC2 was developed mostly by a team in Cape Town, South Africa led by Chris Pinkham. Pinkham provided the initial architecture guidance for EC2 and then built the team and led the development of the project along with Willem van Biljon.\n\n\n== Instance types ==\nInitially, EC2 used Xen virtualization exclusively. However, on November 6, 2017, Amazon announced the new C5 family of instances that were based on a custom architecture around the KVM hypervisor, called Nitro. Each virtual machine, called an \"instance\", functions as a virtual private server. Amazon sizes instances based on \"Elastic Compute Units\". The performance of otherwise identical virtual machines may vary. On November 28, 2017, AWS announced a bare-metal instance, a departure from exclusively offering virtualized instance types.\nAs of January 2019, the following instance types were offered:\n\nGeneral Purpose: A1, T3, T2, M5, M5a, M4, T3a\nCompute Optimized: C5, C5n, C4\nMemory Optimized: R5, R5a, R4, X1e, X1, High Memory, z1d\nAccelerated Computing: P3, P2, G3, F1\nStorage Optimized: H1, I3, D2\nAs of April 2018, the following payment methods by instance were offered:\n\nOn-demand: pay by the hour without commitment.\nReserved: rent instances with one-time payment receiving discounts on the hourly charge.\nSpot: bid-based service: runs the jobs only if the spot price is below the bid specified by bidder. The spot price is claimed to be supply-demand based, however a 2011 study concluded that the price was generally not set to clear the market, but was dominated by an undisclosed reserve price.\n\n\n=== Cost ===\n\nAs of April 2018, Amazon charged about $0.0058 per hour ($4.176 per month) for the smallest \"Nano Instance\" (t2.nano) virtual machine running Linux or Windows. Storage-optimized instances cost as much as $4.992 per hour (i3.16xlarge). \"Reserved\" instances can go as low as $2.50 per month for a three-year prepaid plan. The data transfer charge ranges from free to $0.12 per gigabyte, depending on the direction and monthly volume (inbound data transfer is free on all AWS services).\nEC2 costs can be analyzed using the Amazon Cost and Usage Report. There are many different cost categories for EC2 including: hourly Instance Charges, Data Transfer, EBS Volumes, EBS Volume Snapshots, and Nat Gateway.\n\n\n=== Free tier ===\nAs of December 2010 Amazon offered a bundle of free resource credits to new account holders. The credits are designed to run a \"micro\" sized server, storage (EBS), and bandwidth for one year. Unused credits cannot be carried over from one month to the next.\n\n\n=== Reserved instances ===\nReserved instances enable EC2 or RDS service users to reserve an instance for one or three years. The corresponding hourly rate charged by Amazon to operate the instance is 35 to 75% lower than the rate charged for on-demand instances.\nReserved instances can be purchased with three different payment options: All Upfront, Partial Upfront and No Upfront. The different purchase options allow for different structuring of payment models, with a larger discount given to customers that pay their reservation upfront.\nReserved Instances are purchased based on a resource commitment. These reservations are made based on an instance type and a count of that instance type.  For example, you could reserve 100 i3.large instances for a 3-year term.\nIn September 2016, AWS announced several enhancements to Reserved instances, introducing a new feature called scope and a new reservation type called a Convertible. In October 2017, AWS announced the allowance to subdivide the instances purchased for more flexibility.\n\n\n=== Spot instances ===\nCloud providers maintain large amounts of excess capacity they have to sell or risk incurring losses.\nAmazon EC2 Spot instances are spare compute capacity in the AWS cloud available at up to 90% discount compared to On-Demand prices. As a trade-off, AWS offers no SLA on these instances and customers take the risk that it can be interrupted with only two minutes of notification when Amazon needs the capacity back. Researchers from the Israeli Institute of Technology found that \"they (Spot instances) are typically generated at random from within a tight price interval via a dynamic hidden reserve price\". Some companies, like Spotinst, are using machine learning to predict spot interruptions up to 15 minutes in advance.\n\n\n=== Savings Plans ===\nIn November 2019, Amazon announced Savings Plans. Savings Plans are an alternative to Reserved Instances that come in two different plan types: Compute Savings Plans and EC2 Instances Savings Plans. Compute Savings Plans allow an organization to commit to EC2 and Fargate usage with the freedom to change region, family, size, availability zone, OS and tenancy inside the lifespan of the commitment. EC2 Instance Savings plans provide a larger discount than Compute Savings Plans but are less flexible meaning a user must commit to individual instance families within a region to take advantage, but with the freedom to change instances within the family in that region. \nAWS uses the Cost Explorer to automatically calculate recommendations for the commitments you should make how that commitment will look like as a monthly charge on your AWS bill. AWS Savings Plans are purchased based on hourly spend commitment. This hourly commitment is made using the discounted pricing of the savings plan you are purchasing. For example, you could commit to spending $5 per hour, on a Compute Savings Plan, for a 3-year term. \n\n\n== Features ==\n\n\n=== Operating systems ===\n\nWhen it launched in August 2006, the EC2 service offered Linux and later Sun Microsystems' OpenSolaris and Solaris Express Community Edition. In October 2008, EC2 added the Windows Server 2003 and Windows Server 2008 operating systems to the list of available operating systems.\nIn March 2011, NetBSD AMIs became available. In November 2012, Windows Server 2012 support was added.\nSince 2006, Colin Percival, a FreeBSD developer and Security Officer, solicited Amazon to add FreeBSD. In November 2012, Amazon officially supported running FreeBSD in EC2. The FreeBSD/EC2 platform is maintained by Percival who also developed the secure deduplicating Amazon S3-cloud based backup service Tarsnap.\nAmazon has their own Linux distribution based on Fedora and Red Hat Enterprise Linux as a low cost offering known as the Amazon Linux AMI. Version 2013.03 included: Linux kernel, Java OpenJDK Runtime Environment and GNU Compiler Collection.\nOn November 30, 2020, Amazon announced that it would be adding macOS to the EC2 service. Initial support was announced for macOS Mojave and macOS Catalina running on Mac Mini.\n\n\n=== Managed Container and Kubernetes Services ===\nAmazon Elastic Container Registry (ECR) is a Docker registry service for Amazon EC2 instances to access repositories and images.\nAmazon Elastic Kubernetes Service (EKS) a managed Kubernetes service running on top of EC2 without needing to provision or manage instances.\n\n\n=== Persistent storage ===\nAn EC2 instance may be launched with a choice of two types of storage for its boot disk or \"root device.\" The first option is a local \"instance-store\" disk as a root device (originally the only choice). The second option is to use an EBS volume as a root device. Instance-store volumes are temporary storage, which survive rebooting an EC2 instance, but when the instance is stopped or terminated (e.g., by an API call, or due to a failure), this store is lost.\nThe Amazon Elastic Block Store (EBS) provides raw block devices that can be attached to Amazon EC2 instances. These block devices can then be used like any raw block device. In a typical use case, this would include formatting the device with a filesystem and mounting it. In addition, EBS supports a number of advanced storage features, including snapshotting and cloning. EBS volumes can be up to 16 TB in size. EBS volumes are built on replicated storage, so that the failure of a single component will not cause data loss.\nEBS was introduced to the general public by Amazon in August 2008.\n\nEBS volumes provide persistent storage independent of the lifetime of the EC2 instance, and act much like hard drives on a real server. More accurately, they appear as block devices to the operating system that are backed by Amazon's disk arrays. The OS is free to use the device however it wants. In the most common case, a file system is loaded and the volume acts as a hard drive. Another possible use is the creation of RAID arrays by combining two or more EBS volumes. RAID allows increases of speed and/or reliability of EBS. Users can set up and manage storage volumes of sizes from 1 GB to 16 TB. The volumes support snapshots, which can be taken from a GUI tool or the API. EBS volumes can be attached or detached from instances while they are running, and moved from one instance to another.\nSimple Storage Service (S3) is a storage system in which data is accessible to EC2 instances, or directly over the network to suitably authenticated callers (all communication is over HTTP). Amazon does not charge for the bandwidth for communications between EC2 instances and S3 storage \"in the same region.\" Accessing S3 data stored in a different region (for example, data stored in Europe from a US East Coast EC2 instance) will be billed at Amazon's normal rates.\nS3-based storage is priced per gigabyte per month. Applications access S3 through an API. For example, Apache Hadoop supports a special s3: filesystem to support reading from and writing to S3 storage during a MapReduce job. There are also S3 filesystems for Linux, which mount a remote S3 filestore on an EC2 image, as if it were local storage. As S3 is not a full POSIX filesystem, things may not behave the same as on a local disk (e.g., no locking support).\n\n\n=== Elastic IP addresses ===\n\nAmazon's elastic IP address feature is similar to static IP address in traditional data centers, with one key difference. A user can programmatically map an elastic IP address to any virtual machine instance without a network administrator's help and without having to wait for DNS to propagate the binding. In this sense an Elastic IP Address belongs to the account and not to a virtual machine instance. It exists until it is explicitly removed, and remains associated with the account even while it is associated with no instance.\n\n\n=== Amazon CloudWatch ===\n\nAmazon CloudWatch is a web service that provides real-time monitoring to Amazon's EC2 customers on their resource utilization such as CPU, disk, network and replica lag for RDS Database replicas. CloudWatch does not provide any memory, disk space, or load average metrics without running additional software on the instance. Since December 2017 Amazon provides a CloudWatch Agent for Windows and Linux operating systems included disk and previously not available memory information, previously Amazon provided example scripts for Linux instances to collect OS information. The data is aggregated and provided through AWS management console. It can also be accessed through command line tools and Web APIs, if the customer desires to monitor their EC2 resources through their enterprise monitoring software. Amazon provides an API which allows clients to operate on CloudWatch alarms.\nThe metrics collected by Amazon CloudWatch enables the auto-scaling feature to dynamically add or remove EC2 instances. The customers are charged by the number of monitoring instances.\nSince May 2011, Amazon CloudWatch accepts custom metrics that can be submitted programmatically via Web Services API and then monitored the same way as all other internal metrics, including setting up the alarms for them, and since July 2014 Cloudwatch Logs service is also available.\nBasic Amazon CloudWatch is included in Amazon Free Tier service.\n\n\n=== Automated scaling ===\n\nAmazon's auto-scaling feature of EC2 allows it to automatically adapt computing capacity to site traffic. The schedule-based (e.g. time-of-the-day) and rule-based (e.g. CPU utilization thresholds) auto scaling mechanisms are easy to use and efficient for simple applications. However, one potential problem is that VMs may take up to several minutes to be ready to use, which are not suitable for time critical applications. The VM startup time is dependent on image size, VM type, data center locations, etc. The convenience of using EC2 enables you to dynamically increase capacity in accordance with demand and access resources rapidly.\n\n\n== Pricing ==\nNOTE: the examples, figures and comparison charts in this section are from 2018 in the best case; please bear this in mind, as the situation has changed a lot from then.\n\nOn Demand EC2 instances are priced per hour. An example of this pricing would be $0.096 per hour for a Linux, m5.large, EC2 instance in the us-east-1 region. Pricing will vary based on the instance type, region, and operating system of the instance. Public on-demand pricing for EC2 can be found on the AWS website.\nThe other pricing models for EC2 have different pricing models.\nSpot instances also have a cost per instance hour, but the cost will change on a regular basis based on the supply of EC2 spot capacity. \nReserved Instances and Compute Savings plans are priced per hour. Each of these reservation tools has its own price per hour based on the payment option, term and reservation product being used. These prices are locked in for either a 1-year or 3-year term.\nAmazon EC2 price varies from $2.5 per month for \"nano\" instance with 1 vCPU and 0.5 GB RAM on board to \"xlarge\" type of instances with 32 vCPU and 488 GB RAM billed up to $3997.19 per month.\nThe charts above show how Amazon EC2 pricing is compared to similar Cloud Computing services: Microsoft Azure, Google Cloud Platform, Kamatera, and Vultr.\n\n\n== Reliability ==\n\nTo make EC2 more fault-tolerant, Amazon engineered Availability Zones that are designed to be insulated from failures in other availability zones. Availability zones do not share the same infrastructure. Applications running in more than one availability zone can achieve higher availability.\nEC2 provides users with control over the geographical location of instances that allows for latency optimization and high levels of redundancy. For example, to minimize downtime, a user can set up server instances in multiple zones that are insulated from each other for most causes of failure such that one backs up the other.\nHigher-availability database services, like Amazon Relational Database Service run separately from EC2 instances.\n\n\n== Issues ==\nIn early July 2008, the anti-spam organizations Outblaze and Spamhaus.org began blocking Amazon's EC2 address pool due to problems with the distribution of spam and malware.\nOn December 1, 2010, Amazon pulled its service to WikiLeaks after coming under political pressure in the US. Assange said that WikiLeaks chose Amazon knowing it would probably be kicked off the service \"in order to separate rhetoric from reality\". The Internet group Anonymous attempted to attack EC2 in revenge; however, Amazon was not affected by the attack.\nAmazon's websites were temporarily offline on December 12, 2010, although it was initially unclear if this was due to attacks or a hardware failure. An Amazon official later stated that it was due to a hardware failure.\n\nShortly before 5 am ET on April 21, 2011, an outage started at EC2's Northern Virginia data center that brought down several websites, including Foursquare, Springpad, Reddit, Quora, and Hootsuite. Specifically, attempts to use Amazon's elastic-disk and database services hung, failed, or were slow. Service was restored to some parts of the data center (three of four \"availability zones\" in Amazon's terms) by late afternoon Eastern time that day; problems for at least some customers were continuing as of April 25. 0.07% of EBS volumes in one zone have also been lost; EBS failures were a part of normal operation even before this outage and were a risk documented by Amazon, though the number of failures and the number of simultaneous failures may find some EC2 users unprepared.\nOn Sunday August 6, 2011, Amazon suffered a power outage in one of their Ireland availability zones. Lightning was originally blamed for the outage; however, on August 11, Irish energy supplier ESB Networks dismissed this as a cause, but at time of writing, could not confirm what the cause of the problem was. The power outage raised multiple questions regarding Amazon's EBS infrastructure, which caused several bugs in their software to be exposed. The bugs resulted in some customers' data being deleted when recovering EBS volumes in a mid-write operation during the crash.\nAugust 8, 2011, saw another network connectivity outage of Amazon's Northern Virginia data center, knocking out the likes of Reddit, Quora, Netflix and FourSquare. The outage lasted around 25 minutes.\nAnother Northern Virginia data center outage occurred on October 22, 2012, from approximately 10 am to 4 pm PT. Edmodo, Airbnb, Flipboard, Reddit, and other customers were affected. Anonymous claimed responsibility, but Amazon denied this assertion.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nOfficial website", "link": "https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud"}, "Serverless computing": {"title": "Serverless computing", "content": "Serverless computing is a cloud computing execution model in which the cloud provider allocates machine resources on demand, taking care of the servers on behalf of their customers.  Serverless is a misnomer in the sense that servers are still used by cloud service providers to execute code for developers.  However, developers of serverless applications are not concerned with capacity planning, configuration, management, maintenance, fault tolerance, or scaling of containers, virtual machines, or physical servers. When an app is not in use, there are no computing resources allocated to the app.  Pricing is based on the actual amount of resources consumed by an application. It can be a form of utility computing.\nOne proposed definition for serverless computing that encompasses these ideas is that serverless computing is a \"cloud computing paradigm encompassing a class of cloud computing platforms that allow one to develop, deploy, and run applications (or components thereof) in the cloud without allocating and managing virtualized servers and resources or being concerned about other operational aspects. The responsibility for operational aspects, such as fault tolerance or the elastic scaling of computing, storage, and communication resources to match varying application demands, is offloaded to the cloud provider. Providers apply utilization-based billing: they charge cloud users with fine granularity, in proportion to the resources that applications actually consume from the cloud infrastructure, such as computing time, memory, and storage space.\" Note the definition of serverless has stretched over time. According to Ben Kehoe, serverless is a spectrum; one should not fixate on a strict definition of serverless nor any specific serverless technology. Instead, one should focus on serverless mindset: how to use serverless to solve one's business problems.\nServerless computing can simplify the process of deploying code into production. It does not entirely remove the complexity, but mainly shifts it from the operations team to development team. And the more fine grained the application, the harder it is to manage it. \nServerless code can be used in conjunction with code deployed in traditional styles, such as microservices or monoliths. Alternatively, applications can be written to be purely serverless and use no provisioned servers at all. This should not be confused with computing or networking models that do not require an actual server to function, such as peer-to-peer (P2P).\nAccording to Yan Cui, serverless should be adopted only when it helps to deliver customer value faster. And while adopting, organizations should take small steps and de-risk along the way.\n\n\n== Serverless runtimes ==\nServerless vendors offer compute runtimes that execute application logic but do not store data. Common runtime models are function as a service (FaaS) and container as a service. Common languages supported by serverless runtimes are Java, Python, and PHP. Generally, the functions run within isolation boundaries, such as Linux containers.\n\n\n== Commercial offerings ==\nThe first pay-as-you-go code execution platform was Zimki, released in 2006, but it was not commercially successful. In 2008, Google released Google App Engine, which featured metered billing for applications that used a custom Python framework, but could not execute arbitrary code.  PiCloud, released in 2010, offered FaaS support for Python.\nGoogle App Engine, introduced in 2008, was the first abstract serverless computing offering. App Engine included HTTP functions with a 60-second timeout and a blob store and data store with their own timeouts. No in-memory persistence was allowed. All operations had to be executed within these limits, but this allowed apps built in App Engine to scale near-infinitely and was used to support early customers including Snapchat, as well as many external and internal Google apps. Language support was limited to Python using native Python modules, as well as a limited selection of Python modules in C that were chosen by Google. Like later serverless platforms, App Engine also used pay-for-what-you-use billing.\nAWS Lambda, introduced by Amazon in 2014, popularized the abstract serverless computing model. It is supported by a number of additional AWS serverless tools such as AWS Serverless Application Model (AWS SAM) Amazon CloudWatch, and others.\nGoogle Cloud Platform created a second serverless offering, Google Cloud Functions, in 2016.\nOracle Cloud Functions is a serverless platform offered on Oracle Cloud Infrastructure, and is based on the open-source Fn Project so developers can create applications that can be ported to other cloud and on-premise environments. It supports code in Python, Go, Java, Ruby, and Node.\n\n\n== Serverless databases ==\nSeveral serverless databases have emerged to extend the serverless execution model to the RDBMS, eliminating the need to provision or scale virtualized or physical database hardware.\nNutanix offers a solution named Era which turns an existing RDBMS such as Oracle, MariaDB, PostgreSQL, or Microsoft SQL Server into a serverless service.\nAmazon Aurora offers a serverless version of its databases, based on MySQL and PostgreSQL, providing on-demand, auto-scaling configurations.\nAzure Data Lake is a highly scalable data storage and analytics service. The service is hosted in Azure, Microsoft's public cloud. Azure Data Lake Analytics provides a distributed infrastructure that can dynamically allocate or de-allocate resources so customers pay for only the services they use.\nOracle Cloud offers a serverless version of its Oracle Autonomous Database, which is the Autonomous Transaction Processing service. The serverless service also includes a JSON edition.\nFirebase, also owned by Google, includes a hierarchical database and is available via fixed and pay-as-you-go plans.\n\n\n== Advantages ==\n\n\n=== Cost ===\nServerless can be more cost-effective than renting or purchasing a fixed quantity of servers, which generally involves significant periods of underusage or idle time. It can even be more cost-efficient than provisioning an autoscaling group, due to more efficient bin-packing of the underlying machine resources.\nThis can be described as pay-as-you-go computing or bare-code, as one is charged based solely upon the time and memory allocated to run ones code, without associated fees for idle time. A useful analogy here is between rental car (traditional cloud Virtual Machines) versus ride share apps like Uber or Lyft (serverless computing). Immediate cost benefits are related to the lack of operating costs, including: licenses, installation, dependencies, and personnel cost for maintenance, support, or patching. Due to infinite scalability, developers may experience bill shock as a result of faulty code or a Denial-of-service attack. This is however often refunded, at the expense of the service provider.\n\n\n=== Elasticity versus scalability ===\n\nIn addition, a serverless architecture means that developers and operators do not need to spend time setting up and tuning autoscaling policies or systems; the cloud provider is responsible for scaling the capacity to the demand. As Google puts it: \"from prototype to production to planet-scale.\"\nAs cloud native systems inherently scale down as well as up, these systems are known as elastic rather than scalable.\nSmall teams of developers are able to run code themselves without the dependence upon teams of infrastructure and support engineers; more developers are becoming DevOps-skilled and distinctions between being a software developer or hardware engineer are blurring.\n\n\n=== Productivity ===\nWith function as a service, the units of code exposed to the outside world are simple event-driven functions. This means that typically, the programmer does not have to worry about multithreading or directly handling HTTP requests in their code, simplifying the task of back-end software development.\n\n\n== Disadvantages ==\nServerless applications are prone to fallacies of distributed computing. In addition, they are prone to following fallacies:\n\nVersioning is simple\nCompensating transactions always work\nObservability is optional\n\n\n=== Performance ===\nInfrequently-used serverless code may suffer from greater response latency than code that is continuously running on a dedicated server, virtual machine, or container. This is because, unlike with autoscaling, the cloud provider typically spins down the serverless code completely when not in use. This means that if the runtime (for example, the Java runtime) requires a significant amount of time to start up, it will create additional latency. This is referred to as cold start in serverless computing.\n\n\n=== Resource limits ===\nServerless computing is not suited to some computing workloads, such as high-performance computing, because of the resource limits imposed by cloud providers, and also because it would likely be cheaper to bulk-provision the number of servers believed to be required at any given point in time. This makes it challenging to deploy complex applications (such as those with a directed acyclic graph of functions); serverless computing out of the box is most suited for execution of individual stateless functions. Some commercial offerings like AWS Step Functions from Amazon and Azure Durable Functions from Microsoft are meant to ease this challenge.\n\n\n=== Monitoring and debugging ===\nDiagnosing performance or excessive resource usage problems with serverless code may be more difficult than with traditional server code, because although entire functions can be timed, there is typically no ability to dig into more detail by attaching profilers, debuggers, or APM tools. Furthermore, the environment in which the code runs is typically not open source, so its performance characteristics cannot be precisely replicated in a local environment.\n\n\n=== Security ===\nAccording to OWASP, serverless applications are vulnerable to variations of traditional  attacks, insecure code, and some serverless-specific attacks (like Denial of Wallet). So, the risks have changed and attack prevention requires a shift in mindset.\nServerless is sometimes mistakenly considered as more secure than traditional architectures. While this is true to some extent because OS vulnerabilities are taken care of by the cloud provider, the total attack surface is significantly larger as there are many more components to the application compared to traditional architectures, and each component is an entry point to the serverless application. Moreover, the security solutions that customers used to have to protect their cloud workloads become irrelevant as customers cannot control and install anything on the endpoint and network level such as an intrusion detection/prevention system (IDS/IPS).\nThis is intensified by the mono-culture properties of the entire server network. (A single flaw can be applied globally.) According to Protego, the \"solution to secure serverless apps is close partnership between developers, DevOps, and AppSec, also known as DevSecOps. Find the balance where developers don't own security, but they aren't absolved from responsibility either. Take steps to make it everyone's problem. Create cross-functional teams and work towards tight integration between security specialists and development teams. Collaborate so your organization can resolve security risks at the speed of serverless.\"\n\n\n=== Privacy ===\nMany serverless function environments are based on proprietary public cloud environments. Here, some privacy implications have to be considered, such as shared resources and access by external employees. However, serverless computing can also be done on private cloud environment or even on-premises, using for example the Kubernetes platform. This gives companies full control over privacy mechanisms, just as with hosting in traditional server setups.\n\n\n=== Standards ===\nServerless computing is covered by International Data Center Authority (IDCA) in their Framework AE360. However, the part related to portability can be an issue when moving business logic from one public cloud to another, for which the Docker solution was created. Cloud Native Computing Foundation (CNCF) is also working on developing a specification with Oracle.\n\n\n=== Vendor lock-in ===\nServerless computing is provided as a third-party service. Applications and software that run in the serverless environment are by default locked to a specific cloud vendor. This issue is exacerbated in serverless computing, as with its increased level of abstraction, public vendors only allow customers to upload code to a FaaS platform without the authority to configure underlying environments. More importantly, when considering a more complex workflow that includes Backend-as-a-Service (BaaS), a BaaS offering can typically only natively trigger a FaaS offering from the same provider. This makes the workload migration in serverless computing virtually impossible. Therefore, considering how to design and deploy serverless workflows from a multi-cloud perspective seems promising and is starting to prevail.\n\n\n== Best practices ==\nFollowing DevSecOps practices can help one to use and to secure serverless technologies more effectively. In serverless applications, the line between the infrastructure and business logic is blurred and the apps are usually spread across various services. According to Yan Cui, to get the most value from testing efforts, serverless applications should to be tested mainly for their integrations, and arguably, unit tests should be used only if there is a complex business logic. Also, to make debugging and implementation of serverless applications easier, developers should use orchestration within the bounded context of a microservice, and should use choreography between the bounded-contexts.\nAccording to Yan Cui, ephemeral resources should be kept together to achieve a high cohesion. However, shared resources that have a long spin-up time (e.g. AWS RDS cluster) and landing zone should have their own separate repository, deployment pipeline and stack. \n\n\n== Uses/functions ==\nServerless functions can be used for:\n\nData analytics\nStreaming video processing\nCI/CD operations\nFile conversions\nLog aggregation and restructuring\nSupport for dynamic website content\n\n\n== See also ==\nCloud computing\nFunction as a service\n\n\n== References ==\n\n\n== Further reading ==\nRoberts, Mike (25 July 2016). \"Serverless Architectures\". MartinFowler.com. Retrieved 30 July 2016.\nJamieson, Frazer (4 September 2017). \"Losing the server? Everybody is talking about serverless architecture\". BCS, the Chartered Institute for IT. Retrieved 7 November 2017.\nAnderson, David (9 March 2022). \"Power the Future and Accelerate Your Organization to the Modern Cloud and Serverless with 'The Value Flywheel Effect'\". The Serverless Edge. Retrieved 9 March 2022.\n14 authors from UC Berkeley (9 February 2019). \"Cloud Programming Simplified: A Berkeley View on Serverless Computing\".", "link": "https://en.wikipedia.org/wiki/Serverless_computing"}}, "Bioinformatics": {"Bioinformatics": {"title": "Bioinformatics", "content": "Bioinformatics ( ) is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. Bioinformatics uses biology, chemistry, physics, computer science, computer programming, information engineering, mathematics and statistics to analyze and interpret biological data. The subsequent process of analyzing and interpreting data is often referred to as computational biology, though the distinction between the two terms is often disputed.\nComputational, statistical, and computer programming techniques have been used  for computer simulation analyses of biological queries. They include reused specific analysis \"pipelines\", particularly in the field of genomics, such as by the identification of genes and single nucleotide polymorphisms (SNPs). These pipelines are used to better understand the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. Bioinformatics also includes proteomics, which tries to understand the organizational principles within nucleic acid and protein sequences.\nImage and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics, it aids in sequencing and annotating genomes and their observed mutations. Bioinformatics includes text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in comparing, analyzing and interpreting genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.\n\n\n== History ==\nThe first definition of the term bioinformatics was coined by Paulien Hogeweg and Ben Hesper in 1970, to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biochemistry (the study of chemical processes in biological systems).\nBioinformatics and computational biology involved the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.\nAnalyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.\n\n\n=== Sequences ===\n\nThere has been a tremendous advance in speed and cost reduction since the completion of the Human Genome Project, with some labs able to sequence over 100,000 billion bases each year, and a full genome can be sequenced for $1,000 or less.\nComputers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. Margaret Oakley Dayhoff, a pioneer in the field, compiled one of the first protein sequence databases, initially published as books as well as methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released online with Tai Te Wu between 1980 and 1991.\nIn the 1970s, new techniques for sequencing DNA were applied to bacteriophage MS2 and \u00f8X174, and the extended nucleotide sequences were then parsed with informational and statistical algorithms. These studies illustrated that well known features, such as the coding segments and the triplet code, are revealed in straightforward statistical analyses and were the proof of the concept that bioinformatics would be insightful.\n\n\n== Goals ==\nIn order to study how normal cellular activities are altered in different disease states, raw biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This also includes nucleotide and amino acid sequences, protein domains, and protein structures.\nImportant sub-disciplines within bioinformatics and computational biology include:\n\nDevelopment and implementation of computer programs to efficiently access, manage, and use various types of information.\nDevelopment of new mathematical algorithms and statistical measures to assess relationships among members of large data sets. For example, there are methods to locate a gene within a sequence, to predict protein structure and/or function, and to cluster protein sequences into families of related sequences.\nThe primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\u2013protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.\nBioinformatics entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.\nOver the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.\nCommon activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.\n\n\n== Sequence analysis ==\n\nSince the bacteriophage Phage \u03a6-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Computer programs such as BLAST are used routinely to search sequences\u2014as of 2008, from more than 260,000 organisms, containing over 190 billion nucleotides.\n\n\n=== DNA sequencing ===\n\nBefore sequences can be analyzed, they are obtained from a data storage bank, such as GenBank. DNA sequencing is still a non-trivial problem as the raw data may be noisy or affected by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.\n\n\n=== Sequence assembly ===\n\nMost DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The shotgun sequencing technique (used by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae) generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced (rather than chain-termination or chemical degradation methods), and genome assembly algorithms are a critical area of bioinformatics research.\n\n\n=== Genome annotation ===\n\nIn genomics, annotation refers to the process of marking the stop and start regions of genes and other biological features in a sequenced DNA sequence. Many genomes are too large to be annotated by hand. As the rate of sequencing exceeds the rate of genome annotation, genome annotation has become the new bottleneck in bioinformatics.\nGenome annotation can be classified into three levels: the nucleotide, protein, and process levels.\nGene finding is a chief aspect of nucleotide-level annotation. For complex genomes, a combination of ab initio gene prediction and sequence comparison with expressed sequence databases and other organisms can be successful. Nucleotide-level annotation also allows the integration of genome sequence with other genetic and physical maps of the genome.\nThe principal aim of protein-level annotation is to assign function to the protein products of the genome. Databases of protein sequences and functional domains and motifs are used for this type of annotation. About half of the predicted proteins in a new genome sequence tend to have no obvious function.\nUnderstanding the function of genes and their products in the context of cellular and organismal physiology is the goal of process-level annotation. An obstacle of process-level annotation has been the inconsistency of terms used by different model systems. The Gene Ontology Consortium is helping to solve this problem.\nThe first description of a comprehensive annotation system was published in 1995 by The Institute for Genomic Research, which performed the first complete sequencing and analysis of the genome of a free-living (non-symbiotic) organism, the bacterium Haemophilus influenzae. The system identifies the genes encoding all proteins, transfer RNAs, ribosomal RNAs, in order to make initial functional assignments. The GeneMark program trained to find protein-coding genes in Haemophilus influenzae is constantly changing and improving.\nFollowing the goals that the Human Genome Project left to achieve after its closure in 2003, the ENCODE project was developed by the National Human Genome Research Institute. This project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).\n\n\n==== Gene function prediction ====\nWhile genome annotation is primarily based on sequence similarity (and thus homology), other properties of sequences can be used to predict the function of genes. In fact, most gene function prediction methods focus on protein sequences as they are more informative and more feature-rich. For instance, the distribution of hydrophobic amino acids predicts transmembrane segments in proteins. However, protein function prediction can also use external information such as gene (or protein) expression data, protein structure, or protein-protein interactions.\n\n\n=== Computational evolutionary biology ===\n\nEvolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:\n\ntrace the evolution of a large number of organisms by measuring changes in their DNA, rather than through physical taxonomy or physiological observations alone,\ncompare entire genomes, which permits the study of more complex evolutionary events, such as gene duplication, horizontal gene transfer, and the prediction of factors important in bacterial speciation,\nbuild complex computational population genetics models to predict the outcome of the system over time\ntrack and share information on an increasingly large number of species and organisms\nFuture work endeavours to reconstruct the now more complex tree of life.\n\n\n=== Comparative genomics ===\n\nThe core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. Intergenomic maps are made to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion. Entire genomes are involved in processes of hybridization, polyploidization and endosymbiosis that lead to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.\nMany of these studies are based on the detection of sequence homology to assign sequences to protein families.\n\n\n=== Pan genomics ===\n\nPan genomics is a concept introduced in 2005 by Tettelin and Medini. Pan genome is the complete gene repertoire of a particular monophyletic taxonomic group. Although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum, etc. It is divided in two parts: the Core genome, a set of genes common to all the genomes under study (often housekeeping genes vital for survival), and the Dispensable/Flexible genome: a set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.\n\n\n=== Genetics of disease ===\n\nAs of 2013, the existence of efficient high-throughput next-generation sequencing technology allows for the identification of cause many different human disorders. Simple Mendelian inheritance has been observed for over 3,000 disorders that have been identified at the Online Mendelian Inheritance in Man database, but complex diseases are more difficult. Association studies have found many individual genetic regions that individually are weakly associated with complex diseases (such as infertility, breast cancer and Alzheimer's disease), rather than a single cause. There are currently many challenges to using genes for diagnosis and treatment, such as how we don't know which genes are important, or how stable the choices an algorithm provides. \nGenome-wide association studies have successfully identified thousands of common genetic variants for complex diseases and traits; however, these common variants only explain a small fraction of heritability. Rare variants may account for some of the missing heritability. Large-scale whole genome sequencing studies have rapidly sequenced millions of whole genomes, and such studies have identified hundreds of millions of rare variants. Functional annotations predict the effect or function of a genetic variant and help to prioritize rare functional variants, and incorporating these annotations can effectively boost the power of genetic association of rare variants analysis of whole genome sequencing studies. Some tools have been developed to provide all-in-one rare variant association analysis for whole-genome sequencing data, including integration of genotype data and their functional annotations, association analysis, result summary and visualization. Meta-analysis of whole genome sequencing studies provides an attractive solution to the problem of collecting large sample sizes for discovering rare variants associated with complex phenotypes.\n\n\n=== Analysis of mutations in cancer ===\n\nIn cancer, the genomes of affected cells are rearranged in complex or unpredictable ways. In addition to single-nucleotide polymorphism arrays identifying point mutations that cause cancer, oligonucleotide microarrays can be used to identify chromosomal gains and losses (called comparative genomic hybridization). These detection methods generate terabytes of data per experiment. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.\nTwo important principles can be used to identify cancer by mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second, cancer contains driver mutations which need to be distinguished from passengers.\nFurther improvements in bioinformatics could allow for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples. Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.\n\n\n== Gene and protein expression ==\n\n\n=== Analysis of gene expression ===\nThe expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as \"Whole Transcriptome Shotgun Sequencing\" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.\n\n\n=== Analysis of protein expression ===\nProtein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. The former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples when multiple incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.\n\n\n=== Analysis of regulation ===\nGene regulation is a complex process where a signal, such as an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.\nFor example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the protein-coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.\nExpression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). Clustering algorithms can be then applied to expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.\n\n\n== Analysis of cellular organization ==\nSeveral approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. A gene ontology category, cellular component, has been devised to capture subcellular localization in many biological databases.\n\n\n=== Microscopy and image analysis ===\nMicroscopic pictures allow for the location of organelles as well as molecules, which may be the source of abnormalities in diseases.\n\n\n=== Protein localization ===\nFinding the location of proteins allows us to predict what they do. This is called protein function prediction. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. There are well developed protein subcellular localization prediction resources available, including protein subcellular location databases, and prediction tools.\n\n\n=== Nuclear organization of chromatin ===\n\nData from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.\n\n\n== Structural bioinformatics ==\n\nFinding the structure of proteins is an important application of bioinformatics. The Critical Assessment of Protein Structure Prediction (CASP) is an open competition where worldwide research groups submit protein models for evaluating unknown protein models.\n\n\n=== Amino acid sequence ===\nThe linear amino acid sequence of a protein is called the primary structure. The primary structure can be easily determined from the sequence of codons on the DNA gene that codes for it. In most proteins, the primary structure uniquely determines the 3-dimensional structure of a protein in its native environment. An exception is the misfolded protein involved in bovine spongiform encephalopathy. This structure is linked to the function of the protein. Additional structural information includes the secondary, tertiary and quaternary structure. A viable general solution to the prediction of the function of a protein remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.\n\n\n=== Homology ===\nIn the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In structural bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. Homology modeling is used to predict the structure of an unknown protein from existing homologous proteins.\nOne example of this is hemoglobin in humans and the hemoglobin in legumes (leghemoglobin), which are distant relatives from the same protein superfamily. Both serve the same purpose of transporting oxygen in the organism. Although both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes and shared ancestor.\nOther techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.\nAnother aspect of structural bioinformatics include the use of protein structures for Virtual Screening models such as Quantitative Structure-Activity Relationship models and proteochemometric models (PCM). Furthermore, a protein's crystal structure can be used in simulation of for example ligand-binding studies and in silico mutagenesis studies.\nA 2021 deep-learning algorithms-based software called AlphaFold, developed by Google's DeepMind, greatly outperforms all other prediction software methods, and has released predicted structures for hundreds of millions of proteins in the AlphaFold protein structure database.\n\n\n== Network and systems biology ==\n\nNetwork analysis seeks to understand the relationships within biological networks such as metabolic or protein\u2013protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.\nSystems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.\n\n\n=== Molecular interaction networks ===\n\nTens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\u2013protein interactions only based on these 3D shapes, without performing protein\u2013protein interaction experiments. A variety of methods have been developed to tackle the protein\u2013protein docking problem, though it seems that there is still much work to be done in this field.\nOther interactions encountered in the field include Protein\u2013ligand (including drug) and protein\u2013peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.\n\n\n== Biodiversity informatics ==\n\nBiodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools. A growing area is also macro-ecology, i.e. the study of how biodiversity is connected to ecology and human impact, such as climate change.\n\n\n== Others ==\n\n\n=== Literature analysis ===\n\nThe enormous number of published literature makes it virtually impossible for individuals to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:\n\nAbbreviation recognition \u2013 identify the long-form and abbreviation of biological terms\nNamed-entity recognition \u2013 recognizing biological terms such as gene names\nProtein\u2013protein interaction \u2013 identify which proteins interact with which proteins from text\nThe area of research draws from statistics and computational linguistics.\n\n\n=== High-throughput image analysis ===\nComputational technologies are used to automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems can improve an observer's accuracy, objectivity, or speed. Image analysis is important for both diagnostics and research. Some examples are:\n\nhigh-throughput and high-fidelity quantification and sub-cellular localization (high-content screening, cytohistopathology, Bioimage informatics)\nmorphometrics\nclinical image analysis and visualization\ndetermining the real-time air-flow patterns in breathing lungs of living animals\nquantifying occlusion size in real-time imagery from the development of and recovery during arterial injury\nmaking behavioral observations from extended video recordings of laboratory animals\ninfrared measurements for metabolic activity determination\ninferring clone overlaps in DNA mapping, e.g. the Sulston score\n\n\n=== High-throughput single cell data analysis ===\n\nComputational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.\n\n\n=== Ontologies and data integration ===\nBiological ontologies are directed acyclic graphs of controlled vocabularies. They create categories for biological concepts and descriptions so they can be easily analyzed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.\nThe OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.\n\n\n== Databases ==\n\nDatabases are essential for bioinformatics research and applications. Databases exist for many different information types, including DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases can contain both empirical data (obtained directly from experiments) and predicted data (obtained from analysis of existing data). They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. Databases can have different formats, access mechanisms, and be public or private.\nSome of the most commonly used databases are listed below:\n\nUsed in biological sequence analysis: Genbank, UniProt\nUsed in structure analysis: Protein Data Bank (PDB)\nUsed in finding Protein Families and Motif Finding: InterPro, Pfam\nUsed for Next Generation Sequencing: Sequence Read Archive\nUsed in Network Analysis: Metabolic Pathway Databases (KEGG, BioCyc), Interaction Analysis Databases, Functional Networks\nUsed in design of synthetic genetic circuits: GenoCAD\n\n\n== Software and tools ==\nSoftware tools for bioinformatics include simple command-line tools, more complex graphical programs, and standalone web-services. They are made by bioinformatics companies or by public institutions.\n\n\n=== Open-source bioinformatics software ===\n\nMany free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have created opportunities for research groups to contribute to both bioinformatics regardless of funding. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.\nOpen-source bioinformatics software includes Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD.\nThe non-profit Open Bioinformatics Foundation and the annual Bioinformatics Open Source Conference promote open-source bioinformatics software.\n\n\n=== Web services in bioinformatics ===\nSOAP- and REST-based interfaces have been developed to allow client computers to use algorithms, data and computing resources from servers in other parts of the world. The main advantage are that end users do not have to deal with software and database maintenance overheads.\nBasic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.\n\n\n==== Bioinformatics workflow management systems ====\n\nA bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to\n\nprovide an easy-to-use environment for individual application scientists themselves to create their own workflows,\nprovide interactive tools for the scientists enabling them to execute their workflows and view their results in real-time,\nsimplify the process of sharing and reusing workflows between the scientists, and\nenable scientists to track the provenance of the workflow execution results and the workflow creation steps.\nSome of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.\n\n\n=== BioCompute and BioCompute Objects ===\nIn 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics. Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm. These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.\nIt was decided that the BioCompute paradigm would be in the form of digital 'lab notebooks' which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.\nIn 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as both a \"standard trial use\" document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.\n\n\n== Education platforms ==\nBioinformatics is not only taught as in-person masters degree at many universities. The computational nature of bioinformatics lends it to computer-aided and online learning. Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\u03c0 project or 4273pi project also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils. 4273 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\u03c0 operating system.\nMOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization at the University of California, San Diego, Genomic Data Science Specialization at Johns Hopkins University, and EdX's Data Analysis for Life Sciences XSeries at Harvard University.\n\n\n== Conferences ==\nThere are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nBioinformatics Resource Portal (SIB)", "link": "https://en.wikipedia.org/wiki/Bioinformatics"}, "List of bioinformatics journals": {"title": "List of bioinformatics journals", "content": "This is a list of notable peer-reviewed scientific journals that focus on bioinformatics and computational biology.", "link": "https://en.wikipedia.org/wiki/List_of_bioinformatics_journals"}, "Briefings in Bioinformatics": {"title": "Briefings in Bioinformatics", "content": "Briefings in Bioinformatics is a peer-reviewed scientific journal covering bioinformatics, including reviews of databases and analytical tools for genetics and molecular biology. It also publishes primary research papers on novel bioinformatic models and tools. It is published by Oxford University Press. The EMBnet community was initially involved in the creation of the journal. BiB was also supported by an educational grant from EMBnet.\n\n\n== Abstracting and indexing ==\nThe journal is abstracted and indexed in:\n\nAccording to the Journal Citation Reports, the journal had a 2021 impact factor of 13.994.\n\n\n== References ==\n\n\n== External links ==\nOfficial website", "link": "https://en.wikipedia.org/wiki/Briefings_in_Bioinformatics"}, "List of bioinformatics software": {"title": "List of bioinformatics software", "content": "The list of bioinformatics software tools can be split up according to the license used:\n\nList of proprietary bioinformatics software\nList of open-source bioinformatics software\nAlternatively, here is a categorization according to the respective bioinformatics subfield specialized on:\n\nSequence analysis software\nList of sequence alignment software\nList of alignment visualization software\nAlignment-free sequence analysis\nDe novo sequence assemblers\nList of gene prediction software\nList of disorder prediction software\nList of Protein subcellular localization prediction tools\nList of phylogenetics software\nList of phylogenetic tree visualization software\nCategory:Metagenomics_software\nStructural biology software\nList of molecular graphics systems\nList of protein-ligand docking software\nList of RNA structure prediction software\nList of software for protein model error verification\nList of protein secondary structure prediction programs\nList of protein structure prediction software\nCategory:Molecular dynamics software\nStructural alignment software\nOther\nCompression of genomic sequencing data\nBioinformatics workflow management system\nList of genetic engineering software\nList of systems biology visualization software\nList of systems biology modelling software\n2D gel analysis software\nList of mass spectrometry software", "link": "https://en.wikipedia.org/wiki/List_of_bioinformatics_software"}, "BMC Bioinformatics": {"title": "BMC Bioinformatics", "content": "BMC Bioinformatics is a peer-reviewed open access scientific journal covering bioinformatics and computational biology published by BioMed Central. It was established in 2000, and has been one of the fastest growing and most successful journals in the BMC Series of journals, publishing 1,000 articles in its first five years.\n\n\n== Abstracting and indexing ==\nThe journal is abstracted and indexed in:\n\nAccording to the Journal Citation Reports, the journal has a 2023 impact factor of 2.9.\n\n\n== References ==\n\n\n== External links ==\n\nOfficial website", "link": "https://en.wikipedia.org/wiki/BMC_Bioinformatics"}, "List of open-source bioinformatics software": {"title": "List of open-source bioinformatics software", "content": "This is a list of computer software which is made for bioinformatics and released under open-source software licenses with articles in Wikipedia.\n\n\n== See also ==\nList of sequence alignment software\nList of open-source healthcare software\nList of biomedical cybernetics software\nList of freeware health software\nList of genetic engineering software\nList of molecular graphics systems\nList of systems biology modelling software\nComparison of software for molecular mechanics modeling\nList of proprietary bioinformatics software\n\n\n== References ==\n\n\n== External links ==\nFree Biology Software \u2013 Free Software Directory \u2013 Free Software Foundation", "link": "https://en.wikipedia.org/wiki/List_of_open-source_bioinformatics_software"}, "List of bioinformatics institutions": {"title": "List of bioinformatics institutions", "content": "This is a list of major bioinformatics institutions.\n\nNational Center for Biotechnology Information (NCBI)\nEuropean Bioinformatics Institute (EMBL-EBI)\nAustralia Bioinformatics Resource  (EMBL-ABR)\nSwiss Institute of Bioinformatics (SIB)\nScripps Research Institute (TSRI)\nEuropean Molecular Biology Laboratory (EMBL)\nWellcome Trust Sanger Institute (WTSI)\nComputational Biology Department\nBroad Institute\nWhitehead Institute\nThe Institute for Genomic Research\nCenter for Biomolecular Science and Engineering\nNetherlands Bioinformatics Centre\nCOSBI\n Institute of Bioinformatics (IOB)\nMax Planck Institute for Molecular Cell Biology and Genetics (MPI-CBG)\nPartner Institute for Computational Biology\nFlatiron Institute\nDDBJ Center (DDBJ)\nBioinformatics Institute (Singapore)\nDatabase Center for Life Science (DBCLS)\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/List_of_bioinformatics_institutions"}, "Bioinformatics (journal)": {"title": "Bioinformatics (journal)", "content": "Bioinformatics is a biweekly peer-reviewed open-access scientific journal covering research and software in bioinformatics and computational biology. It is the official journal of the International Society for Computational Biology (ISCB), together with PLOS Computational Biology.  \nThe journal was established as Computer Applications in the Biosciences (CABIOS) in 1985. The founding editor-in-chief was Robert J. Beynon. In 1998, the journal obtained its current name and established an online version of the journal. It is published by Oxford University Press and, as of 2014, the editors-in-chief are Alfonso Valencia and Janet Kelso. Previous editors include Chris Sander, Gary Stormo, Christos Ouzounis, Martin Bishop, and Alex Bateman. In 2014, these five editors were appointed the first Honorary Editors of Bioinformatics. According to the Journal Citation Reports, the journal has a 2019 impact factor of 5.610\nFrom 1998 to 2004, Bioinformatics was the official journal of the ISCB. In 2004, as many ISCB members had institutional subscriptions to Bioinformatics, ISCB decided not to renew its contract with the journal. As of 2005, PLOS Computational Biology became the official ISCB journal. In January 2009 Bioinformatics again became an official journal of the ISCB, alongside PLOS Computational Biology.\nThe proceedings of the Intelligent Systems for Molecular Biology conference and the European Conference on Computational Biology have been published in special issues of Bioinformatics since 2001 and 2002, respectively.\nFollowing budget problems, Greek universities dropped their subscriptions to Bioinformatics in 2013.\n\n\n== References ==\n\n\n== External links ==\nOfficial website", "link": "https://en.wikipedia.org/wiki/Bioinformatics_(journal)"}, "List of bioinformatics companies": {"title": "List of bioinformatics companies", "content": "This is a list of bioinformatics companies that have articles at Wikipedia:\n\nApplied Maths provides the software suite BioNumerics\nAstrid Research\nBIOBASE\nBioBam Bioinformatics creator of Blast2GO\nBiomax Informatics AG bioinformatics services.\nBiovia (formerly Accelrys).\nChemical Computing Group MOE software for structural modelling\nCLC Bio Bioinformatics workbenches.\nDiscngine provides software and services for new molecule discovery.\nDNASTAR provides software for sequence editing and annotation, primer and clone design; sequence assembly & analysis; and protein sequence analysis and structure prediction.\nGene Codes Corporation\nGenedata software for data analysis and storage.\nGeneTalk web-based services.\nGenoCAD\nGenostar provides streamlined bioinformatics.\nInte:Ligand\nIntegromics\nInvitae\nInvitrogen creator of Vector NTI\nLeidos Biomedical Research Inc. formerly SAIC. Services are aimed at the Federal Government market.\nMacVector\nQIAGEN Silicon Valley (formerly Ingenuity Systems)\nQlucore\nPhalanx Biotech Group\nSeqera Labs\nSimBioSys created the eHITS software\nSRA International services aimed at the Federal Government market.\nStrand Life Sciences\nTimeLogic offers DeCypher FPGA-accelerated BLAST, Smith-Waterman, HMMER and other sequence search tools.", "link": "https://en.wikipedia.org/wiki/List_of_bioinformatics_companies"}, "DAVID": {"title": "David", "content": "David (; Biblical Hebrew: \u05d3\u05b8\u05bc\u05d5\u05b4\u05d3\u200e, romanized: D\u0101w\u012b\u1e0f, \"beloved one\") was a king of ancient Israel and Judah and the third king of the United Monarchy, according to the Hebrew Bible and Old Testament.\nAccording to Jewish works such as the Seder Olam Rabbah, Seder Olam Zutta, and Sefer ha-Qabbalah (all written over a thousand years later), David ascended the throne as the king of Judah in 885 BCE. The Tel Dan stele, an Aramaic-inscribed stone erected by a king of Aram-Damascus in the late 9th/early 8th centuries BCE to commemorate a victory over two enemy kings, contains the phrase bytdwd (\ud802\udd01\ud802\udd09\ud802\udd15\ud802\udd03\ud802\udd05\ud802\udd03), which is translated as \"House of David\" by most scholars. The Mesha stele, erected by King Mesha of Moab in the 9th century BCE, may also refer to the \"House of David\", although this is disputed. Apart from this, all that is known of David comes from biblical literature, the historicity of which has been extensively challenged, and there is little detail about David that is concrete and undisputed. Debates persist over several controversial issues: the exact timeframe of David's reign and the geographical boundaries of his kingdom; whether the story serves as a political defense of David's dynasty against accusations of tyranny, murder and regicide; the homoerotic relationship between David and Jonathan; whether the text is a Homer-like heroic tale adopting elements from its Ancient Near East parallels; and whether elements of the text date as late as the Hasmonean period.\nIn the biblical narrative of the Books of Samuel, David is described as a young shepherd and harpist whose heart is devoted to Yahweh, the one true God. He gains fame and becomes a hero by killing Goliath. He becomes a favorite of Saul, the first king of Israel, but is forced to go into hiding when Saul suspects David of plotting to take his throne. After Saul and his son Jonathan are killed in battle, David is anointed king by the tribe of Judah and eventually all the tribes of Israel. He conquers Jerusalem, makes it the capital of a united Israel, and brings the Ark of the Covenant to the city. He commits adultery with Bathsheba and arranges the death of her husband, Uriah the Hittite. David's son Absalom later tries to overthrow him, but David returns to Jerusalem after Absalom's death to continue his reign. David desires to build a temple to Yahweh, but is denied because of the bloodshed of his reign. He dies at age 70 and chooses Solomon, his son with Bathsheba, as his successor instead of his eldest son Adonijah. David is honored as an ideal king and the forefather of the future Hebrew Messiah in Jewish prophetic literature, and many psalms are attributed to him.\nDavid is also richly represented in post-biblical Jewish written and oral tradition and referenced in the New Testament. Early Christians interpreted the life of Jesus of Nazareth in light of references to the Hebrew Messiah and to David; Jesus is described as being directly descended from David in the Gospel of Matthew and the Gospel of Luke. In the Quran and hadith, David is described as an Israelite king as well as a prophet of Allah. The biblical David has inspired many interpretations in art and literature over the centuries.\n\n\n== Biblical account ==\n\n\n=== Family ===\n\nThe First Book of Samuel and the First Book of Chronicles both identify David as the son of Jesse, the Bethlehemite, the youngest of eight sons. He also had at least two sisters: Zeruiah, whose sons all went on to serve in David's army, and Abigail, whose son Amasa served in Absalom's army, Absalom being one of David's younger sons. While the Bible does not name his mother, the Talmud identifies her as Nitzevet, a daughter of a man named Adael, and the Book of Ruth claims him as the great-grandson of Ruth, the Moabite, by Boaz.\nDavid is described as cementing his relations with various political and national groups through marriage. According to 1 Samuel 17:25, King Saul said that he would make whoever killed Goliath a very wealthy man, give his daughter to him and declare his father's family exempt from taxes in Israel. Saul offered David his oldest daughter, Merab, a marriage David respectfully declined. Saul then gave Merab in marriage to Adriel the Meholathite. Having been told that his younger daughter Michal was in love with David, Saul gave her in marriage to David upon David's payment in Philistine foreskins (ancient Jewish historian Josephus lists the dowry as 100 Philistine heads). Saul became jealous of David and tried to have him killed. David escaped. Then Saul sent Michal to Galim to marry Palti, son of Laish. David then took wives in Hebron, according to 2 Samuel 3; they were Ahinoam the Yizre'elite; Abigail, the widow of Nabal the Carmelite; Maacah, the daughter of Talmay, king of Geshur; Haggith; Abital; and Eglah. Later, David wanted Michal back and Abner, Ish-bosheth's army commander, delivered her to him, causing Palti great grief.\nThe Book of Chronicles lists his sons with his various wives and concubines. In Hebron, David had six sons: Amnon, by Ahinoam; Daniel, by Abigail; Absalom, by Maachah; Adonijah, by Haggith; Shephatiah, by Abital; and Ithream, by Eglah. By Bathsheba, his sons were Shammua, Shobab, Nathan, and Solomon. David's sons born in Jerusalem of his other wives included Ibhar, Elishua, Eliphelet, Nogah, Nepheg, Japhia, Elishama and Eliada. Jerimoth, who is not mentioned in any of the genealogies, is mentioned as another of his sons in 2 Chronicles 11:18. His daughter Tamar, by Maachah, is raped by her half-brother Amnon. David fails to bring Amnon to justice for his violation of Tamar, because he is his firstborn and he loves him, and so Absalom (her full brother) kills Amnon to avenge Tamar. Despite the great sins they had committed, David showed grief at his sons' deaths, weeping twice for Amnon [2 Samuel 13:31\u201326] and seven times for Absalom.\n\n\n=== Narrative ===\n\nGod is angered when Saul, Israel's king, unlawfully offers a sacrifice and later disobeys a divine command both to kill all of the Amalekites and to destroy their confiscated property. Consequently, God sends the prophet Samuel to anoint a shepherd, David, the youngest son of Jesse of Bethlehem, to be king instead.\n\nAfter God sends an evil spirit to torment Saul, his servants recommend that he send for a man skilled in playing the lyre. A servant proposes David, whom the servant describes as \"skillful in playing, a man of valor, a warrior, prudent in speech, and a man of good presence; and the Lord is with him.\" David enters Saul's service as one of the royal armour-bearers and plays the lyre to soothe the king.\n\nWar comes between Israel and the Philistines, and the giant Goliath challenges the Israelites to send out a champion to face him in single combat. David, sent by his father to bring provisions to his brothers serving in Saul's army, declares that he can defeat Goliath. Refusing the king's offer of the royal armour, he kills Goliath with his sling. Saul inquires the name of the young hero's father.\nSaul sets David over his army. All Israel loves David, but his popularity causes Saul to fear him (\"What else can he wish but the kingdom?\"). Saul plots his death, but Saul's son Jonathan, who loves David, warns him of his father's schemes and David flees. He goes first to Nob, where he is fed by the priest Ahimelech and given Goliath's sword, and then to Gath, the Philistine city of Goliath, intending to seek refuge with King Achish there. Achish's servants or officials question his loyalty, and David sees that he is in danger there. He goes next to the cave of Adullam, where his family joins him. From there he goes to seek refuge with the king of Moab, but the prophet Gad advises him to leave and he goes to the Forest of Hereth, and then to Keilah, where he is involved in a further battle with the Philistines. Saul plans to besiege Keilah so that he can capture David, so David leaves the city in order to protect its inhabitants. From there he takes refuge in the mountainous Wilderness of Ziph.\n\nJonathan meets with David again and confirms his loyalty to David as the future king. After the people of Ziph notify Saul that David is taking refuge in their territory, Saul seeks confirmation and plans to capture David in the Wilderness of Maon, but his attention is diverted by a renewed Philistine invasion and David is able to secure some respite at Ein Gedi. Returning from battle with the Philistines, Saul heads to Ein Gedi in pursuit of David. Needing privacy \"to attend to his needs\", Saul enters the cave where, as it happens, David and his supporters are hiding. David realises he has an opportunity to kill Saul, but instead, he secretly cuts off a piece of Saul's robe. When Saul leaves the cave, David comes out to pay homage to the king, and to demonstrate using the piece of robe that he holds no malice towards him. The two are thus reconciled and Saul recognises David as his successor.\nA similar passage occurs in 1 Samuel 26, when David is able to infiltrate Saul's camp on the hill of Hachilah and remove his spear and a jug of water from his side while he and his guards lie asleep. In this account, David is advised by Abishai that this is his opportunity to kill Saul, but David declines, saying he will not \"stretch out [his] hand against the Lord's anointed\". In the morning, David once again demonstrates to Saul that, despite ample opportunity, he did not deign to harm him. Saul, despite having already reconciled with David, confesses that he has been wrong to pursue David, and blesses him.\nIn 1 Samuel 27:1\u20134, David begins to doubt Saul's sincerity, and reasons that the king will eventually make another attempt on his life. David appeals to king Achish of Gath to grant him and his family sanctuary. Achish agrees, and upon hearing that David has fled to Philistia, Saul ceases to pursue him, though no such pursuit seemed to be in progress at the time. Achish permits David to reside in Ziklag, close to the border between Philistia and Judah. To further ingratiate himself to Achish and the Philistines, David and his men raid the Geshurites, the Girzites, and the Amalekites, but lead the royal court to believe they are attacking the Israelites, the Jerahmeelites, and the Kenites. While Achish comes to believe that David had become a loyal vassal, the princes (or lords) of Gath remain unconvinced, and at their request, Achish instructs David to remain behind to guard the camp when the Philistines march against Saul. David returns to Ziklag and saves his wives and the citizens from an Amalekite raid. Jonathan and Saul are killed in battle with the Philistines, and after hearing of their deaths, David travels to Hebron, where he is anointed king over Judah. In the north, Saul's son Ish-Bosheth is anointed king of Israel, and war ensues until Ish-Bosheth is murdered.\n\nWith the death of Saul's son, the elders of Israel come to Hebron and David is anointed king over all of Israel. He conquers Jerusalem, previously a Jebusite stronghold, and makes it his capital. He brings the Ark of the Covenant to the city, intending to build a temple for God, but the prophet Nathan forbids it, prophesying that the temple would be built by one of David's sons. Nathan also prophesies that God has made a covenant with the house of David stating, \"your throne shall be established forever\". David wins additional victories over the Philistines, Moabites, Edomites, Amalekites, Ammonites and king Hadadezer of Aram-Zobah, after which they become tributaries. His fame increases as a result, earning the praise of figures like King Toi of Hamath, Hadadezer's rival.\n\nDuring a siege of the Ammonite capital of Rabbah, David remains in Jerusalem. He spies a woman, Bathsheba, bathing and summons her; she becomes pregnant. The text in the Bible does not explicitly state whether Bathsheba consented to sex with David. David calls her husband, Uriah the Hittite, back from the battle to rest, hoping that he will go home to have sex with his wife and the child will be presumed to be his. Uriah does not visit his wife, however, so David conspires to have him killed in the heat of battle. David then marries the widowed Bathsheba. In response, Nathan, after trapping the king in his guilt with a parable that actually described his sin in analogy, prophesies the punishment that will fall upon him, stating \"the sword shall never depart from your house.\" When David acknowledges that he has sinned, Nathan advises him that his sin is forgiven and he will not die, but the child will.\n\nIn fulfillment of Nathan's words, the child born of the union between David and Bathsheba dies, and another of David's sons, Absalom, fueled by vengeance and lust for power, rebels. Thanks to Hushai, a friend of David who was ordered to infiltrate Absalom's court to successfully sabotage his plans, Absalom's forces are routed at the battle of the Wood of Ephraim, and he is caught by his long hair in the branches of a tree where, contrary to David's order, he is killed by Joab, the commander of David's army. David laments the death of his favourite son: \"O my son Absalom, my son, my son Absalom! Would I had died instead of you, O Absalom, my son, my son!\" until Joab persuades him to recover from \"the extravagance of his grief\" and to fulfill his duty to his people. David returns to Gilgal and is escorted across the River Jordan and back to Jerusalem by the tribes of Judah and Benjamin.\n\nWhen David is old and bedridden, Adonijah, his eldest surviving son and natural heir, declares himself king. Bathsheba and Nathan go to David and obtain his agreement to crown Bathsheba's son Solomon as king, according to David's earlier promise, and the revolt of Adonijah is put down. David dies at the age of 70 after reigning for 40 years, and on his deathbed counsels Solomon to walk in the ways of God and to take revenge on his enemies.\n\n\n=== Psalms ===\n\nThe Book of Samuel calls David a skillful harp (lyre) player and \"the sweet psalmist of Israel.\" Yet, while almost half of the Psalms are headed \"A Psalm of David\" (also translated as \"to David\" or \"for David\") and tradition identifies several with specific events in David's life (e.g., Psalms 3, 7, 18, 34, 51, 52, 54, 56, 57, 59, 60, 63 and 142), the headings are late additions and no psalm can be attributed to David with certainty.\nPsalm 34 is attributed to David on the occasion of his escape from Abimelech (or King Achish) by pretending to be insane. According to the parallel narrative in 1 Samuel 21, instead of killing the man who had exacted so many casualties from him, Abimelech allows David to leave, exclaiming, \"Am I so short of madmen that you have to bring this fellow here to carry on like this in front of me? Must this man come into my house?\"\n\n\n== Interpretation in Abrahamic tradition ==\n\n\n=== Rabbinic Judaism ===\nDavid is an important figure in Rabbinic Judaism, with many legends about him. According to one tradition, David was raised as the son of his father Jesse and spent his early years herding his father's sheep in the wilderness while his brothers were in school.\nDavid's adultery with Bathsheba is interpreted as an opportunity to demonstrate the power of repentance, and the Talmud says it was not adultery at all, citing a Jewish practice of divorce on the eve of battle. Furthermore, according to Talmudic sources, Uriah's death was not murder, because Uriah had committed a capital offense by refusing to obey a direct command from the King. However, in tractate Sanhedrin, David expressed remorse over his transgressions and sought forgiveness. God ultimately forgave David and Bathsheba but would not remove their sins from Scripture.\nIn Jewish legend, David's sin with Bathsheba is the punishment for David's excessive self-consciousness. He had besought God to lead him into temptation so that he might give proof of his constancy like Abraham, Isaac, and Jacob, who successfully passed the test and whose names later were united with God's, while David failed through the temptation of a woman.\nAccording to midrashim, Adam gave up 70 years of his life for the life of David. Also, according to the Talmud Yerushalmi, David was born and died on the Jewish holiday of Shavuot (Feast of Weeks). His piety was said to be so great that his prayers could bring down things from Heaven.\n\n\n=== Christianity ===\n\nThe Messiah concept is fundamental in Christianity. Originally an earthly king ruling by divine appointment (\"the anointed one\", as the title Messiah had it), in the last two centuries BCE the \"son of David\" became the apocalyptic and heavenly one who would deliver Israel and usher in a new kingdom. This was the background to the concept of Messiahship in early Christianity, which interpreted the career of Jesus \"by means of the titles and functions assigned to David in the mysticism of the Zion cult, in which he served as priest-king and in which he was the mediator between God and man\".\nThe early Church believed that \"the life of David foreshadowed the life of Christ; Bethlehem is the birthplace of both; the shepherd life of David points out Christ, the Good Shepherd; the five stones chosen to slay Goliath are typical of the five wounds; the betrayal by his trusted counsellor, Ahitophel, and the passage over the Cedron remind us of Christ's Sacred Passion. Many of the Davidic Psalms, as we learn from the New Testament, are clearly typical of the future Messiah.\" In the Middle Ages, \"Charlemagne thought of himself, and was viewed by his court scholars, as a 'new David'. [This was] not in itself a new idea, but [one whose] content and significance were greatly enlarged by him\".\nWestern Rite churches (Lutheran, Roman Catholic) celebrate David's feast day on 29 December or 6 October, Eastern-rite on 19 December. The Eastern Orthodox and Eastern Catholic Churches celebrate the feast day of the \"Holy Righteous Prophet and King David\" on the Sunday of the Holy Forefathers (two Sundays before the Great Feast of the Nativity of the Lord) and on the Sunday of the Holy Fathers (Sunday before the Nativity), when he is commemorated together with other ancestors of Jesus. He is also commemorated on the Sunday after the Nativity, together with Joseph and James, the Brother of the Lord and on 26 December (Synaxis of the Mother of God).\n\n\n==== Middle Ages ====\n\nIn European Christian culture of the Middle Ages, David was made a member of the Nine Worthies, a group of heroes encapsulating all the ideal qualities of chivalry. His life was thus proposed as a valuable subject for study by those aspiring to chivalric status. This aspect of David in the Nine Worthies was popularised first through literature, and thereafter adopted as a frequent subject for painters and sculptors.\nDavid was considered a model ruler and a symbol of divinely ordained monarchy throughout medieval Western Europe and Eastern Christendom. He was perceived as the biblical predecessor to Christian Roman and Byzantine emperors and the name \"New David\" was used as an honorific reference to these rulers. The Georgian Bagratids and the Solomonic dynasty of Ethiopia claimed direct biological descent from him. Likewise, kings of the Frankish Carolingian dynasty frequently connected themselves to David; Charlemagne himself occasionally used \"David\" his pseudonym.\n\n\n=== Islam ===\n\nDavid (Arabic: \u062f\u0627\u0648\u0648\u062f D\u0101'\u016bd or D\u0101w\u016bd) is an important figure in Islam as one of the major prophets God sent to guide the Israelites. He is mentioned several times in the Quran with the Arabic name \u062f\u0627\u0648\u062f, D\u0101w\u016bd or D\u0101'\u016bd, often with his son Solomon. In the Quran, David killed Goliath (Q2:251), a giant soldier in the Philistine army. When David killed Goliath, God granted him kingship and wisdom and enforced it (Q38:20). David was made God's \"vicegerent on earth\" (Q38:26) and God further gave David sound judgment (Q21:78; Q37:21\u201324, Q26) as well as the Psalms, regarded as books of divine wisdom (Q4:163; Q17:55). The birds and mountains united with David in uttering praise to God (Q21:79; Q34:10; Q38:18), while God made iron soft for David (Q34:10), God also instructed David in the art of fashioning chain mail out of iron (Q21:80); this knowledge gave David a major advantage over his bronze and cast iron-armed opponents, not to mention the cultural and economic impact. Together with Solomon, David gave judgment in a case of damage to the fields (Q21:78) and David judged the matter between two disputants in his prayer chamber (Q38:21\u201323). Since there is no mention in the Quran of the wrong David did to Uriah nor any reference to Bathsheba, Muslims reject this narrative.\nMuslim tradition and the hadith stress David's zeal in daily prayer as well as in fasting. Quran commentators, historians and compilers of the numerous Stories of the Prophets elaborate upon David's concise quranic narratives and specifically mention David's gift in singing his Psalms, his beautiful recitation, and his vocal talents. His voice is described as having a captivating power, weaving its influence not only over man but over all beasts and nature, who would unite with him to praise God.\n\n\n== Historicity ==\n\n\n=== Literary analysis ===\n\nBiblical literature and archaeological finds are the only sources that attest to David's life. Some scholars have concluded that this was likely compiled from contemporary records of the 11th and 10th centuries BCE, but that there is no clear historical basis for determining the exact date of compilation. Other scholars believe that the Books of Samuel were substantially composed during the time of Josiah, king of Judah, at the end of the 7th century BCE, extended during the Babylonian captivity and substantially complete by about 550 BCE. Old Testament scholar A. Graeme Auld contends that further editing was done even after then\u2014the silver quarter-shekel Saul's servant offers to Samuel in 1 Samuel 9:8 \"almost certainly fixes the date of the story in the Persian or Hellenistic period\" because a quarter-shekel was known to exist in Hasmonean times. The authors and editors of Samuel drew on many earlier sources, including, for their history of David, the \"history of David's rise\" and the \"succession narrative\". The Books of Chronicles, which tells the story from a different point of view, was probably composed in the period 350\u2013300 BCE, and uses Samuel and Kings as its source.\nBiblical evidence indicates that David's Judah was something less than a full-fledged monarchy: it often calls him nagid \"prince, chief\" (Hebrew: \u05e0\u05b8\u05d2\u05b4\u05d9\u05d3, romanized: n\u0101g\u012b\u1e0f), rather than melekh \"king\" (\u05de\u05b6\u05dc\u05b6\u05da\u05b0); David sets up none of the complex bureaucracy that a kingdom needs. His army is made up of volunteers and his followers are largely relations or from his home region of Hebron.\nBeyond this, the full range of possible interpretations is available. A number of scholars consider the David story to be a heroic tale similar to the legend of King Arthur or the epics of Homer, while others find such comparisons questionable.\nOne theme paralleled with other Near Eastern literature is the homoerotic nature of the relationship between David and Jonathan. The instance in the Book of Jashar, excerpted in 2 Samuel 1:26, where David \"proclaims that Jonathan's love was sweeter to him than the love of a woman\", has been compared to Achilles' comparison of Patroclus to a girl and Gilgamesh's love for Enkidu \"as a woman\". Others hold that the David story is a political apology\u2014an answer to contemporary charges against him, of his involvement in murders and regicide. The authors and editors of Samuel and Chronicles aimed not to record history but to promote David's reign as inevitable and desirable, and for this reason there is little about David that is concrete and undisputed. Other scholars argue that, notwithstanding the apologetic tenor of the story, the authors of Samuel were also critical of David in several respects, suggesting that the text presents a complex portrait of him rather than a purely propagandistic one.\nSome other studies of David have been written: Baruch Halpern has pictured him as a brutal tyrant, a murderer, and a lifelong vassal of Achish, the Philistine king of Gath; Steven McKenzie argues that David came from a wealthy family, and was an \"ambitious and ruthless\" tyrant who murdered his opponents, including his sons. Joel S. Baden has called him \"an ambitious, ruthless, flesh-and-blood man who achieved power by any means necessary, including murder, theft, bribery, sex, deceit, and treason\". William G. Dever described him as \"a serial killer\".\nJacob L. Wright has written that the most popular legends about David, including his killing of Goliath, his affair with Bathsheba, and his ruling of the unified Kingdom of Israel rather than just Judah, are the creation of those who lived generations after him, in particular those living in the late Persian or Hellenistic periods.\n\n\n=== Archaeological findings ===\n\nThe Tel Dan stele, discovered in 1993, is an inscribed stone erected by Hazael, a king of Damascus in the late 9th/early 8th centuries BCE. It commemorates the king's victory over two enemy kings, and contains the phrase \ud802\udd01\ud802\udd09\ud802\udd15\ud802\udd03\ud802\udd05\ud802\udd03, bytdwd, which most scholars translate as \"House of David\". Other scholars have challenged this reading, but this is likely a reference to a dynasty of the Kingdom of Judah which traced its ancestry to a founder named David.\nTwo epigraphers, Andr\u00e9 Lemaire and \u00c9mile Puech, hypothesised in 1994 that the Mesha Stele from Moab, dating from the 9th century, also contain the words \"House of David\" at the end of Line 31, although this was considered as less certain than the mention in the Tel Dan inscription. In May 2019, Israel Finkelstein, Nadav Na'aman, and Thomas R\u00f6mer concluded from the new images that the ruler's name contained three consonants and started with a bet, which excludes the reading \"House of David\" and, in conjunction with the monarch's city of residence \"Horonaim\" in Moab, makes it likely that the one mentioned is King Balak, a name also known from the Hebrew Bible. Later that year, Michael Langlois used high-resolution photographs of both the inscription itself, and the 19th-century original squeeze of the then still intact stele to reaffirm Lemaire's view that line 31 contains the phrase \"House of David\". Replying to Langlois, Na'aman argued that the \"House of David\" reading is unacceptable because the resulting sentence structure is extremely rare in West Semitic royal inscriptions.\n\nBesides the two steles, Bible scholar and Egyptologist Kenneth Kitchen suggests that David's name also appears in a relief of the pharaoh Shoshenq I, who is usually identified with Shishak in the Bible. The relief claims that Shoshenq raided places in Palestine in 925 BCE, and Kitchen interprets one place as \"Heights of David\", which was in southern Judah and the Negev where the Bible says David took refuge from Saul. The relief is damaged and interpretation is uncertain.\n\n\n=== Archaeological analysis ===\nOf the evidence in question, John Haralson Hayes and James Maxwell Miller wrote in 2006: \"If one is not convinced in advance by the biblical profile, then there is nothing in the archaeological evidence itself to suggest that much of consequence was going on in Palestine during the tenth century BCE, and certainly nothing to suggest that Jerusalem was a great political and cultural center.\" This echoed the 1995 conclusion of Am\u00e9lie Kuhrt, who noted that \"there are no royal inscriptions from the time of the united monarchy (indeed very little written material altogether), and not a single contemporary reference to either David or Solomon,\" while noting, \"against this must be set the evidence for substantial development and growth at several sites, which is plausibly related to the tenth century.\"\nIn 2007, Israel Finkelstein and Neil Asher Silberman stated that the archaeological evidence shows that Judah was sparsely inhabited and Jerusalem no more than a small village. The evidence suggested that David ruled only as a chieftain over an area which cannot be described as a state or as a kingdom, but more as a chiefdom, much smaller and always overshadowed by the older and more powerful kingdom of Israel to the north. They posited that Israel and Judah were not monotheistic at the time and that later 7th-century redactors sought to portray a past golden age of a united, monotheistic monarchy in order to serve contemporary needs. They noted a lack of archeological evidence for David's military campaigns and a relative underdevelopment of Jerusalem, the capital of Judah, compared to a more developed and urbanized Samaria, capital of Israel during the 9th century BCE.\nIn 2010, Amihai Mazar wrote that the United Monarchy of the 10th century BCE can be described as a \"state in development\". He compared David to Labaya, a Caananite warlord living during the time of Pharaoh Akhenaten. While Mazar believes that David reigned over Israel during the 11th century BCE, he argues that much of the Biblical text is of \"literary-legendary nature\". According to William G. Dever, the reigns of Saul, David and Solomon are reasonably well attested, but \"most archeologists today would argue that the United Monarchy was not much more than a kind of hill-country chiefdom\".\nLester L. Grabbe wrote in 2017: \"The main question is what kind of settlement Jerusalem was in Iron IIA: was it a minor settlement, perhaps a large village or possibly a citadel but not a city, or was it the capital of a flourishing\u2014or at least an emerging\u2014state? Assessments differ considerably\". Isaac Kalimi wrote in 2018, \"No contemporaneous extra-biblical source offers any account of the political situation in Israel and Judah during the tenth century BCE, and as we have seen, the archaeological remains themselves cannot provide any unambiguous evidence of events.\"\nThe view of Davidic Jerusalem as a village has been challenged by Eilat Mazar's excavation of the Large Stone Structure and the Stepped Stone Structure in 2005. Mazar proposed that these two structures may have been architecturally linked as one unit and that they date to the time of King David. Mazar supports this dating with a number of artifacts, including pottery, two Phoenician-style ivory inlays, a black-and-red jug, and a radiocarbon-dated bone, estimated to be from the 10th century. Dever, Amihai Mazar, Avraham Faust, and Nadav Na'aman have argued in favour of the 10th-century BCE dating and responded to challenges to it. In 2010, Eilat Mazar announced the discovery of part of the ancient city walls around the City of David, which she believes date to the 10th century BCE. According to Mazar, this would prove that an organized state did exist in the 10th century. In 2006, Kenneth Kitchen came to a similar conclusion, arguing that \"the physical archaeology of tenth-century Canaan is consistent with the former existence of a unified state on its terrain.\"\nScholars such as Israel Finkelstein, Lily Singer-Avitz, Ze'ev Herzog and David Ussishkin do not accept these conclusions. Finkelstein does not accept the dating of these structures to the 10th century BCE, based in part on the fact that later structures on the site penetrated deep into underlying layers, that the entire area had been excavated in the early 20th century and then backfilled, that pottery from later periods was found below earlier strata, and that consequently the finds collected by E. Mazar cannot necessarily be considered as retrieved in situ. Aren Maeir said in 2010 that he has seen no evidence that these structures are from the 10th century BCE and that proof of the existence of a strong, centralized kingdom at that time remains \"tenuous.\"\nExcavations at Khirbet Qeiyafa by archaeologists Yosef Garfinkel and Saar Ganor found an urbanized settlement radiocarbon dated to the 10th century, which supports the existence of an urbanised kingdom. The Israel Antiquities Authority stated: \"The excavations at Khirbat Qeiyafa clearly reveal an urban society that existed in Judah already in the late eleventh century BCE. It can no longer be argued that the Kingdom of Judah developed only in the late eighth century BCE or at some other later date.\" But other scholars have criticized the techniques and interpretations to reach some conclusions related to Khirbet Qeiyafa, such as Israel Finkelstein and Alexander Fantalkin of Tel Aviv University, who have instead proposed that the city is to be identified as part of a northern Israelite polity.\nIn 2018, Avraham Faust and Yair Sapir stated that a Canaanite site at Tel Eton, about 30 miles from Jerusalem, was taken over by a Judahite community by peaceful assimilation and transformed from a village into a central town at some point in the late 11th or early 10th century BCE. This transformation used some ashlar blocks in construction, which they argued supports the United Monarchy theory.\n\n\n== Art and literature ==\n\n\n=== Literature ===\n\nLiterary works about David include:\n\n1517 The Davidiad is a Neo-Latin epic poem by the Croatian national poet, Roman Catholic priest, and Renaissance humanist Marko Maruli\u0107 (whose name is sometimes Latinized as \"Marcus Marulus\"). In addition to the small portions that attempt to recall the epics of Homer, The Davidiad is heavily modeled upon Virgil's Aeneid. This is so much the case that Maruli\u0107's contemporaries called him the \"Christian Virgil from Split.\" The philologist Miroslav Marcovich also detects, \"the influence of Ovid, Lucan, and Statius\" in the work.\n1681\u201382 Dryden's long poem Absalom and Achitophel is an allegory that uses the story of the rebellion of Absalom against King David as the basis for his satire of the contemporary political situation, including events such as the Monmouth Rebellion (1685), the Popish Plot (1678) and the Exclusion Crisis.\n1893 Sir Arthur Conan Doyle may have used the story of David and Bathsheba as a foundation for the Sherlock Holmes story The Adventure of the Crooked Man. Holmes mentions \"the small affair of Uriah and Bathsheba\" at the end of the story.\n1928 Elmer Davis's novel Giant Killer retells and embellishes the biblical story of David, casting David as primarily a poet who managed always to find others to do the \"dirty work\" of heroism and kingship. In the novel, Elhanan in fact killed Goliath but David claimed the credit; and Joab, David's cousin and general, took it upon himself to make many of the difficult decisions of war and statecraft when David vacillated or wrote poetry instead.\n1936 William Faulkner's Absalom, Absalom! refers to the story of Absalom, David's son; his rebellion against his father and his death at the hands of David's general, Joab. In addition it parallels Absalom's vengeance for the rape of his sister Tamar by his half-brother, Amnon.\n1946 Gladys Schmitt's novel David the King was a richly embellished biography of David's entire life. The book took a risk, especially for its time, in portraying David's relationship with Jonathan as overtly homoerotic, but was ultimately panned by critics as a bland rendition of the title character.\n1966 Juan Bosch, a Dominican political leader and writer, wrote David: Biography of a King, as a realistic portrayal of David's life and political career.\n1970 Dan Jacobson's The Rape of Tamar is an imagined account, by one of David's courtiers Yonadab, of the rape of Tamar by Amnon.\n1972 Stefan Heym wrote The King David Report in which the historian Ethan compiles upon King Solomon's orders \"a true and authoritative report on the life of David, Son of Jesse\"\u2014the East German writer's wry depiction of a court historian writing an \"authorized\" history, many incidents clearly intended as satirical references to the writer's own time.\n1974 In Thomas Burnett Swann's biblical fantasy novel How are the Mighty Fallen, David and Jonathan are explicitly stated to be lovers. Moreover, Jonathan is a member of a winged semi-human race (possibly nephilim), one of several such races coexisting with humanity but often persecuted by it.\n1980 Malachi Martin's factional novel King of Kings: A Novel of the Life of David relates the life of David, Adonai's champion in his battle with the Philistine deity Dagon.\n1984 Joseph Heller wrote a novel based on David called God Knows, published by Simon & Schuster. Told from the perspective of an aging David, the humanity\u2014rather than the heroism\u2014of various biblical characters is emphasized. The portrayal of David as a man of flaws such as greed, lust, selfishness, and his alienation from God, the falling apart of his family is a distinctly 20th-century interpretation of the events told in the Bible.\n1993 Madeleine L'Engle's novel Certain Women explores family, the Christian faith, and the nature of God through the story of King David's family and an analogous modern family's saga.\n1995 Allan Massie wrote King David, a novel about David's career that portrays the king's relationship to Jonathan as sexual.\n2015 Geraldine Brooks wrote a novel about David, The Secret Chord, told from the point of view of the prophet Nathan.\n2020 Michael Arditti wrote The Anointed, a novel about David told by three of his wives, Michal, Abigail and Bathsheba.\n\n\n=== Paintings ===\n1599 Caravaggio David and Goliath\nc.\u20091610 Caravaggio David with the Head of Goliath\n1616 Peter Paul Rubens David Slaying Goliath\nc. 1619 Caravaggio, David and Goliath\n\n\n=== Sculptures ===\n\n1440? Donatello, David\n1473\u20131475 Verrocchio, David\n1501\u20131504 Michelangelo, David\n1623\u20131624 Gian Lorenzo Bernini, David\n\n\n=== Film ===\nDavid has been depicted several times in films; these are some of the best-known:\n\n1951 David and Bathsheba, directed by Henry King, with Gregory Peck in the role of David.\n1959 Solomon and Sheba, directed by King Vidor, with Finlay Currie in the role of an aged King David.\n1961 A Story of David, directed by Bob McNaught, with Jeff Chandler in the role of David.\n1985 King David, directed by Bruce Beresford, with Richard Gere in the role of David.\n1996 Dave and the Giant Pickle\n\n\n=== Television ===\n1976 The Story of David, a made-for-TV film with Timothy Bottoms and Keith Michell as King David at different ages.\n1997 David, a TV-film with Nathaniel Parker as King David and Leonard Nimoy as the Prophet Samuel.\n1997 Solomon, a sequel to David, with Max von Sydow playing an older King David.\n2009 Kings, a re-imagining loosely based on the biblical story, with David played by Christopher Egan.\nKing David is the focus of the second episode of History Channel's Battles BC documentary, which detailed all of his military exploits in the bible.\n2012 Rei Davi, a Brazilian miniseries with Leonardo Br\u00edcio as David.\n2013 The Bible, a miniseries with Langley Kirkwood in the role of David.\n2016 Of Kings and Prophets in which David is played by Olly Rix.\n\n\n=== Music ===\n\nThe traditional birthday song Las Ma\u00f1anitas mentions King David as the original singer in its lyrics.\n1622 Thomas Tomkins's choral anthem \"When David Heard\", about David's response to the death of his son Absalom, is published in the anthology Songs of 1622.\n1738 George Frideric Handel's oratorio Saul features David as one of its main characters.\n1921 Arthur Honegger's oratorio Le Roi David with a libretto by Ren\u00e9 Morax, instantly became a staple of the choral repertoire.\n1954 Darius Milhaud's opera David premieres in Jerusalem in celebration of the 3,000th anniversary of the establishment of that city by David.\n1964 Bob Dylan alludes to David in the last line of his song \"When The Ship Comes In\" (\"And like Goliath, they'll be conquered\").\n1965 Leonard Bernstein described the second movement of his Chichester Psalms, which features a setting of Psalm 23, sung by a boy soloist accompanied by a harp, as a \"musical evocation of King David, the shepherd-psalmist\".\n1983 Bob Dylan refers to David in his song \"Jokerman\" (\"Michelangelo indeed could've carved out your features\").\n1984 Leonard Cohen's song \"Hallelujah\" has references to David (\"there was a secret chord that David played and it pleased the Lord\", \"The baffled king composing Hallelujah\") and Bathsheba (\"you saw her bathing on the roof\") in its opening verses.\n1990 The song \"One of the Broken\" by Paddy McAloon, performed by Prefab Sprout on the album Jordan: The Comeback, has a reference to David (\"I remember King David, with his harp and his beautiful, beautiful songs, I answered his prayers, and showed him a place where his music belongs\").\n1991 \"Mad About You\", a song on Sting's album The Soul Cages, explores David's obsession with Bathsheba from David's perspective.\n2000 The song \"Gimme a Stone\" appears on the Little Feat album Chinese Work Songs chronicles the duel with Goliath and contains a lament to Absalom as a bridge.\n\n\n=== Musical theater ===\n1997 King David, sometimes described as a modern oratorio, with a book and lyrics by Tim Rice and music by Alan Menken.\n\n\n=== Radio ===\n1962 Twilight of a Hero, an Australian radio play that sold to the BBC\n\n\n=== Playing cards ===\nFor a considerable period, starting in the 15th century and continuing until the 19th, French playing card manufacturers assigned to each of the court cards names taken from history or mythology.  In this context, the King of spades was often known as \"David\".\n\n\n== Image gallery ==\n\n\n== See also ==\n\nDavid and Jonathan\nDavid's Mighty Warriors\nDavid's Tomb\nCity of David\nTower of David\nKings of Israel and Judah\nLarge Stone Structure\nMidrash Shmuel (aggadah)\nSons of David\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\nComplete Bible Genealogy\u2014David's family tree\nDavid engravings from the De Verda collection\nKing David at the Christian Iconography web site\nThe History of David, by William Caxton\n\"David\" by Kent Harold Richards at Bible Odyssey", "link": "https://en.wikipedia.org/wiki/David"}}, "Data mining": {"Examples of data mining": {"title": "Examples of data mining", "content": "Data mining, the process of discovering patterns in large data sets, has been used in many applications.\n\n\n== Business ==\nIn business, data mining is the analysis of historical business activities, stored as static data in data warehouse databases. The goal is to reveal hidden patterns and trends. Data mining software uses advanced pattern recognition algorithms to sift through large amounts of data to assist in discovering previously unknown strategic business information. Examples of what businesses use data mining for is to include performing market analysis to identify new product bundles, finding the root cause of manufacturing problems, to prevent customer attrition and acquire new customers, cross-selling to existing customers, and profiling customers with more accuracy.\n\nIn today's world raw data is being collected by companies at an exploding rate. For example, Walmart processes over 20 million point-of-sale transactions every day. This information is stored in a centralized database, but would be useless without some type of data mining software to analyze it. If Walmart analyzed their point-of-sale data with data mining techniques they would be able to determine sales trends, develop marketing campaigns, and more accurately predict customer loyalty.\nCategorization of the items available in the e-commerce site is a fundamental problem. A correct item categorization system is essential for user experience as it helps determine the items relevant to him for search and browsing.  Item categorization can be formulated as a supervised classification problem in data mining where the categories are the target classes and the features are the words composing some textual description of the items. One of the approaches is to find groups initially which are similar and place them together in a latent group.  Now given a new item, first classify into a latent group which is called coarse level classification. Then, do a second round of classification to find the category to which the item belongs to.\nEvery time a credit card or a store loyalty card is being used, or a warranty card is being filled, data is being collected about the user's behavior. Many people find the amount of information stored about us from companies, such as Google, Facebook, and Amazon, disturbing and are concerned about privacy. Although there is the potential for our personal data to be used in harmful, or unwanted, ways it is also being used to make our lives better. For example, Ford and Audi hope to one day collect information about customer driving patterns so they can recommend safer routes and warn drivers about dangerous road conditions.\nData mining in customer relationship management applications can contribute significantly to the bottom line. Rather than randomly contacting a prospect or customer through a call center or sending mail, a company can concentrate its efforts on prospects that are predicted to have a high likelihood of responding to an offer. More sophisticated methods may be used to optimize resources across campaigns so that one may predict to which channel and to which offer an individual is most likely to respond (across all potential offers). Additionally, sophisticated applications could be used to automate mailing. Once the results from data mining (potential prospect/customer and channel/offer) are determined, this \"sophisticated application\" can either automatically send an e-mail or a regular mail. Finally, in cases where many people will take an action without an offer, \"uplift modeling\" can be used to determine which people have the greatest increase in response if given an offer. Uplift modeling thereby enables marketers to focus mailings and offers on persuadable people, and not to send offers to people who will buy the product without an offer. Data clustering can also be used to automatically discover the segments or groups within a customer data set.\nBusinesses employing data mining may see a return on investment, but also they recognize that the number of predictive models can quickly become very large. For example, rather than using one model to predict how many customers will churn, a business may choose to build a separate model for each region and customer type. In situations where a large number of models need to be maintained, some businesses turn to more automated data mining methodologies.\nData mining can be helpful to human resources (HR) departments in identifying the characteristics of their most successful employees. Information obtained \u2013 such as universities attended by highly successful employees \u2013 can help HR focus recruiting efforts accordingly. Additionally, Strategic Enterprise Management applications help a company translate corporate-level goals, such as profit and margin share targets, into operational decisions, such as production plans and workforce levels.\nData mining can be helpful to organizations. Organizational Data Mining (ODM) is defined as leveraging data mining (DM) tools and technologies to enhance organizational decision-making process by transforming data into valuable and actionable knowledge in order to gain a strategic and business competitive advantage. Data obtained \u2013 such as employee turnover rates \u2013 can help organizations focus their retention efforts accordingly. Additionally, organizational performance management data-mining and analytics applications help firms translate company-level goals, such as profit and sales targets, into operational decisions, as workers KPI and required measured effort levels. \nMarket basket analysis has been used to identify the purchase patterns of the Alpha Consumer. Analyzing the data collected on this type of user has allowed companies to predict future buying trends and forecast supply demands.\nData mining is a highly effective tool in the catalog marketing industry. Catalogers have a rich database of history of their customer transactions for millions of customers dating back a number of years. Data mining tools can identify patterns among customers and help identify the most likely customers to respond to upcoming mailing campaigns.\nData mining for business applications can be integrated into a complex modeling and decision making process. LIONsolver uses Reactive business intelligence (RBI) to advocate a \"holistic\" approach that integrates data mining, modeling, and interactive visualization into an end-to-end discovery and continuous innovation process powered by human and automated learning.\nIn the area of decision making, the RBI approach has been used to mine knowledge that is progressively acquired from the decision maker, and then self-tune the decision method accordingly. The relation between the quality of a data mining system and the amount of investment that the decision maker is willing to make was formalized by providing an economic perspective on the value of \u201cextracted knowledge\u201d in terms of its payoff to the organization This decision-theoretic classification framework was applied to a real-world semiconductor wafer manufacturing line, where decision rules for effectively monitoring and controlling the semiconductor wafer fabrication line were developed.\nAn example of data mining related to an integrated-circuit (IC) production line is described in the paper \"Mining IC Test Data to Optimize VLSI Testing.\" In this paper, the application of data mining and decision analysis to the problem of die-level functional testing is described. Experiments mentioned demonstrate the ability to apply a system of mining historical die-test data to create a probabilistic model of patterns of die failure. These patterns are then utilized to decide, in real time, which die to test next and when to stop testing. This system has been shown, based on experiments with historical test data, to have the potential to improve profits on mature IC products. Other examples of the application of data mining methodologies in semiconductor manufacturing environments suggest that data mining methodologies may be particularly useful when data is scarce, and the various physical and chemical parameters that affect the process exhibit highly complex interactions. Another implication is that on-line monitoring of the semiconductor manufacturing process using data mining may be highly effective.\n\n\n== Science and engineering ==\nIn recent years, data mining has been used widely in the areas of science and engineering, such as bioinformatics, genetics, medicine, education and electrical power engineering.\n\nIn the study of human genetics, sequence mining helps address the important goal of understanding the mapping relationship between the inter-individual variations in human DNA sequence and the variability in disease susceptibility. In simple terms, it aims to find out how the changes in an individual's DNA sequence affects the risks of developing common diseases such as cancer, which is of great importance to improving methods of diagnosing, preventing, and treating these diseases. One data mining method that is used to perform this task is known as multifactor dimensionality reduction.\nIn the area of electrical power engineering, data mining methods have been widely used for condition monitoring of high voltage electrical equipment. The purpose of condition monitoring is to obtain valuable information on, for example, the status of the insulation (or other important safety-related parameters). Data clustering techniques \u2013 such as the self-organizing map (SOM), have been applied to vibration monitoring and analysis of transformer on-load tap-changers (OLTCS). Using vibration monitoring, it can be observed that each tap change operation generates a signal that contains information about the condition of the tap changer contacts and the drive mechanisms. Obviously, different tap positions will generate different signals. However, there was considerable variability amongst normal condition signals for exactly the same tap position. SOM has been applied to detect abnormal conditions and to hypothesize about the nature of the abnormalities.\nData mining methods have been applied to dissolved gas analysis (DGA) in power transformers. DGA, as a diagnostics for power transformers, has been available for many years. Methods such as SOM has been applied to analyze generated data and to determine trends which are not obvious to the standard DGA ratio methods (such as Duval Triangle).\nIn educational research, where data mining has been used to study the factors leading students to choose to engage in behaviors which reduce their learning, and to understand factors influencing university student retention. A similar example of social application of data mining is its use in expertise finding systems, whereby descriptors of human expertise are extracted, normalized, and classified so as to facilitate the finding of experts, particularly in scientific and technical fields. In this way, data mining can facilitate institutional memory.\nData mining methods of biomedical data facilitated by domain ontologies, mining clinical trial data, and traffic analysis using SOM.\nIn adverse drug reaction surveillance, the Uppsala Monitoring Centre has, since 1998, used data mining methods to routinely screen for reporting patterns indicative of emerging drug safety issues in the WHO global database of 4.6 million suspected adverse drug reaction incidents. Recently, similar methodology has been developed to mine large collections of electronic health records for temporal patterns associating drug prescriptions to medical diagnoses.\nData mining has been applied to software artifacts within the realm of software engineering: Mining Software Repositories.\nIn the field of microbiology, data mining methods have been used for predicting population behavior of bacteria in food.\n\n\n== Human rights ==\nData mining of government records \u2013 particularly records of the justice system (i.e., courts, prisons) \u2013 enables the discovery of systemic human rights violations in connection to generation and publication of invalid or fraudulent legal records by various government agencies.\n\n\n== Medical data mining ==\n\nSome machine learning algorithms can be applied in medical field as second-opinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases.\nOne of these classifiers (called Prototype exemplar learning classifier (PEL-C) is able to discover syndromes as well as atypical clinical cases.\nA current medical field that utilizes the process of data mining is Metabolomics, which is the investigation and study of biological molecules and how their interaction with bodily fluids, cells, tissues, etc. is characterized. Metabolomics is a very data heavy subject, and often involves sifting through massive amounts of irrelevant data before finding any conclusions. Data mining has allowed this relatively new field of medical research to grow considerably within the last decade, and will likely be the method of which new research is found within the subject.\nIn 2011, the case of Sorrell v. IMS Health, Inc., decided by the Supreme Court of the United States, ruled that pharmacies may share information with outside companies. This practice was authorized under the 1st Amendment of the Constitution, protecting the \"freedom of speech.\" However, the passage of the Health Information Technology for Economic and Clinical Health Act (HITECH Act) helped to initiate the adoption of the electronic health record (EHR) and supporting technology in the United States. The HITECH Act was signed into law on February 17, 2009 as part of the American Recovery and Reinvestment Act (ARRA) and helped to open the door to medical data mining. Prior to the signing of this law, estimates of only 20% of United States-based physicians were utilizing electronic patient records. S\u00f8ren Brunak notes that \u201cthe patient record becomes as information-rich as possible\u201d and thereby \u201cmaximizes the data mining opportunities.\u201d Hence, electronic patient records further expands the possibilities regarding medical data mining thereby opening the door to a vast source of medical data analysis.\n\n\n== Spatial data mining ==\nSpatial data mining is the application of data mining methods to spatial data. The end objective of spatial data mining is to find patterns in data with respect to geography. So far, data mining and Geographic Information Systems (GIS) have existed as two separate technologies, each with its own methods, traditions, and approaches to visualization and data analysis. Particularly, most contemporary GIS have only very basic spatial analysis functionality. The immense explosion in geographically referenced data occasioned by developments in IT, digital mapping, remote sensing, and the global diffusion of GIS emphasizes the importance of developing data-driven inductive approaches to geographical analysis and modeling.\nData mining offers great potential benefits for GIS-based applied decision-making. Recently, the task of integrating these two technologies has become of critical importance, especially as various public and private sector organizations possessing huge databases with thematic and geographically referenced data begin to realize the huge potential of the information contained therein. Among those organizations are:\n\nOffices requiring analysis or dissemination of geo-referenced statistical data\nPublic health services searching for explanations of disease clustering\nEnvironmental agencies assessing the impact of changing land-use patterns on climate change\nGeo-marketing companies doing customer segmentation based on spatial location.\nChallenges in Spatial mining:\nGeospatial data repositories tend to be very large. Moreover, existing GIS datasets are often splintered into feature and attribute components that are conventionally archived in hybrid data management systems. Algorithmic requirements differ substantially for relational (attribute) data management and for topological (feature) data management. Related to this is the range and diversity of geographic data formats, which present unique challenges. The digital geographic data revolution is creating new types of data formats beyond the traditional \"vector\" and \"raster\" formats. Geographic data repositories increasingly include ill-structured data, such as imagery and geo-referenced multi-media.\nThere are several critical research challenges in geographic knowledge discovery and data mining. Miller and Han offer the following list of emerging research topics in the field:\n\nDeveloping and supporting geographic data warehouses (GDW's): Spatial properties are often reduced to simple aspatial attributes in mainstream data warehouses. Creating an integrated GDW requires solving issues of spatial and temporal data interoperability \u2013 including differences in semantics, referencing systems, geometry, accuracy, and position.\nBetter spatio-temporal representations in geographic knowledge discovery: Current geographic knowledge discovery (GKD) methods generally use very simple representations of geographic objects and spatial relationships. Geographic data mining methods should recognize more complex geographic objects (i.e., lines and polygons) and relationships (i.e., non-Euclidean distances, direction, connectivity, and interaction through attributed geographic space such as terrain). Furthermore, the time dimension needs to be more fully integrated into these geographic representations and relationships.\nGeographic knowledge discovery using diverse data types: GKD methods should be developed that can handle diverse data types beyond the traditional raster and vector models, including imagery and geo-referenced multimedia, as well as dynamic data types (video streams, animation).\n\n\n== Temporal data mining ==\nData may contain attributes generated and recorded at different times. In this case finding meaningful relationships in the data may require considering the temporal order of the attributes. A temporal relationship may indicate a causal relationship, or simply an association.\n\n\n== Sensor data mining ==\nWireless sensor networks can be used for facilitating the collection of data for spatial data mining for a variety of applications such as air pollution monitoring. A characteristic of such networks is that nearby sensor nodes monitoring an environmental feature typically register similar values. This kind of data redundancy due to the spatial correlation between sensor observations inspires the techniques for in-network data aggregation and mining. By measuring the spatial correlation between data sampled by different sensors, a wide class of specialized algorithms can be developed to develop more efficient spatial data mining algorithms.\n\n\n== Visual data mining ==\nIn the process of turning from analog into digital, large data sets have been generated, collected, and stored discovering statistical patterns, trends and information which is hidden in data, in order to build predictive patterns. Studies suggest visual data mining is faster and much more intuitive than is traditional data mining. See also Computer vision.\n\n\n== Music data mining ==\nData mining techniques, and in particular co-occurrence analysis, has been used to discover relevant similarities among music corpora (radio lists, CD databases) for purposes including classifying music into genres in a more objective manner.\n\n\n== Surveillance ==\nData mining has been used by the U.S. government. Programs include the Total Information Awareness (TIA) program, Secure Flight (formerly known as Computer-Assisted Passenger Prescreening System (CAPPS II)), Analysis, Dissemination, Visualization, Insight, Semantic Enhancement (ADVISE), and the Multi-state Anti-Terrorism Information Exchange (MATRIX). These programs have been discontinued due to controversy over whether they violate the 4th Amendment to the United States Constitution, although many programs that were formed under them continue to be funded by different organizations or under different names.\nIn the context of combating terrorism, two particularly plausible methods of data mining are \"pattern mining\" and \"subject-based data mining\".\n\n\n== Pattern mining ==\n\"Pattern mining\" is a data mining method that involves finding existing patterns in data. In this context patterns often means association rules. The original motivation for searching association rules came from the desire to analyze supermarket transaction data, that is, to examine customer behavior in terms of the purchased products. For example, an association rule \"beer \u21d2 potato chips (80%)\" states that four out of five customers that bought beer also bought potato chips.\nIn the context of pattern mining as a tool to identify terrorist activity, the National Research Council provides the following definition: \"Pattern-based data mining looks for patterns (including anomalous data patterns) that might be associated with terrorist activity \u2014 these patterns might be regarded as small signals in a large ocean of noise.\" Pattern Mining includes new areas such a Music Information Retrieval (MIR) where patterns seen both in the temporal and non temporal domains are imported to classical knowledge discovery search methods.\n\n\n== Subject-based data mining ==\n\"Subject-based data mining\" is a data mining method involving the search for associations between individuals in data. In the context of combating terrorism, the National Research Council provides the following definition: \"Subject-based data mining uses an initiating individual or other datum that is considered, based on other information, to be of high interest, and the goal is to determine what other persons or financial transactions or movements, etc., are related to that initiating datum.\"\n\n\n== Knowledge grid ==\nKnowledge discovery \"On the Grid\" generally refers to conducting knowledge discovery in an open environment using grid computing concepts, allowing users to integrate data from various online data sources, as well make use of remote resources, for executing their data mining tasks. The earliest example was the Discovery Net, developed at Imperial College London, which won the \"Most Innovative Data-Intensive Application Award\" at the ACM SC02 (Supercomputing 2002) conference and exhibition, based on a demonstration of a fully interactive distributed knowledge discovery application for a bioinformatics application. Other examples include work conducted by researchers at the University of Calabria, who developed a Knowledge Grid architecture for distributed knowledge discovery, based on grid computing.\n\n\n== References ==\n\n\n== External links ==\nWikipedia:Data mining Wikipedia", "link": "https://en.wikipedia.org/wiki/Examples_of_data_mining"}, "Educational data mining": {"title": "Educational data mining", "content": "Educational data mining (EDM) is a research field concerned with the application of data mining, machine learning and statistics to information generated from educational settings (e.g., universities and intelligent tutoring systems). At a high level, the field seeks to develop and improve methods for exploring this data, which often has multiple levels of meaningful hierarchy, in order to discover new insights about how people learn in the context of such settings. In doing so, EDM has contributed to theories of learning investigated by researchers in educational psychology and the learning sciences. The field is closely tied to that of learning analytics, and the two have been compared and contrasted.\n\n\n== Definition ==\nEducational data mining refers to techniques, tools, and research designed for automatically extracting meaning from large repositories of data generated by or related to people's learning activities in educational settings. Quite often, this data is extensive, fine-grained, and precise. For example, several learning management systems (LMSs) track information such as when each student accessed each learning object, how many times they accessed it, and how many minutes the learning object was displayed on the user's computer screen. As another example, intelligent tutoring systems record data every time a learner submits a solution to a problem. They may collect the time of the submission, whether or not the solution matches the expected solution, the amount of time that has passed since the last submission, the order in which solution components were entered into the interface, etc. The precision of this data is such that even a fairly short session with a computer-based learning environment (e.g. 30 minutes) may produce a large amount of process data for analysis.\nIn other cases, the data is less fine-grained. For example, a student's university transcript may contain a temporally ordered list of courses taken by the student, the grade that the student earned in each course, and when the student selected or changed his or her academic major. EDM leverages both types of data to discover meaningful information about different types of learners and how they learn, the structure of domain knowledge, and the effect of instructional strategies embedded within various learning environments. These analyses provide new information that would be difficult to discern by looking at the raw data. For example, analyzing data from an LMS may reveal a relationship between the learning objects that a student accessed during the course and their final course grade. Similarly, analyzing student transcript data may reveal a relationship between a student's grade in a particular course and their decision to change their academic major. Such information provides insight into the design of learning environments, which allows students, teachers, school administrators, and educational policy makers to make informed decisions about how to interact with, provide, and manage educational resources.\n\n\n== History ==\nWhile the analysis of educational data is not itself a new practice, recent advances in educational technology, including the increase in computing power and the ability to log fine-grained data about students' use of a computer-based learning environment, have led to an increased interest in developing techniques for analyzing the large amounts of data generated in educational settings. This interest translated into a series of EDM workshops held from 2000 to 2007 as part of several international research conferences. In 2008, a group of researchers established what has become an annual international research conference on EDM, the first of which took place in Montreal, Quebec, Canada.\nAs interest in EDM continued to increase, EDM researchers established an academic journal in 2009, the Journal of Educational Data Mining, for sharing and disseminating research results. In 2011, EDM researchers established the International Educational Data Mining Society to connect EDM researchers and continue to grow the field.\nWith the introduction of public educational data repositories in 2008, such as the Pittsburgh Science of Learning Centre's (PSLC) DataShop and the National Center for Education Statistics (NCES), public data sets have made educational data mining more accessible and feasible, contributing to its growth.\n\n\n== Goals ==\nRyan S. Baker and Kalina Yacef  identified the following four goals of EDM:\n\nPredicting students' future learning behavior \u2013 With the use of student modeling, this goal can be achieved by creating student models that incorporate the learner's characteristics, including detailed information such as their knowledge, behaviours and motivation to learn. The user experience of the learner and their overall satisfaction with learning are also measured.\nDiscovering or improving domain models \u2013 Through the various methods and applications of EDM, discovery of new and improvements to existing models is possible. Examples include illustrating the educational content to engage learners and determining optimal instructional sequences to support the student's learning style.\nStudying the effects of educational support that can be achieved through learning systems.\nAdvancing scientific knowledge about learning and learners by building and incorporating student models, the field of EDM research and the technology and software used.\n\n\n== Users and stakeholders ==\nThere are four main users and stakeholders involved with educational data mining. These include: \n\nLearners \u2013 Learners are interested in understanding student needs and methods to improve the learner's experience and performance. For example, learners can also benefit from the discovered knowledge by using the EDM tools to suggest activities and resources that they can use based on their interactions with the online learning tool and insights from past or similar learners. For younger learners, educational data mining can also inform parents about their child's learning progress. It is also necessary to effectively group learners in an online environment. The challenge is using the complex data to learn and interpret these groups through developing actionable models.\nEducators \u2013 Educators attempt to understand the learning process and the methods they can use to improve their teaching methods. Educators can use the applications of EDM to determine how to organize and structure the curriculum, the best methods to deliver course information and the tools to use to engage their learners for optimal learning outcomes. In particular, the distillation of data for human judgment technique provides an opportunity for educators to benefit from EDM because it enables educators to quickly identify behavioural patterns, which can support their teaching methods during the duration of the course or to improve future courses. Educators can determine indicators that show student satisfaction and engagement of course material, and also monitor learning progress.\nResearchers \u2013 Researchers focus on the development and the evaluation of data mining techniques for effectiveness. A yearly international conference for researchers began in 2008. The wide range of topics in EDM ranges from using data mining to improve institutional effectiveness to student performance.\nAdministrators \u2013 Administrators are responsible for allocating the resources for implementation in institutions. As institutions are increasingly held responsible for student success, the administering of EDM applications are becoming more common in educational settings. Faculty and advisors are becoming more proactive in identifying and addressing at-risk students. However, it is sometimes a challenge to get the information to the decision makers to administer the application in a timely and efficient manner.\n\n\n== Phases ==\nAs research in the field of educational data mining has continued to grow, a myriad of data mining techniques have been applied to a variety of educational contexts. In each case, the goal is to translate raw data into meaningful information about the learning process in order to make better decisions about the design and trajectory of a learning environment. Thus, EDM generally consists of four phases:\n\nThe first phase of the EDM process (not counting pre-processing) is discovering relationships in data. This involves searching through a repository of data from an educational environment with the goal of finding consistent relationships between variables. Several algorithms for identifying such relationships have been utilized, including classification, regression, clustering, factor analysis, social network analysis, association rule mining, and sequential pattern mining.\nDiscovered relationships must then be validated in order to avoid overfitting.\nValidated relationships are applied to make predictions about future events in the learning environment.\nPredictions are used to support decision-making processes and policy decisions.\nDuring phases 3 and 4, data is often visualized or in some other way distilled for human judgment. A large amount of research has been conducted in best practices for visualizing data.\n\n\n== Main approaches ==\nOf the general categories of methods mentioned, prediction, clustering and relationship mining are considered universal methods across all types of data mining; however, Discovery with Models and Distillation of Data for Human Judgment are considered more prominent approaches within educational data mining.\n\n\n=== Discovery with models ===\nIn the Discovery with Model method, a model is developed via prediction, clustering or by human reasoning knowledge engineering and then used as a component in another analysis, namely in prediction and relationship mining. In the prediction method use, the created model's predictions are used to predict a new variable. For the use of relationship mining, the created model enables the analysis between new predictions and additional variables in the study. In many cases, discovery with models uses validated prediction models that have proven generalizability across contexts.\nKey applications of this method include discovering relationships between student behaviors, characteristics and contextual variables in the learning environment. Further discovery of broad and specific research questions across a wide range of contexts can also be explored using this method.\n\n\n=== Distillation of data for human judgment ===\nHumans can make inferences about data that may be beyond the scope in which an automated data mining method provides. For the use of education data mining, data is distilled for human judgment for two key purposes, identification and classification.\nFor the purpose of identification, data is distilled to enable humans to identify well-known patterns, which may otherwise be difficult to interpret. For example, the learning curve, classic to educational studies, is a pattern that clearly reflects the relationship between learning and experience over time.\nData is also distilled for the purposes of classifying features of data, which for educational data mining, is used to support the development of the prediction model. Classification helps expedite the development of the prediction model, tremendously.\nThe goal of this method is to summarize and present the information in a useful, interactive and visually appealing way in order to understand the large amounts of education data and to support decision making. In particular, this method is beneficial to educators in understanding usage information and effectiveness in course activities. Key applications for the distillation of data for human judgment include identifying patterns in student learning, behavior, opportunities for collaboration and labeling data for future uses in prediction models.\n\n\n== Applications ==\nA list of the primary applications of EDM is provided by Cristobal Romero and Sebastian Ventura. In their taxonomy, the areas of EDM application are:\n\nAnalysis and visualization of data\nProviding feedback for supporting instructors\nRecommendations for students\nPredicting student performance\nStudent modeling\nDetecting undesirable student behaviors\nGrouping students\nSocial network analysis\nDeveloping concept maps\nConstructing courseware \u2013 EDM can be applied to course management systems such as open source Moodle. Moodle contains usage data that includes various activities by users such as test results, amount of readings completed and participation in discussion forums. Data mining tools can be used to customize learning activities for each user and adapt the pace in which the student completes the course. This is in particularly beneficial for online courses with varying levels of competency.\nPlanning and scheduling\nNew research on mobile learning environments also suggests that data mining can be useful. Data mining can be used to help provide personalized content to mobile users, despite the differences in managing content between mobile devices and standard PCs and web browsers.\nNew EDM applications will focus on allowing non-technical users use and engage in data mining tools and activities, making data collection and processing more accessible for all users of EDM. Examples include statistical and visualization tools that analyzes social networks and their influence on learning outcomes and productivity.\n\n\n== Courses ==\nIn October 2013, Coursera offered a free online course on \"Big Data in Education\" that taught how and when to use key methods for EDM. This course moved to edX in the summer of 2015, and has continued to run on edX annually since then. A course archive is now available online.\nTeachers College, Columbia University offers a MS in Learning Analytics.\n\n\n== Publication venues ==\nConsiderable amounts of EDM work are published at the peer-reviewed International Conference on Educational Data Mining, organized by the International Educational Data Mining Society.\n\n1st International Conference on Educational Data Mining (2008) \u2013 Montreal, Canada\n2nd International Conference on Educational Data Mining (2009) \u2013 Cordoba, Spain\n3rd International Conference on Educational Data Mining (2010) \u2013 Pittsburgh, PA, USA\n4th International Conference on Educational Data Mining (2011) \u2013 Eindhoven, Netherlands\n5th International Conference on Educational Data Mining (2012) \u2013 Chania, Greece\n6th International Conference on Educational Data Mining (2013) \u2013 Memphis, TN, USA\n7th International Conference on Educational Data Mining (2014) \u2013 London, UK\n8th International Conference on Educational Data Mining] (2015) \u2013 Madrid, Spain\n9th International Conference on Educational Data Mining] (2016) \u2013 Raleigh, NC, USA\n10th International Conference on Educational Data Mining] (2017) \u2013 Wuhan, China\n11th International Conference on Educational Data Mining] (2018) \u2013 Buffalo, NY, USA\n12th International Conference on Educational Data Mining] (2019) \u2013 Montr\u00e9al, QC, Canada\n13th International Conference on Educational Data Mining] (2020) \u2013 Virtual\n14th International Conference on Educational Data Mining (2021) \u2013 Paris, France\nEDM papers are also published in the Journal of Educational Data Mining (JEDM).\nMany EDM papers are routinely published in related conferences, such as Artificial Intelligence and Education, Intelligent Tutoring Systems, and User Modeling, Adaptation, and Personalization.\nIn 2011, Chapman & Hall/CRC Press, Taylor and Francis Group published the first Handbook of Educational Data Mining. This resource was created for those that are interested in participating in the educational data mining community.\n\n\n== Contests ==\nIn 2010, the Association for Computing Machinery's KDD Cup was conducted using data from an educational setting. The data set was provided by the DataShop, and it consisted of over 1,000,000 data points from students using a cognitive tutor. Six hundred teams competed for over US$8,000 in prize money (which was donated by Facebook). The goal for contestants was to design an algorithm that, after learning from the provided data, would make the most accurate predictions from new data. The winners submitted an algorithm that utilized feature generation (a form of representation learning), random forests, and Bayesian networks.\n\n\n== Costs and challenges ==\nAlong with technological advancements are costs and challenges associated with implementing EDM applications. These include the costs to store logged data and the cost associated with hiring staff dedicated to managing data systems. Moreover, data systems may not always integrate seamlessly with one another and even with the support of statistical and visualization tools, creating one simplified version of the data can be difficult. Furthermore, choosing which data to mine and analyze can also be challenging, making the initial stages very time-consuming and labor-intensive. From beginning to end, the EDM strategy and implementation requires one to uphold privacy and ethics for all stakeholders involved.\n\n\n== Criticisms ==\nGeneralizability \u2013 Research in EDM may be specific to the particular educational setting and time in which the research was conducted, and as such, may not be generalizable to other institutions. Research also indicates that the field of educational data mining is concentrated in western countries and cultures and subsequently, other countries and cultures may not be represented in the research and findings. Development of future models should consider applications across multiple contexts.\nPrivacy \u2013 Individual privacy is a continued concern for the application of data mining tools. With free, accessible and user-friendly tools in the market, students and their families may be at risk from the information that learners provide to the learning system, in hopes to receive feedback that will benefit their future performance. As users become savvy in their understanding of online privacy, administrators of educational data mining tools need to be proactive in protecting the privacy of their users and be transparent about how and with whom the information will be used and shared. Development of EDM tools should consider protecting individual privacy while still advancing the research in this field.\nPlagiarism \u2013 Plagiarism detection is an ongoing challenge for educators and faculty whether in the classroom or online. However, due to the complexities associated with detecting and preventing digital plagiarism in particular, educational data mining tools are not currently sophisticated enough to accurately address this issue. Thus, the development of predictive capability in plagiarism-related issues should be an area of focus in future research.\nAdoption \u2013 It is unknown how widespread the adoption of EDM is and the extent to which institutions have applied and considered implementing an EDM strategy. As such, it is unclear whether there are any barriers that prevent users from adopting EDM in their educational settings.\n\n\n== See also ==\nBig data\nData mining\nEducation\nEducational technology\nGlossary of education terms\nLearning analytics\nMachine learning\nStatistics\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Educational_data_mining"}, "Lift (data mining)": {"title": "Lift (data mining)", "content": "In data mining and association rule learning, lift is a measure of the performance of a targeting model (association rule) at predicting or classifying cases as having an enhanced response (with respect to the population as a whole), measured against a random choice targeting model. A targeting model is doing a good job if the response within the target (\n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n) is much better than the baseline (\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n) average for the population as a whole. Lift is simply the ratio of these values: target response divided by average response. Mathematically,\n\n  \n    \n      \n        lift\n        =\n        \n          \n            \n              P\n              (\n              T\n              \u2223\n              B\n              )\n            \n            \n              P\n              (\n              T\n              )\n            \n          \n        \n        =\n        \n          \n            \n              P\n              (\n              T\n              \u2227\n              B\n              )\n            \n            \n              P\n              (\n              T\n              )\n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\operatorname {lift} ={\\frac {P(T\\mid B)}{P(T)}}={\\frac {P(T\\wedge B)}{P(T)P(B)}}}\n  \n\nFor example, suppose a population has an average response rate of 5%, but a certain model (or rule) has identified a segment with a response rate of 20%. Then that segment would have a lift of 4.0 (20%/5%).\n\n\n== Applications ==\nTypically, the modeller seeks to divide the population into quantiles, and rank the quantiles by lift. Organizations can then consider each quantile, and by weighing the predicted response rate (and associated financial benefit) against the cost, they can decide whether to market to that quantile or not.\nThe lift curve can also be considered a variation on the receiver operating characteristic (ROC) curve, and is also known in econometrics as the Lorenz or power curve.\n\n\n== Example ==\nAssume the data set being mined is:\n\nwhere the antecedent is the input variable that we can control, and the consequent is the variable we are trying to predict. Real mining problems would typically have more complex antecedents, but usually focus on single-value consequents.\nMost mining algorithms would determine the following rules (targeting models):\n\nRule 1: A implies 0\nRule 2: B implies 1\nbecause these are simply the most common patterns found in the data. A simple review of the above table should make these rules obvious.\nThe support for Rule 1 is 3/7 because that is the number of items in the dataset in which the antecedent is A and the consequent 0. The support for Rule 2 is 2/7 because two of the seven records meet the antecedent of B and the consequent of 1. The supports can be written as:\n\n  \n    \n      \n        supp\n        \u2061\n        (\n        A\n        \u21d2\n        0\n        )\n        =\n        P\n        (\n        A\n        \u2227\n        0\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        0\n        \u2223\n        A\n        )\n        =\n        P\n        (\n        0\n        )\n        P\n        (\n        A\n        \u2223\n        0\n        )\n      \n    \n    {\\displaystyle \\operatorname {supp} (A\\Rightarrow 0)=P(A\\land 0)=P(A)P(0\\mid A)=P(0)P(A\\mid 0)}\n  \n\n  \n    \n      \n        supp\n        \u2061\n        (\n        B\n        \u21d2\n        1\n        )\n        =\n        P\n        (\n        B\n        \u2227\n        1\n        )\n        =\n        P\n        (\n        B\n        )\n        P\n        (\n        1\n        \u2223\n        B\n        )\n        =\n        P\n        (\n        1\n        )\n        P\n        (\n        B\n        \u2223\n        1\n        )\n      \n    \n    {\\displaystyle \\operatorname {supp} (B\\Rightarrow 1)=P(B\\land 1)=P(B)P(1\\mid B)=P(1)P(B\\mid 1)}\n  \n\nThe confidence for Rule 1 is 3/4 because three of the four records that meet the antecedent of A meet the consequent of 0. The confidence for Rule 2 is 2/3 because two of the three records that meet the antecedent of B meet the consequent of 1. The confidences can be written as:\n\n  \n    \n      \n        conf\n        \u2061\n        (\n        A\n        \u21d2\n        0\n        )\n        =\n        P\n        (\n        0\n        \u2223\n        A\n        )\n      \n    \n    {\\displaystyle \\operatorname {conf} (A\\Rightarrow 0)=P(0\\mid A)}\n  \n\n  \n    \n      \n        conf\n        \u2061\n        (\n        B\n        \u21d2\n        1\n        )\n        =\n        P\n        (\n        1\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle \\operatorname {conf} (B\\Rightarrow 1)=P(1\\mid B)}\n  \n\nLift can be found by dividing the confidence by the unconditional probability of the consequent, or by dividing the support by the probability of the antecedent times the probability of the consequent, so:\n\nThe lift for Rule 1 is (3/4)/(4/7) = (3*7)/(4 * 4) = 21/16 \u2248 1.31\nThe lift for Rule 2 is (2/3)/(3/7) = (2*7)/(3 * 3) = 14/9 \u2248 1.56\n\n  \n    \n      \n        lift\n        \u2061\n        (\n        A\n        \u21d2\n        0\n        )\n        =\n        \n          \n            \n              P\n              (\n              0\n              \u2223\n              A\n              )\n            \n            \n              P\n              (\n              0\n              )\n            \n          \n        \n        =\n        \n          \n            \n              P\n              (\n              A\n              \u2227\n              0\n              )\n            \n            \n              P\n              (\n              A\n              )\n              P\n              (\n              0\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\operatorname {lift} (A\\Rightarrow 0)={\\frac {P(0\\mid A)}{P(0)}}={\\frac {P(A\\land 0)}{P(A)P(0)}}}\n  \n\n  \n    \n      \n        lift\n        \u2061\n        (\n        B\n        \u21d2\n        1\n        )\n        =\n        \n          \n            \n              P\n              (\n              1\n              \u2223\n              B\n              )\n            \n            \n              P\n              (\n              1\n              )\n            \n          \n        \n        =\n        \n          \n            \n              P\n              (\n              B\n              \u2227\n              1\n              )\n            \n            \n              P\n              (\n              B\n              )\n              P\n              (\n              1\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\operatorname {lift} (B\\Rightarrow 1)={\\frac {P(1\\mid B)}{P(1)}}={\\frac {P(B\\land 1)}{P(B)P(1)}}}\n  \n\nIf some rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.\nIf the lift is > 1, like it is here for Rules 1 and 2, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.\nObserve that even though Rule 1 has higher confidence, it has lower lift. Intuitively, it would seem that Rule 1 is more valuable because of its higher confidence\u2014it seems more accurate (better supported). But accuracy of the rule independent of the data set can be misleading. The value of lift is that it considers both the confidence of the rule and the overall data set.\n\n\n== References ==\n\nCoppock, David S. (2002-06-21). \"Why Lift?\". Retrieved 2015-07-05.\n\n\n== See also ==\nCorrelation and dependence\nUplift modelling", "link": "https://en.wikipedia.org/wiki/Lift_(data_mining)"}, "Java Data Mining": {"title": "Java Data Mining", "content": "Java Data Mining (JDM) is a standard Java API for developing data mining applications and tools. JDM defines an object model and Java API for data mining objects and processes. JDM enables applications to integrate data mining technology for developing predictive analytics applications and tools.  The JDM 1.0 standard was developed under the Java Community Process as JSR 73. In 2006, the JDM 2.0 specification was being developed under JSR 247, but has been withdrawn in 2011 without standardization.\nVarious data mining functions and techniques like statistical classification and association, regression analysis, data clustering, and attribute importance are covered by the 1.0 release of this standard.\nIt never received wide acceptance, and there is no known implementation.\n\n\n== See also ==\nPredictive Model Markup Language\n\n\n== Books ==\nJava Data Mining: Strategy, Standard, and Practice, Hornick, Marcad\u00e9, Venkayala, ISBN 0-12-370452-9\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Java_Data_Mining"}, "Data stream mining": {"title": "Data stream mining", "content": "Data Stream Mining (also known as stream learning) is the process of extracting knowledge structures from continuous, rapid data records. A data stream is an ordered sequence of instances that in many applications of data stream mining can be read only once or a small number of times using limited computing and storage capabilities.\nIn many data stream mining applications, the goal is to predict the class or value of new instances in the data stream given some knowledge about the class membership or values of previous instances in the data stream.\nMachine learning techniques can be used to learn this prediction task from labeled examples in an automated fashion.\nOften, concepts from the field of incremental learning are applied to cope with structural changes, on-line learning and real-time demands. \nIn many applications, especially operating within non-stationary environments, the distribution underlying the instances or the rules underlying their labeling may change over time, i.e. the goal of the prediction, the class to be predicted or the target value to be predicted, may change over time. This problem is referred to as concept drift. Detecting concept drift is a central issue to data stream mining. Other challenges that arise when applying machine learning to streaming data include: partially and delayed labeled data, recovery from concept drifts, and temporal dependencies.\nExamples of data streams include computer network traffic, phone conversations, ATM transactions, web searches, and sensor data.\nData stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery.\n\n\n== Software for data stream mining ==\nMOA (Massive Online Analysis): free open-source software specific for mining data streams with concept drift developed in Java. It has several machine learning algorithms (classification, regression, clustering, outlier detection and recommender systems).  Also, it contains a prequential evaluation method, the EDDM concept drift methods, a reader of ARFF real datasets, and artificial stream generators as SEA concepts, STAGGER, rotating hyperplane, random tree, and random radius based functions. MOA supports bi-directional interaction with Weka (machine learning).\nscikit-multiflow: A machine learning framework for multi-output/multi-label and stream data implemented in Python. scikit-multiflow contains stream generators, stream learning methods for single-target and multi-target, concept drift detectors, evaluation and visualisation methods. (This software is discontinued)\nStreamDM: StreamDM is an open source framework for big data stream mining that uses the Spark Streaming extension of the core Spark API. One advantage of StreamDM in comparison to existing frameworks is that it directly benefits from the Spark Streaming API, which handles much of the complex problems of the underlying data sources, such as out of order data and recovery from failures.\nRapidMiner: commercial software for knowledge discovery, data mining, and machine learning also featuring data stream mining, learning time-varying concepts, and tracking drifting concept (if used in combination with its data stream mining plugin (formerly: Concept Drift plugin))\nRiverML: River is a Python library for online machine learning. It is the result of a merger between creme and scikit-multiflow. River's ambition is to be the go-to library for doing machine learning on streaming data.\nGAENARI: C++ incremental decision tree. It continuously executes inserts and updates of chunked data sets. Rebuild support for concept drift issues.\n\n\n== Events ==\nInternational Workshop on Ubiquitous Data Mining Archived 2013-02-23 at the Wayback Machine held in conjunction with the International Joint Conference on Artificial Intelligence (IJCAI)  in Beijing, China, August 3\u20135, 2013.\nInternational Workshop on Knowledge Discovery from Ubiquitous Data Streams Archived 2012-02-16 at the Wayback Machine held in conjunction with the 18th European Conference on Machine Learning (ECML) and the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD) in Warsaw, Poland, in September 2007.\nACM Symposium on Applied Computing Data Streams Track held in conjunction with the 2007 ACM Symposium on Applied Computing (SAC-2007) in Seoul, Korea, in March 2007.\nIEEE International Workshop on Mining Evolving and Streaming Data (IWMESD 2006) to be held in conjunction with the 2006 IEEE International Conference on Data Mining (ICDM-2006) in Hong Kong in December 2006.\nFourth International Workshop on Knowledge Discovery from Data Streams (IWKDDS) to be held in conjunction with the 17th European Conference on Machine Learning (ECML) and the 10th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD) (ECML/PKDD-2006) in Berlin, Germany, in September 2006.\n\n\n== See also ==\nConcept drift\nData Mining\nSequence mining\nStreaming algorithm\nStream processing\nWireless sensor network\nLambda architecture\n\n\n== Books ==\nBifet, Albert; Gavald\u00e0, Ricard; Holmes, Geoff; Pfahringer, Bernhard (2018). Machine Learning for Data Streams with Practical Examples in MOA. Adaptive Computation and Machine Learning. MIT Press. p. 288. ISBN 9780262037792.\nGama, Jo\u00e3o; Gaber, Mohamed Medhat, eds. (2007). Learning from Data Streams: Processing Techniques in Sensor Networks. Springer. p. 244. doi:10.1007/3-540-73679-4. ISBN 9783540736783.\nGanguly, Auroop R.; Gama, Jo\u00e3o; Omitaomu, Olufemi A.; Gaber, Mohamed M.; Vatsavai, Ranga R., eds. (2008). Knowledge Discovery from Sensor Data. Industrial Innovation. CRC Press. p. 215. ISBN 9781420082326.\nGama, Jo\u00e3o (2010). Knowledge Discovery from Data Streams. Data Mining and Knowledge Discovery. Chapman and Hall. p. 255. ISBN 9781439826119.\nLughofer, Edwin (2011). Evolving Fuzzy Systems - Methodologies, Advanced Concepts and Applications. Studies in Fuzziness and Soft Computing. Vol. 266. Heidelberg: Springer. p. 456. doi:10.1007/978-3-642-18087-3. ISBN 9783642180866.\nSayed-Mouchaweh, Moamar; Lughofer, Edwin, eds. (2012). Learning in Non-Stationary Environments: Methods and Applications. New York: Springer. p. 440. CiteSeerX 10.1.1.709.437. doi:10.1007/978-1-4419-8020-5. ISBN 9781441980199.\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Data_stream_mining"}, "Text mining": {"title": "Text mining", "content": "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005), there are three perspectives of text mining: information extraction, data mining, and knowledge discovery in databases (KDD). Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\nText analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via the application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.\nA typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted. The document is the basic element when starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.\n\n\n== Text analytics ==\n\nText analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation. The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\" in 2004 to describe \"text analytics\". The latter term is now used more frequently in business settings while \"text mining\" is used in some of the earliest application areas, dating to the 1980s, notably life-sciences research and government intelligence.\nThe term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80% of business-relevant information originates in unstructured form, primarily text. These techniques and processes discover and present knowledge \u2013 facts, business rules, and relationships \u2013 that is otherwise locked in textual form, impenetrable to automated processing.\n\n\n== Text analysis processes ==\nSubtasks\u2014components of a larger text-analytics effort\u2014typically include:\n\nDimensionality reduction is important technique for pre-processing data. It is used to identify the root word for actual words and reduce the size of the text data.\nInformation retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.\nAlthough some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.\nNamed entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on.\nDisambiguation\u2014the use of contextual clues\u2014may be required to decide where, for instance, \"Ford\" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.\nRecognition of pattern-identified entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.\nDocument clustering: identification of sets of similar text documents.\nCoreference resolution: identification of noun phrases and other terms that refer to the same object.\nExtraction of relationships, facts and events: identification of associations among entities and other information in texts.\nSentiment analysis: discerning of subjective material and extracting information about attitudes: sentiment, opinion, mood, and emotion. This is done at the entity, concept, or topic level and aims to distinguish opinion holders and objects.\nQuantitative text analysis: a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.\nPre-processing usually involves tasks such as tokenization, filtering and stemming.\n\n\n== Applications ==\nText mining technology is now broadly applied to a wide variety of government, research, and business needs. All these groups may use text mining for records management and searching documents relevant to their daily activities. Legal professionals may use text mining for e-discovery, for example. Governments and military groups use text mining for national security and intelligence purposes. Scientific researchers incorporate text mining approaches into efforts to organize large sets of text data (i.e., addressing the problem of unstructured data), to determine ideas communicated through text (e.g., sentiment analysis in social media) and to support scientific discovery in fields such as the life sciences and bioinformatics. In business, applications are used to support competitive intelligence and automated ad placement, among numerous other activities.\n\n\n=== Security applications ===\nMany text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes. It is also involved in the study of text encryption/decryption.\n\n\n=== Biomedical applications ===\n\nA range of text mining applications in the biomedical literature has been described, including computational approaches to assist with studies in protein docking, protein interactions, and protein-disease associations. In addition, with large patient textual datasets in the clinical field, datasets of demographic information in population studies and adverse event reports, text mining can facilitate clinical studies and precision medicine. Text mining algorithms can facilitate the stratification and indexing of specific clinical events in large patient textual datasets of symptoms, side effects, and comorbidities from electronic health records, event reports, and reports from specific diagnostic tests. One online text mining application in the biomedical literature is PubGene, a publicly accessible search engine that combines biomedical text mining with network visualization. GoPubMed is a knowledge-based search engine for biomedical texts. Text mining techniques also enable us to extract unknown knowledge from unstructured documents in the clinical domain\n\n\n=== Software applications ===\nText mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. Within the public sector, much effort has been concentrated on creating software for tracking and monitoring terrorist activities. For study purposes, Weka software is one of the most popular options in the scientific world, acting as an excellent entry point for beginners. For Python programmers, there is an excellent toolkit called NLTK for more general purposes. For more advanced programmers, there's also the Gensim library, which focuses on word embedding-based text representations.\n\n\n=== Online media applications ===\nText mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site \"stickiness\" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.\n\n\n=== Business and marketing applications ===\nText analytics is being used in business, particularly, in marketing, such as in customer relationship management. Coussement and Van den Poel (2008) apply it to improve predictive analytics models for customer churn (customer attrition). Text mining is also being applied in stock returns prediction.\n\n\n=== Sentiment analysis ===\nSentiment analysis may involve analysis of products such as movies, books, or hotel reviews for estimating how favorable a review is for the product.\nSuch an analysis may need a labeled data set or labeling of the affectivity of words.\nResources for affectivity of words and concepts have been made for WordNet and ConceptNet, respectively.\nText has been used to detect emotions in the related area of affective computing. Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.\n\n\n=== Scientific literature mining and academic applications ===\nThe issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within the written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within the text without removing publisher barriers to public access.\nAcademic institutions have also become involved in the text mining initiative:\n\nThe National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. NaCTeM is operated by the University of Manchester in close collaboration with the Tsujii Lab, University of Tokyo. NaCTeM provides customised tools, research facilities and offers advice to the academic community. They are funded by the Joint Information Systems Committee (JISC) and two of the UK research councils (EPSRC & BBSRC). With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.\nIn the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.\nThe Text Analysis Portal for Research (TAPoR), currently housed at the University of Alberta, is a scholarly project to catalogue text analysis applications and create a gateway for researchers new to the practice.\n\n\n==== Methods for scientific literature mining ====\nComputational methods have been developed to assist with information retrieval from scientific literature. Published approaches include methods for searching, determining novelty, and clarifying homonyms among technical reports.\n\n\n=== Digital humanities and computational sociology ===\nThe automatic analysis of vast textual corpora has created the possibility for scholars to analyze\nmillions of documents in multiple languages with very limited manual intervention. Key enabling technologies have been parsing, machine translation, topic categorization, and machine learning.\n\nThe automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data. The resulting networks, which can contain thousands of nodes, are then analyzed by using tools from network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes. This automates the approach introduced by quantitative narrative analysis, whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.\nContent analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a \"big data\" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. The analysis of readability, gender bias and topic bias was demonstrated in Flaounas et al. showing how different topics have different gender biases and levels of readability; the possibility to detect mood patterns in a vast population by analyzing Twitter content was demonstrated as well.\n\n\n== Software ==\n\nText mining computer programs are available from many commercial and open source companies and sources.\n\n\n== Intellectual property law ==\n\n\n=== Situation in Europe ===\n\nUnder European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is illegal. In the UK in 2014, on the recommendation of the Hargreaves review, the government amended copyright law to allow text mining as a limitation and exception. It was the second country in the world to do so, following Japan, which introduced a mining-specific exception in 2009. However, owing to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions.\nThe European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licenses for Europe. The fact that the focus on the solution to this legal issue was licenses, and not limitations and exceptions to copyright law, led representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.\n\n\n=== Situation in the United States ===\nUS copyright law, and in particular its fair use provisions, means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea, is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed\u2014one such use being text and data mining.\n\n\n=== Situation in Australia ===\nThere is no exception in copyright law of Australia for text or data mining within the Copyright Act 1968. The Australian Law Reform Commission has noted that it is unlikely that the \"research and study\" fair dealing exception would extend to cover such a topic either, given it would be beyond the \"reasonable portion\" requirement.\n\n\n== Implications ==\nUntil recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events. For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence. In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Sources ===\nAnaniadou, S. and McNaught, J. (Editors) (2006). Text Mining for Biology and Biomedicine. Artech House Books. ISBN 978-1-58053-984-5\nBilisoly, R. (2008). Practical Text Mining with Perl. New York: John Wiley & Sons. ISBN 978-0-470-17643-6\nFeldman, R., and Sanger, J. (2006). The Text Mining Handbook. New York: Cambridge University Press. ISBN 978-0-521-83657-9\nHotho, A., N\u00fcrnberger, A. and Paa\u00df, G. (2005). \"A brief survey of text mining\". In Ldv Forum, Vol. 20(1), p. 19-62\nIndurkhya, N., and Damerau, F. (2010). Handbook of Natural Language Processing, 2nd Edition. Boca Raton, FL: CRC Press. ISBN 978-1-4200-8592-1\nKao, A., and Poteet, S. (Editors). Natural Language Processing and Text Mining. Springer. ISBN 1-84628-175-X\nKonchady, M. Text Mining Application Programming (Programming Series). Charles River Media. ISBN 1-58450-460-9\nManning, C., and Schutze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press. ISBN 978-0-262-13360-9\nMiner, G., Elder, J., Hill. T, Nisbet, R., Delen, D. and Fast, A. (2012). Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications. Elsevier Academic Press. ISBN 978-0-12-386979-1\nMcKnight, W. (2005). \"Building business intelligence: Text data mining in business intelligence\". DM Review, 21\u201322.\nSrivastava, A., and Sahami. M. (2009). Text Mining: Classification, Clustering, and Applications. Boca Raton, FL: CRC Press. ISBN 978-1-4200-5940-3\nZanasi, A. (Editor) (2007). Text Mining and its Applications to Intelligence, CRM and Knowledge Management. WIT Press. ISBN 978-1-84564-131-3\n\n\n== External links ==\nMarti Hearst: What Is Text Mining? (October 2003)\nAutomatic Content Extraction, Linguistic Data Consortium Archived 2013-09-25 at the Wayback Machine\nAutomatic Content Extraction, NIST", "link": "https://en.wikipedia.org/wiki/Text_mining"}, "Relational data mining": {"title": "Relational data mining", "content": "Relational data mining is the data mining technique for relational\ndatabases. Unlike traditional data mining algorithms, which look for\npatterns in a single table (propositional patterns), \nrelational data mining algorithms look for patterns among multiple tables\n(relational patterns). For most types of propositional\npatterns, there are corresponding relational patterns. For example,\nthere are relational classification rules (relational classification), relational regression tree, and relational association rules.\nThere are several approaches to relational data mining:\n\nInductive Logic Programming (ILP)\nStatistical Relational Learning (SRL)\nGraph Mining\nPropositionalization\nMulti-view learning\n\n\n== Algorithms ==\nMulti-Relation Association Rules: Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: \u201cThose who live in a place which is near by a city with humid climate type and also are younger than 20 -> their health condition is good\u201d. Such association rules are extractable from RDBMS data or semantic web data.\n\n\n== Software ==\nSafarii: a Data Mining environment for analysing large relational databases based on a multi-relational data mining engine.\nDataconda: a software, free for research and teaching purposes, that helps mining relational databases without the use of SQL.\n\n\n== Datasets ==\nRelational dataset repository: a collection of publicly available relational datasets.\n\n\n== See also ==\nData mining\nStructure mining\nDatabase mining\n\n\n== References ==\n\n\n== External links ==\nWeb page for a text book on relational data mining", "link": "https://en.wikipedia.org/wiki/Relational_data_mining"}, "Wrapper (data mining)": {"title": "Wrapper (data mining)", "content": "Wrapper in data mining is a procedure that extracts regular subcontent of an unstructured or loosely-structured information source and translates it into a relational form, so it can be processed as structured data. Wrapper induction is the problem of devising extraction procedures on an automatic basis, with minimal reliance on hand-crafted rules. \nMany web pages are automatically generated from structured data \u2013 telephone directories, product catalogs, etc. \u2013 wrapped in a loosely structured presentation language (usually some variant of HTML), formatted for human browsing and navigation. Structured data are typically descriptions of objects retrieved from underlying databases and displayed in web pages following fixed templates at a low level, injected into pages where the high-level structure can vary from week to week, per the rapidly evolving fashion of the site's presentation skin. The precise dividing line between the fluid high-level skin and the less fluid structured data templates is rarely documented for public consumption, outside of the content management team at the web property. Software systems using such resources must translate HTML content into a relational form. Wrappers are commonly used as such translators. Formally, a wrapper is a function from a page to the set of tuples it contains.\n\n\n== Wrapper generation ==\nThere are two main approaches to wrapper generation: wrapper induction and automated data extraction.\nWrapper induction uses supervised learning to learn data extraction rules from manually labeled training examples. The disadvantages of wrapper induction are\n\nthe time-consuming manual labeling process and\nthe difficulty of wrapper maintenance.\nDue to the manual labeling effort, it is hard to extract data from a large number of sites as each site has its own templates and requires separate manual labeling for wrapper learning.\nWrapper maintenance is also a major issue because whenever a site changes the wrappers built for the site become obsolete. Due to these shortcomings, researchers have studied automated wrapper generation using unsupervised pattern mining. Automated extraction is possible because most Web data objects follow fixed templates. Discovering such templates or patterns enables the system to perform extraction automatically.\nWrapper generation on the Web is an important problem with a wide range of applications. Extraction of such data enables one to integrate data/information from multiple Web sites to provide value-added services, e.g., comparative shopping, object search, and information integration.\n\n\n== See also ==\nBusiness intelligence (section semi-structured or unstructured data)\nWeb scraping\n\n\n== Sources ==", "link": "https://en.wikipedia.org/wiki/Wrapper_(data_mining)"}, "Cross-industry standard process for data mining": {"title": "Cross-industry standard process for data mining", "content": "The Cross-industry standard process for data mining, known as CRISP-DM, is an open standard process model that describes common approaches used by data mining experts. It is the most widely-used analytics model.\nIn 2015, IBM released a new methodology called Analytics Solutions Unified Method for Data Mining/Predictive Analytics (also known as ASUM-DM), which refines and extends CRISP-DM.\n\n\n== History ==\nCRISP-DM was conceived in 1996 and became a European Union project under the ESPRIT funding initiative in 1997.  The project was led by five companies: Integral Solutions Ltd (ISL), Teradata, Daimler AG, NCR Corporation, and OHRA, an insurance company.\nThis core consortium brought different experiences to the project. ISL, was later acquired and merged into SPSS. The computer giant NCR Corporation produced the Teradata data warehouse and its own data mining software. Daimler-Benz had a significant data mining team. OHRA was starting to explore the potential use of data mining.\nThe first version of the methodology was presented at the 4th CRISP-DM SIG Workshop in Brussels in March 1999, and published as a step-by-step data mining guide later that year.\nBetween 2006 and 2008, a CRISP-DM 2.0 SIG was formed, and there were discussions about updating the CRISP-DM process model. The current status of these efforts is not known. However, the original crisp-dm.org website cited in the reviews, and the CRISP-DM 2.0 SIG website are both no longer active.\nWhile many non-IBM data mining practitioners use CRISP-DM, IBM is the primary corporation that currently uses the CRISP-DM process model. It makes some of the old CRISP-DM documents available for download and it has incorporated it into its SPSS Modeler product.\nBased on current research, CRISP-DM is the most widely used form of data-mining model because of its various advantages which solved the existing problems in the data mining industries. Some of the drawbacks of this model is that it does not perform project management activities. The success of  CRISP-DM is largely attributable to the fact that it is industry, tool, and application neutral.\n\n\n== Major phases ==\n\nCRISP-DM breaks the process of data mining into six major phases:\n\nBusiness Understanding\nData Understanding\nData Preparation\nModeling\nEvaluation\nDeployment\nThe sequence of the phases is not strict and moving back and forth between different phases is usually required. The arrows in the process diagram indicate the most important and frequent dependencies between phases. The outer circle in the diagram symbolizes the cyclic nature of data mining itself. A data mining process continues after a solution has been deployed. The lessons learned during the process can trigger new, often more focused business questions, and subsequent data mining processes will benefit from the experiences of previous ones.\n\n\n== Polls and Alternative Process Frameworks ==\nPolls conducted at the same website (KDNuggets) in 2002, 2004, 2007, and 2014 show that it was the leading methodology used by industry data miners who decided to respond to the survey. The only other data mining approach named in these polls was SEMMA. However, SAS Institute clearly states that SEMMA is not a data mining methodology, but rather a \"logical organization of the functional toolset of SAS Enterprise Miner.\" A review and critique of data mining process models in 2009 called the CRISP-DM the \"de facto standard for developing data mining and knowledge discovery projects.\"  Other reviews of CRISP-DM and data mining process models include Kurgan and Musilek's 2006 review, and Azevedo and Santos' 2008 comparison of CRISP-DM and SEMMA. Efforts to update the methodology started in 2006, but have, as of June 2015, not led to a new version, and the \"Special Interest Group\" (SIG) responsible along with the website has long disappeared (see History of CRISP-DM).\nIn 2024, Harvard Business Review published an updated framework, bizML, that is designed for greater relevance to business personnel and to be specific for machine learning projects in particular, rather than for analytics, data science, or data mining projects in general.\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining"}}, "Cybersecurity": {"Computer security": {"title": "Computer security", "content": "Computer security (also cybersecurity, digital security, or information technology (IT) security) is the protection of computer software, systems and networks from threats that can lead to unauthorized information disclosure, theft or damage to hardware, software, or data, as well as from the disruption or misdirection of the services they provide.\nThe significance of the field stems from the expanded reliance on computer systems, the Internet, and wireless network standards. Its importance is further amplified by the growth of smart devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity has emerged as one of the most significant new challenges facing the contemporary world, due to both the complexity of information systems and the societies they support. Security is particularly crucial for systems that govern large-scale systems with far-reaching physical effects, such as power distribution, elections, and finance.\nAlthough many aspects of computer security involve digital security, such as electronic passwords and encryption, physical security measures such as metal locks are still used to prevent unauthorized tampering. IT security is not a perfect subset of information security, therefore does not  completely align into the security convergence schema.\n\n\n== Vulnerabilities and attacks ==\n\nA vulnerability refers to a flaw in the structure, execution, functioning, or internal oversight of a computer or system that compromises its security. Most of the vulnerabilities that have been discovered are documented in the Common Vulnerabilities and Exposures (CVE) database. An exploitable vulnerability is one for which at least one working attack or exploit exists. Actors maliciously seeking vulnerabilities are known as threats. Vulnerabilities can be researched, reverse-engineered, hunted, or exploited using automated tools or customized scripts.\nVarious people or parties are vulnerable to cyber attacks; however, different groups are likely to experience different types of attacks more than others.\nIn April 2023, the United Kingdom Department for Science, Innovation & Technology released a report on cyber attacks over the last 12 months. They surveyed 2,263 UK businesses, 1,174 UK registered charities, and 554 education institutions. The research found that \"32% of businesses and 24% of charities overall recall any breaches or attacks from the last 12 months.\" These figures were much higher for \"medium businesses (59%), large businesses (69%), and high-income charities with \u00a3500,000 or more in annual income (56%).\" Yet, although medium or large businesses are more often the victims, since larger companies have generally improved their security over the last decade, small and midsize businesses (SMBs) have also become increasingly vulnerable as they often \"do not have advanced tools to defend the business.\" SMBs are most likely to be affected by malware, ransomware, phishing, man-in-the-middle attacks, and Denial-of Service (DoS) Attacks.\nNormal internet users are most likely to be affected by untargeted cyberattacks. These are where attackers indiscriminately target as many devices, services, or users as possible. They do this using techniques that take advantage of the openness of the Internet. These strategies mostly include phishing, ransomware, water holing and scanning.\nTo secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of the following categories:\n\n\n=== Backdoor ===\nA backdoor in a computer system, a cryptosystem, or an algorithm is any secret method of bypassing normal authentication or security controls. These weaknesses may exist for many reasons, including original design or poor configuration. Due to the nature of backdoors, they are of greater concern to companies and databases as opposed to individuals.\nBackdoors may be added by an authorized party to allow some legitimate access or by an attacker for malicious reasons. Criminals often use malware to install backdoors, giving them remote administrative access to a system. Once they have access, cybercriminals can \"modify files, steal personal information, install unwanted software, and even take control of the entire computer.\"\nBackdoors can be very hard to detect and are usually discovered by someone who has access to the application source code or intimate knowledge of the operating system of the computer.\n\n\n=== Denial-of-service attack ===\nDenial-of-service attacks (DoS) are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim's account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of distributed denial-of-service (DDoS) attacks are possible, where the attack comes from a large number of points. In this case, defending against these attacks is much more difficult. Such attacks can originate from the zombie computers of a botnet or from a range of other possible techniques, including distributed reflective denial-of-service (DRDoS), where innocent systems are fooled into sending traffic to the victim. With such attacks, the amplification factor makes the attack easier for the attacker because they have to use little bandwidth themselves. To understand why attackers may carry out these attacks, see the 'attacker motivation' section.\n\n\n=== Direct-access attacks ===\nA direct-access attack is when an unauthorized user (an attacker) gains physical access to a computer, most likely to directly copy data from it or steal information. Attackers may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless microphones. Even when the system is protected by standard security measures, these may be bypassed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and the Trusted Platform Module standard are designed to prevent these attacks.\nDirect service attackers are related in concept to direct memory attacks which allow an attacker to gain direct access to a computer's memory. The attacks \"take advantage of a feature of modern computers that allows certain devices, such as external hard drives, graphics cards, or network cards, to access the computer's memory directly.\"\n\n\n=== Eavesdropping ===\nEavesdropping is the act of surreptitiously listening to a private computer conversation (communication), usually between hosts on a network. It typically occurs when a user connects to a network where traffic is not secured or encrypted and sends sensitive business data to a colleague, which, when listened to by an attacker, could be exploited. Data transmitted across an \"open network\" allows an attacker to exploit a vulnerability and intercept it via various methods.\nUnlike malware, direct-access attacks, or other forms of cyber attacks, eavesdropping attacks are unlikely to negatively affect the performance of networks or devices, making them difficult to notice. In fact, \"the attacker does not need to have any ongoing connection to the software at all. The attacker can insert the software onto a compromised device, perhaps by direct insertion or perhaps by a virus or other malware, and then come back some time later to retrieve any data that is found or trigger the software to send the data at some determined time.\"\nUsing a virtual private network (VPN), which encrypts data between two points, is one of the most common forms of protection against eavesdropping. Using the best form of encryption possible for wireless networks is best practice, as well as using HTTPS instead of an unencrypted HTTP.\nPrograms such as Carnivore and NarusInSight have been used by the Federal Bureau of Investigation (FBI) and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact with the outside world) can be eavesdropped upon by monitoring the faint electromagnetic transmissions generated by the hardware. TEMPEST is a specification by the NSA referring to these attacks.\n\n\n=== Malware ===\nMalicious software (malware) is any software code or computer program \"intentionally written to harm a computer system or its users.\" Once present on a computer, it can leak sensitive details such as personal information, business information and passwords, can give control of the system to the attacker, and can corrupt or delete data permanently. Another type of malware is ransomware, which is when \"malware installs itself onto a victim's machine, encrypts their files, and then turns around and demands a ransom (usually in Bitcoin) to return that data to the user.\"\nTypes of malware include some of the following:\n\nViruses are a specific type of malware, and are normally a malicious code that hijacks software with the intention to \"do damage and spread copies of itself.\" Copies are made with the aim to spread to other programs on a computer.\nWorms are similar to viruses, however viruses can only function when a user runs (opens) a compromised program. Worms are self-replicating malware that spread between programs, apps and devices without the need for human interaction.\nTrojan horses are programs that pretend to be helpful or hide themselves within desired or legitimate software to \"trick users into installing them.\" Once installed, a RAT (remote access trojan) can create a secret backdoor on the affected device to cause damage.\nSpyware is a type of malware that secretly gathers information from an infected computer and transmits the sensitive information back to the attacker. One of the most common forms of spyware are keyloggers, which record all of a user's keyboard inputs/keystrokes, to \"allow hackers to harvest usernames, passwords, bank account and credit card numbers.\"\nScareware, as the name suggests, is a form of malware which uses social engineering (manipulation) to scare, shock, trigger anxiety, or suggest the perception of a threat in order to manipulate users into buying or installing unwanted software. These attacks often begin with a \"sudden pop-up with an urgent message, usually warning the user that they've broken the law or their device has a virus.\"\n\n\n=== Man-in-the-middle attacks ===\nMan-in-the-middle attacks (MITM) involve a malicious attacker trying to intercept, surveil or modify communications between two parties by spoofing one or both party's identities and injecting themselves in-between. Types of MITM attacks include:\n\nIP address spoofing is where the attacker hijacks routing protocols to reroute the targets traffic to a vulnerable network node for traffic interception or injection.\nMessage spoofing (via email, SMS or OTT messaging) is where the attacker spoofs the identity or carrier service while the target is using messaging protocols like email, SMS or OTT (IP-based) messaging apps. The attacker can then monitor conversations, launch social attacks or trigger zero-day-vulnerabilities to allow for further attacks.\nWiFi SSID spoofing is where the attacker simulates a WIFI base station SSID to capture and modify internet traffic and transactions. The attacker can also use local network addressing and reduced network defenses to penetrate the target's firewall by breaching known vulnerabilities. Sometimes known as a Pineapple attack thanks to a popular device. See also Malicious association.\nDNS spoofing is where attackers hijack domain name assignments to redirect traffic to systems under the attackers control, in order to surveil traffic or launch other attacks.\nSSL hijacking, typically coupled with another media-level MITM attack, is where the attacker spoofs the SSL authentication and encryption protocol by way of Certificate Authority injection in order to decrypt, surveil and modify traffic. See also TLS interception\n\n\n=== Multi-vector, polymorphic attacks ===\nSurfacing in 2017, a new class of multi-vector, polymorphic cyber threats combine several types of attacks and change form to avoid cybersecurity controls as they spread.\nMulti-vector polymorphic attacks, as the name describes, are both multi-vectored and polymorphic. Firstly, they are a singular attack that involves multiple methods of attack. In this sense, they are \u201cmulti-vectored (i.e. the attack can use multiple means of propagation such as via the Web, email and applications.\" However,  they are also multi-staged, meaning that \u201cthey can infiltrate networks and move laterally inside the network.\u201d The attacks can be polymorphic, meaning that the cyberattacks used such as viruses, worms or trojans \u201cconstantly change (\u201cmorph\u201d) making it nearly impossible to detect them using signature-based defences.\u201d\n\n\n=== Phishing ===\n\nPhishing is the attempt of acquiring sensitive information such as usernames, passwords, and credit card details directly from users by deceiving the users. Phishing is typically carried out by email spoofing, instant messaging, text message, or on a phone call. They often direct users to enter details at a fake website whose look and feel are almost identical to the legitimate one. The fake website often asks for personal information, such as login details and passwords. This information can then be used to gain access to the individual's real account on the real website.\nPreying on a victim's trust, phishing can be classified as a form of social engineering. Attackers can use creative ways to gain access to real accounts. A common scam is for attackers to send fake electronic invoices to individuals showing that they recently purchased music, apps, or others, and instructing them to click on a link if the purchases were not authorized. A more strategic type of phishing is spear-phishing which leverages personal or organization-specific details to make the attacker appear like a trusted source. Spear-phishing attacks target specific individuals, rather than the broad net cast by phishing attempts.\n\n\n=== Privilege escalation ===\nPrivilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level. For example, a standard computer user may be able to exploit a vulnerability in the system to gain access to restricted data; or even become root and have full unrestricted access to a system. The severity of attacks can range from attacks simply sending an unsolicited email to a ransomware attack on large amounts of data. Privilege escalation usually starts with social engineering techniques, often phishing.\nPrivilege escalation can be separated into two strategies, horizontal and vertical privilege escalation:\n\nHorizontal escalation (or account takeover) is where an attacker gains access to a normal user account that has relatively low-level privileges. This may be through stealing the user's username and password. Once they have access, they have gained a \u201cfoothold,\u201d and using this foothold the attacker then may move around the network of users at this same lower level, gaining access to information of this similar privilege.\nVertical escalation however targets people higher up in a company and often with more administrative power, such as an employee in IT with a higher privilege. Using this privileged account will then enable the attacker to invade other accounts.\n\n\n=== Side-channel attack ===\n\nAny computational system affects its environment in some form. This effect it has on its environment can range from electromagnetic radiation, to residual effect on RAM cells which as a consequence make a Cold boot attack possible, to hardware implementation faults that allow for access or guessing of other values that normally should be inaccessible. In Side-channel attack scenarios, the attacker would gather such information about a system or network to guess its internal state and as a result access the information which is assumed by the victim to be secure.\n\n\n=== Social engineering ===\nSocial engineering, in the context of computer security, aims to convince a user to disclose secrets such as passwords, card numbers, etc. or grant physical access by, for example, impersonating a senior executive, bank, a contractor, or a customer. This generally involves exploiting people's trust, and relying on their cognitive biases. A common scam involves emails sent to accounting and finance department personnel, impersonating their CEO and urgently requesting some action. One of the main techniques of social engineering are phishing attacks.\nIn early 2016, the FBI reported that such business email compromise (BEC) scams had cost US businesses more than $2 billion in about two years.\nIn May 2016, the Milwaukee Bucks NBA team was the victim of this type of cyber scam with a perpetrator impersonating the team's president Peter Feigin, resulting in the handover of all the team's employees' 2015 W-2 tax forms.\n\n\n=== Spoofing ===\n\nSpoofing is an act of pretending to be a valid entity through the falsification of data (such as an IP address or username), in order to gain access to information or resources that one is otherwise unauthorized to obtain. Spoofing is closely related to phishing. There are several types of spoofing, including:\n\nEmail spoofing, is where an attacker forges the sending (From, or source) address of an email.\nIP address spoofing, where an attacker alters the source IP address in a network packet to hide their identity or impersonate another computing system.\nMAC spoofing, where an attacker modifies the Media Access Control (MAC) address of their network interface controller to obscure their identity, or to pose as another.\nBiometric spoofing, where an attacker produces a fake biometric sample to pose as another user.\nAddress Resolution Protocol (ARP) spoofing, where an attacker sends spoofed address resolution protocol onto a local area network to associate their Media Access Control address with a different host's IP address. This causes data to be sent to the attacker rather than the intended host.\nIn 2018, the cybersecurity firm Trellix published research on the life-threatening risk of spoofing in the healthcare industry.\n\n\n=== Tampering ===\nTampering describes a malicious modification or alteration of data. It is an intentional but unauthorized act resulting in the modification of a system, components of systems, its intended behavior, or data. So-called Evil Maid attacks and security services planting of surveillance capability into routers are examples.\n\n\n=== HTML smuggling ===\nHTML smuggling allows an attacker to \"smuggle\" a malicious code inside a particular HTML or web page. HTML files can carry payloads concealed as benign, inert data in order to defeat content filters. These payloads can be reconstructed on the other side of the filter.\nWhen a target user opens the HTML, the malicious code is activated; the web browser then \"decodes\" the script, which then unleashes the malware onto the target's device.\n\n\n== Information security practices ==\nEmployee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness toward information security within an organization. Information security culture is the \"...totality of patterns of behavior in an organization that contributes to the protection of information of all kinds.\"\nAndersson and Reimers (2014) found that employees often do not see themselves as part of their organization's information security effort and often take actions that impede organizational changes. Indeed, the Verizon Data Breach Investigations Report 2020, which examined 3,950 security breaches, discovered 30% of cybersecurity incidents involved internal actors within a company. Research shows information security culture needs to be improved continuously. In \"Information Security Culture from Analysis to Change\", authors commented, \"It's a never-ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.\n\nPre-evaluation: To identify the awareness of information security within employees and to analyze the current security policies.\nStrategic planning: To come up with a better awareness program, clear targets need to be set. Assembling a team of skilled professionals is helpful to achieve it.\nOperative planning: A good security culture can be established based on internal communication, management buy-in, security awareness and a training program.\nImplementation: Four stages should be used to implement the information security culture. They are:\nCommitment of the management\nCommunication with organizational members\nCourses for all organizational members\nCommitment of the employees\nPost-evaluation: To assess the success of the planning and implementation, and to identify unresolved areas of concern.\n\n\n== Computer protection (countermeasures) ==\nIn computer security, a countermeasure is an action, device, procedure or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.\nSome common countermeasures are listed in the following sections:\n\n\n=== Security by design ===\n\nSecurity by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered a main feature.\nThe UK government's National Cyber Security Centre separates secure cyber design principles into five sections:\n\nBefore a secure system is created or updated, companies should ensure they understand the fundamentals and the context around the system they are trying to create and identify any weaknesses in the system.\nCompanies should design and centre their security around techniques and defences which make attacking their data or systems inherently more challenging for attackers.\nCompanies should ensure that their core services that rely on technology are protected so that the systems are essentially never down.\nAlthough systems can be created which are safe against a multitude of attacks, that does not mean that attacks will not be attempted. Despite one's security, all companies' systems should aim to be able to detect and spot attacks as soon as they occur to ensure the most effective response to them.\nCompanies should create secure systems designed so that any attack that is \"successful\" has minimal severity.\nThese design principles of security by design can include some of the following techniques:\n\nThe principle of least privilege, where each part of the system has only the privileges that are needed for its function. That way, even if an attacker gains access to that part, they only have limited access to the whole system.\nAutomated theorem proving to prove the correctness of crucial software subsystems.\nCode reviews and unit testing, approaches to make modules more secure where formal correctness proofs are not possible.\nDefense in depth, where the design is such that more than one subsystem needs to be violated to compromise the integrity of the system and the information it holds.\nDefault secure settings, and design to fail secure rather than fail insecure (see fail-safe for the equivalent in safety engineering). Ideally, a secure system should require a deliberate, conscious, knowledgeable and free decision on the part of legitimate authorities in order to make it insecure.\nAudit trails track system activity so that when a security breach occurs, the mechanism and extent of the breach can be determined. Storing audit trails remotely, where they can only be appended to, can keep intruders from covering their tracks.\nFull disclosure of all vulnerabilities, to ensure that the window of vulnerability is kept as short as possible when bugs are discovered.\n\n\n=== Security architecture ===\nSecurity architecture can be defined as the \"practice of designing computer systems to achieve security goals.\" These goals have overlap with the principles of \"security by design\" explored above, including to \"make initial compromise of the system difficult,\" and to \"limit the impact of any compromise.\" In practice, the role of a security architect would be to ensure the structure of a system reinforces the security of the system, and that new changes are safe and meet the security requirements of the organization.\nSimilarly, Techopedia defines security architecture as \"a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment. It also specifies when and where to apply security controls. The design process is generally reproducible.\" The key attributes of security architecture are:\n\nthe relationship of different components and how they depend on each other.\ndetermination of controls based on risk assessment, good practices, finances, and legal matters.\nthe standardization of controls.\nPracticing security architecture provides the right foundation to systematically address business, IT and security concerns in an organization.\n\n\n=== Security measures ===\nA state of computer security is the conceptual ideal, attained by the use of three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:\n\nLimiting the access of individuals using user account access controls and using cryptography can protect systems files and data, respectively.\nFirewalls are by far the most common prevention systems from a network security perspective as they can (if properly configured) shield access to internal network services and block certain kinds of attacks through packet filtering. Firewalls can be both hardware and software-based. Firewalls monitor and control incoming and outgoing traffic of a computer network and establish a barrier between a trusted network and an untrusted network.\nIntrusion Detection System (IDS) products are designed to detect network attacks in-progress and assist in post-attack forensics, while audit trails and logs serve a similar function for individual systems.\nResponse is necessarily defined by the assessed security requirements of an individual system and may cover the range from simple upgrade of protections to notification of legal authorities, counter-attacks, and the like. In some special cases, the complete destruction of the compromised system is favored, as it may happen that not all the compromised resources are detected.\nCyber security awareness training to cope with cyber threats and attacks.\nForward web proxy solutions can prevent the client to visit malicious web pages and inspect the content before downloading to the client machines.\nToday, computer security consists mainly of preventive measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet. They can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real-time filtering and blocking. Another implementation is a so-called physical firewall, which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.\nSome organizations are turning to big data platforms, such as Apache Hadoop, to extend data accessibility and machine learning to detect advanced persistent threats.\nIn order to ensure adequate security, the confidentiality, integrity and availability of a network, better known as the CIA triad, must be protected and is considered the foundation to information security. To achieve those objectives, administrative, physical and technical security measures should be employed. The amount of security afforded to an asset can only be determined when its value is known.\n\n\n=== Vulnerability management ===\n\nVulnerability management is the cycle of identifying, fixing or mitigating vulnerabilities, especially in software and firmware. Vulnerability management is integral to computer security and network security.\nVulnerabilities can be discovered with a vulnerability scanner, which analyzes a computer system in search of known vulnerabilities, such as open ports, insecure software configuration, and susceptibility to malware.  In order for these tools to be effective, they must be kept up to date with every new update the vendor release.  Typically, these updates will scan for the new vulnerabilities that were introduced recently.\nBeyond vulnerability scanning, many organizations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities. In some sectors, this is a contractual requirement.\n\n\n=== Reducing vulnerabilities ===\nThe act of assessing and reducing vulnerabilities to cyber attacks is commonly referred to as information technology security assessments. They aim to assess systems for risk and to predict and test for their vulnerabilities. While formal verification of the correctness of computer systems is possible, it is not yet common. Operating systems formally verified include seL4, and SYSGO's PikeOS \u2013 but these make up a very small percentage of the market.\nIt is possible to reduce an attacker's chances by keeping systems up to date with security patches and updates and by hiring people with expertise in security. Large companies with significant threats can hire Security Operations Centre (SOC) Analysts. These are specialists in cyber defences, with their role ranging from \"conducting threat analysis to investigating reports of any new issues and preparing and testing disaster recovery plans.\"\nWhilst no measures can completely guarantee the prevention of an attack, these measures can help mitigate the damage of possible attacks. The effects of data loss/damage can be also reduced by careful backing up and insurance.\nOutside of formal assessments, there are various methods of reducing vulnerabilities. Two factor authentication is a method for mitigating unauthorized access to a system or sensitive information. It requires something you know: a password or PIN, and something you have: a card, dongle, cellphone, or another piece of hardware. This increases security as an unauthorized person needs both of these to gain access.\nProtecting against social engineering and direct computer access (physical) attacks can only happen by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk by improving people's knowledge of how to protect themselves and by increasing people's awareness of threats. However, even in highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.\nInoculation, derived from inoculation theory, seeks to prevent social engineering and other fraudulent tricks and traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts.\n\n\n=== Hardware protection mechanisms ===\n\nHardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.\n\nUSB dongles are typically used in software licensing schemes to unlock software capabilities, but they can also be seen as a way to prevent unauthorized access to a computer or other device's software. The dongle, or key, essentially creates a secure encrypted tunnel between the software application and the key. The principle is that an encryption scheme on the dongle, such as Advanced Encryption Standard (AES) provides a stronger measure of security since it is harder to hack and replicate the dongle than to simply copy the native software to another machine and use it. Another security application for dongles is to use them for accessing web-based content such as cloud software or Virtual Private Networks (VPNs). In addition, a USB dongle can be configured to lock or unlock a computer.\nTrusted platform modules (TPMs) secure devices by integrating cryptographic capabilities onto access devices, through the use of microprocessors, or so-called computers-on-a-chip. TPMs used in conjunction with server-side software offer a way to detect and authenticate hardware devices, preventing unauthorized network and data access.\nComputer case intrusion detection refers to a device, typically a push-button switch, which detects when a computer case is opened. The firmware or BIOS is programmed to show an alert to the operator when the computer is booted up the next time.\nDrive locks are essentially software tools to encrypt hard drives, making them inaccessible to thieves. Tools exist specifically for encrypting external drives as well.\nDisabling USB ports is a security option for preventing unauthorized and malicious access to an otherwise secure computer. Infected USB dongles connected to a network from a computer inside the firewall are considered by the magazine Network World as the most common hardware threat facing computer networks.\nDisconnecting or disabling peripheral devices (like camera, GPS, removable storage, etc.), that are not in use.\nMobile-enabled access devices are growing in popularity due to the ubiquitous nature of cell phones. Built-in capabilities such as Bluetooth, the newer Bluetooth low energy (LE), near-field communication (NFC) on non-iOS devices and biometric validation such as thumbprint readers, as well as QR code reader software designed for mobile devices, offer new, secure ways for mobile phones to connect to access control systems. These control systems provide computer security and can also be used for controlling access to secure buildings.\nIOMMUs allow for hardware-based sandboxing of components in mobile and desktop computers by utilizing direct memory access protections.\nPhysical Unclonable Functions (PUFs) can be used as a digital fingerprint or a unique identifier to integrated circuits and hardware, providing users the ability to secure the hardware supply chains going into their systems.\n\n\n=== Secure operating systems ===\n\nOne use of the term computer security refers to technology that is used to implement secure operating systems. Using secure operating systems is a good way of ensuring computer security. These are systems that have achieved certification from an external security-auditing organization, the most popular evaluations are Common Criteria (CC).\n\n\n=== Secure coding ===\n\nIn software engineering, secure coding aims to guard against the accidental introduction of security vulnerabilities. It is also possible to create software designed from the ground up to be secure. Such systems are secure by design. Beyond this, formal verification aims to prove the correctness of the algorithms underlying a system;\nimportant for cryptographic protocols for example.\n\n\n=== Capabilities and access control lists ===\n\nWithin computer systems, two of the main security models capable of enforcing privilege separation are access control lists (ACLs) and role-based access control (RBAC).\nAn access-control list (ACL), with respect to a computer file system, is a list of permissions associated with an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.\nRole-based access control is an approach to restricting system access to authorized users,  used by the majority of enterprises with more than 500 employees, and can implement mandatory access control (MAC) or discretionary access control (DAC).\nA further approach, capability-based security has been mostly restricted to research operating systems. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open-source project in the area is the E language.\n\n\n=== User security training ===\nThe end-user is widely recognized as the weakest link in the security chain and it is estimated that more than 90% of security incidents and breaches involve some kind of human error. Among the most commonly recorded forms of errors and misjudgment are poor password management, sending emails containing sensitive data and attachments to the wrong recipient, the inability to recognize misleading URLs and to identify fake websites and dangerous email attachments.  A common mistake that users make is saving their user id/password in their browsers to make it easier to log in to banking sites.  This is a gift to attackers who have obtained access to a machine by some means.  The risk may be mitigated by the use of two-factor authentication.\nAs the human component of cyber risk is particularly relevant in determining the global cyber risk an organization is facing, security awareness training, at all levels, not only provides formal compliance with regulatory and industry mandates but is considered essential in reducing cyber risk and protecting individuals and companies from the great majority of cyber threats.\nThe focus on the end-user represents a profound cultural change for many security practitioners, who have traditionally approached cybersecurity exclusively from a technical perspective, and moves along the lines suggested by major security centers to develop a culture of cyber awareness within the organization, recognizing that a security-aware user provides an important line of defense against cyber attacks.\n\n\n=== Digital hygiene ===\nRelated to end-user training, digital hygiene or cyber hygiene is a fundamental principle relating to information security and, as the analogy with personal hygiene shows, is the equivalent of establishing simple routine measures to minimize the risks from cyber threats. The assumption is that good cyber hygiene practices can give networked users another layer of protection, reducing the risk that one vulnerable node will be used to either mount attacks or compromise another node or network, especially from common cyberattacks. Cyber hygiene should also not be mistaken for proactive cyber defence, a military term.\nThe most common acts of digital hygiene can include updating malware protection, cloud back-ups, passwords, and ensuring restricted admin rights and network firewalls. As opposed to a purely technology-based defense against threats, cyber hygiene mostly regards routine measures that are technically simple to implement and mostly dependent on discipline or education. It can be thought of as an abstract list of tips or measures that have been demonstrated as having a positive effect on personal or collective digital security. As such, these measures can be performed by laypeople, not just security experts.\nCyber hygiene relates to personal hygiene as computer viruses relate to biological viruses (or pathogens). However, while the term computer virus was coined almost simultaneously with the creation of the first working computer viruses, the term cyber hygiene is a much later invention, perhaps as late as 2000 by Internet pioneer Vint Cerf. It has since been adopted by the Congress and Senate of the United States, the FBI, EU institutions and heads of state.\n\n\n=== Difficulty of responding to breaches ===\nResponding to attempted security breaches is often very difficult for a variety of reasons, including:\n\nIdentifying attackers is difficult, as they may operate through proxies, temporary anonymous dial-up accounts, wireless connections, and other anonymizing procedures which make back-tracing difficult \u2013 and are often located in another jurisdiction. If they successfully breach security, they have also often gained enough administrative access to enable them to delete logs to cover their tracks.\nThe sheer number of attempted attacks, often by automated vulnerability scanners and computer worms, is so large that organizations cannot spend time pursuing each.\nLaw enforcement officers often lack the skills, interest or budget to pursue attackers. Furthermore, identifying attackers across a network may necessitate collecting logs from multiple locations within the network and across various countries, a process that can be both difficult and time-consuming.\nWhere an attack succeeds and a breach occurs, many jurisdictions now have in place mandatory security breach notification laws.\n\n\n=== Types of security and privacy ===\n\n\n== Systems at risk ==\nThe growth in the number of computer systems and the increasing reliance upon them by individuals, businesses, industries, and governments means that there are an increasing number of systems at risk.\n\n\n=== Financial systems ===\nThe computer systems of financial regulators and financial institutions like the U.S. Securities and Exchange Commission, SWIFT, investment banks, and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains. Websites and apps that accept or store credit card numbers, brokerage accounts, and bank account information are also prominent hacking targets, because of the potential for immediate financial gain from transferring money, making purchases, or selling the information on the black market. In-store payment systems and ATMs have also been tampered with in order to gather customer account data and PINs.\nThe UCLA Internet Report: Surveying the Digital Future (2000) found that the privacy of personal data created barriers to online sales and that more than nine out of 10 internet users were somewhat or very concerned about credit card security.\nThe most common web technologies for improving security between browsers and websites are named SSL (Secure Sockets Layer), and its successor TLS (Transport Layer Security), identity management and authentication services, and domain name services allow companies and consumers to engage in secure communications and commerce. Several versions of SSL and TLS are commonly used today in applications such as web browsing, e-mail, internet faxing, instant messaging, and VoIP (voice-over-IP). There are various interoperable implementations of these technologies, including at least one implementation that is open source. Open source allows anyone to view the application's source code, and look for and report vulnerabilities.\nThe credit card companies Visa and MasterCard cooperated to develop the secure EMV chip which is embedded in credit cards. Further developments include the Chip Authentication Program where banks give customers hand-held card readers to perform online secure transactions. Other developments in this arena include the development of technology such as Instant Issuance which has enabled shopping mall kiosks acting on behalf of banks to issue on-the-spot credit cards to interested customers.\n\n\n=== Utilities and industrial equipment ===\nComputers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.\n\n\n=== Aviation ===\nThe aviation industry is very reliant on a series of complex systems which could be attacked. A simple power outage at one airport can cause repercussions worldwide, much of the system relies on radio transmissions which could be disrupted, and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore. There is also potential for attack from within an aircraft.\nImplementing fixes in aerospace systems poses a unique challenge because efficient air transportation is heavily affected by weight and volume. Improving security by adding physical devices to airplanes could increase their unloaded weight, and could potentially reduce cargo or passenger capacity.\nIn Europe, with the (Pan-European Network Service) and NewPENS, and in the US with the NextGen program, air navigation service providers are moving to create their own dedicated networks.\nMany modern passports are now biometric passports, containing an embedded microchip that stores a digitized photograph and personal information such as name, gender, and date of birth. In addition, more countries are introducing facial recognition technology to reduce identity-related fraud. The introduction of the ePassport has assisted border officials in verifying the identity of the passport holder, thus allowing for quick passenger processing. Plans are under way in the US, the UK, and Australia to introduce SmartGate kiosks with both retina and fingerprint recognition technology. The airline industry is moving from the use of traditional paper tickets towards the use of electronic tickets (e-tickets). These have been made possible by advances in online credit card transactions in partnership with the airlines. Long-distance bus companies are also switching over to e-ticketing transactions today.\nThe consequences of a successful attack range from loss of confidentiality to loss of system integrity, air traffic control outages, loss of aircraft, and even loss of life.\n\n\n=== Consumer devices ===\nDesktop computers and laptops are commonly targeted to gather passwords or financial account information or to construct a botnet to attack another target. Smartphones, tablet computers, smart watches, and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. WiFi, Bluetooth, and cell phone networks on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.\nThe increasing number of home automation devices such as the Nest thermostat are also potential targets.\n\n\n=== Healthcare ===\nToday many healthcare providers and health insurance companies use the internet to provide enhanced products and services, for example through use of tele-health to potentially offer better quality and access to healthcare, or fitness trackers to lower insurance premiums.\nThe health care company Humana partners with WebMD, Oracle Corporation, EDS and Microsoft to enable its members to access their health care records, as well as to provide an overview of health care plans. Patient records are increasingly being placed on secure in-house networks, alleviating the need for extra storage space.\n\n\n=== Large corporations ===\nLarge corporations are common targets. In many cases attacks are aimed at financial gain through identity theft and involve data breaches. Examples include the loss of millions of clients' credit card and financial details by Home Depot, Staples, Target Corporation, and Equifax.\nMedical records have been targeted in general identify theft, health insurance fraud, and impersonating patients to obtain prescription drugs for recreational purposes or resale. Although cyber threats continue to increase, 62% of all organizations did not increase security training for their business in 2015.\nNot all attacks are financially motivated, however: security firm HBGary Federal had a serious series of attacks in 2011 from hacktivist group Anonymous in retaliation for the firm's CEO claiming to have infiltrated their group, and Sony Pictures was hacked in 2014 with the apparent dual motive of embarrassing the company through data leaks and crippling the company by wiping workstations and servers.\n\n\n=== Automobiles ===\n\nVehicles are increasingly computerized, with engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver-assistance systems on many models. Additionally, connected cars may use WiFi and Bluetooth to communicate with onboard consumer devices and the cell phone network. Self-driving cars are expected to be even more complex. All of these systems carry some security risks, and such issues have gained wide attention.\nSimple examples of risk include a malicious compact disc being used as an attack vector, and the car's onboard microphones being used for eavesdropping. However, if access is gained to a car's internal controller area network, the danger is much greater \u2013 and in a widely publicized 2015 test, hackers remotely carjacked a vehicle from 10 miles away and drove it into a ditch.\nManufacturers are reacting in numerous ways, with Tesla in 2016 pushing out some security fixes over the air into its cars' computer systems. In the area of autonomous vehicles, in September 2016 the United States Department of Transportation announced some initial safety standards, and called for states to come up with uniform policies.\nAdditionally, e-Drivers' licenses are being developed using the same technology. For example, Mexico's licensing authority (ICV) has used a smart card platform to issue the first e-Drivers' licenses to the city of Monterrey, in the state of Nuevo Le\u00f3n.\n\n\n=== Shipping ===\nShipping companies have adopted RFID (Radio Frequency Identification) technology as an efficient, digitally secure, tracking device. Unlike a barcode, RFID can be read up to 20 feet away. RFID is used by FedEx and UPS.\n\n\n=== Government ===\nGovernment and military computer systems are commonly attacked by activists and foreign powers. Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, as well as student records.\nThe FBI, CIA, and Pentagon, all utilize secure controlled access technology for any of their buildings. However, the use of this form of technology is spreading into the entrepreneurial world. More and more companies are taking advantage of the development of digitally secure controlled access technology. GE's ACUVision, for example, offers a single panel platform for access control, alarm monitoring and digital recording.\n\n\n=== Internet of things and physical vulnerabilities ===\nThe Internet of things (IoT) is the network of physical objects such as devices, vehicles, and buildings that are embedded with electronics, software, sensors, and network connectivity that enables them to collect and exchange data. Concerns have been raised that this is being developed without appropriate consideration of the security challenges involved.\nWhile the IoT creates opportunities for more direct integration of the physical world into computer-based systems,\nit also provides opportunities for misuse. In particular, as the Internet of Things spreads widely, cyberattacks are likely to become an increasingly physical (rather than simply virtual) threat. If a front door's lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.\nAn attack aimed at physical infrastructure or human lives is often called a cyber-kinetic attack. As IoT devices and appliances become more widespread, the prevalence and potential damage of cyber-kinetic attacks can increase substantially.\n\n\n=== Medical systems ===\n\nMedical devices have either been successfully attacked or had potentially deadly vulnerabilities demonstrated, including both in-hospital diagnostic equipment and implanted devices including pacemakers and insulin pumps. There are many reports of hospitals and hospital organizations getting hacked, including ransomware attacks, Windows XP exploits, viruses, and data breaches of sensitive data stored on hospital servers. On 28 December 2016 the US Food and Drug Administration released its recommendations for how medical device manufacturers should maintain the security of Internet-connected devices \u2013 but no structure for enforcement.\n\n\n=== Energy sector ===\nIn distributed generation systems, the risk of a cyber attack is real, according to Daily Energy Insider. An attack could cause a loss of power in a large area for a long period of time, and such an attack could have just as severe consequences as a natural disaster. The District of Columbia is considering creating a Distributed Energy Resources (DER) Authority within the city, with the goal being for customers to have more insight into their own energy use and giving the local electric utility, Pepco, the chance to better estimate energy demand. The D.C. proposal, however, would \"allow third-party vendors to create numerous points of energy distribution, which could potentially create more opportunities for cyber attackers to threaten the electric grid.\"\n\n\n=== Telecommunications ===\nPerhaps the most widely known digitally secure telecommunication device is the SIM (Subscriber Identity Module) card, a device that is embedded in most of the world's cellular devices before any service can be obtained. The SIM card is just the beginning of this digitally secure environment.\nThe Smart Card Web Servers draft standard (SCWS) defines the interfaces to an HTTP server in a smart card. Tests are being conducted to secure OTA (\"over-the-air\") payment and credit card information from and to a mobile phone. \nCombination SIM/DVD devices are being developed through Smart Video Card technology which embeds a DVD-compliant optical disc into the card body of a regular SIM card.\nOther telecommunication developments involving digital security include mobile signatures, which use the embedded SIM card to generate a legally binding electronic signature.\n\n\n== Cost and impact of security breaches ==\nSerious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. \"Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal.\"\nHowever, reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions. According to the classic Gordon-Loeb Model analyzing the optimal investment level in information security, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).\n\n\n== Attacker motivation ==\nAs with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, some are activists, others are criminals looking for financial gain. State-sponsored attackers are now common and well resourced but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll in The Cuckoo's Egg.\nAttackers motivations can vary for all types of attacks from pleasure to political goals. For example, \"hacktivists\" may target a company or organization that carries out activities they do not agree with. This would be to create bad publicity for the company by having its website crash.\nHigh capability hackers, often with larger backing or state sponsorship, may attack based on the demands of their financial backers. These attacks are more likely to attempt more serious attack. An example of a more serious attack was the 2015 Ukraine power grid hack, which reportedly utilised the spear-phising, destruction of files, and denial-of-service attacks to carry out the full attack.\nAdditionally, recent attacker motivations can be traced back to extremist organizations seeking to gain political advantage or disrupt social agendas. The growth of the internet, mobile technologies, and inexpensive computing devices have led to a rise in capabilities but also to the risk to environments that are deemed as vital to operations. All critical targeted environments are susceptible to compromise and this has led to a series of proactive studies on how to migrate the risk by taking into consideration motivations by these types of actors. Several stark differences exist between the hacker motivation and that of nation state actors seeking to attack based on an ideological preference.\nA key aspect of threat modeling for any system is identifying the motivations behind potential attacks and the individuals or groups likely to carry them out. The level and detail of security measures will differ based on the specific system being protected. For instance, a home personal computer, a bank, and a classified military network each face distinct threats, despite using similar underlying technologies.\n\n\n== Computer security incident management ==\nComputer security incident management is an organized approach to addressing and managing the aftermath of a computer security incident or compromise with the goal of preventing a breach or thwarting a cyberattack. An incident that is not identified and managed at the time of intrusion typically escalates to a more damaging event such as a data breach or system failure. The intended outcome of a computer security incident response plan is to contain the incident, limit damage and assist recovery to business as usual. Responding to compromises quickly can mitigate exploited vulnerabilities, restore services and processes and minimize losses.\nIncident response planning allows an organization to establish a series of best practices to stop an intrusion before it causes damage. Typical incident response plans contain a set of written instructions that outline the organization's response to a cyberattack. Without a documented plan in place, an organization may not successfully detect an intrusion or compromise and stakeholders may not understand their roles, processes and procedures during an escalation, slowing the organization's response and resolution.\nThere are four key components of a computer security incident response plan:\n\nPreparation: Preparing stakeholders on the procedures for handling computer security incidents or compromises\nDetection and analysis: Identifying and investigating suspicious activity to confirm a security incident, prioritizing the response based on impact and coordinating notification of the incident\nContainment, eradication and recovery: Isolating affected systems to prevent escalation and limit impact, pinpointing the genesis of the incident, removing malware, affected systems and bad actors from the environment and restoring systems and data when a threat no longer remains\nPost incident activity: Post mortem analysis of the incident, its root cause and the organization's response with the intent of improving the incident response plan and future response efforts.\n\n\n== Notable attacks and breaches ==\n\nSome illustrative examples of different types of computer security breaches are given below.\n\n\n=== Robert Morris and the first computer worm ===\n\nIn 1988, 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On 2 November 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers \u2013 the first internet computer worm. The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris who said \"he wanted to count how many machines were connected to the Internet\".\n\n\n=== Rome Laboratory ===\nIn 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.\n\n\n=== TJX customer credit card details ===\nIn early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.\n\n\n=== Stuxnet attack ===\nIn 2010, the computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran's nuclear centrifuges. It did so by disrupting industrial programmable logic controllers (PLCs) in a targeted attack. This is generally believed to have been launched by Israel and the United States to disrupt Iran's nuclear program \u2013 although neither has publicly admitted this.\n\n\n=== Global surveillance disclosures ===\n\nIn early 2013, documents provided by Edward Snowden were published by The Washington Post and The Guardian exposing the massive scale of NSA global surveillance. There were also indications that the NSA may have inserted a backdoor in a NIST standard for encryption. This standard was later withdrawn due to widespread criticism. The NSA additionally were revealed to have tapped the links between Google's data centers.\n\n\n=== Target and Home Depot breaches ===\nA Ukrainian hacker known as Rescator broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. \"The malware utilized is absolutely unsophisticated and uninteresting,\" says Jim Walter, director of threat intelligence operations at security technology company McAfee \u2013 meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.\n\n\n=== Office of Personnel Management data breach ===\nIn April 2015, the Office of Personnel Management discovered it had been hacked more than a year earlier in a data breach, resulting in the theft of approximately 21.5 million personnel records handled by the office. The Office of Personnel Management hack has been described by federal officials as among the largest breaches of government data in the history of the United States. Data targeted in the breach included personally identifiable information such as Social Security numbers, names, dates and places of birth, addresses, and fingerprints of current and former government employees as well as anyone who had undergone a government background check. It is believed the hack was perpetrated by Chinese hackers.\n\n\n=== Ashley Madison breach ===\n\nIn July 2015, a hacker group is known as The Impact Team successfully breached the extramarital relationship website Ashley Madison, created by Avid Life Media. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently. When Avid Life Media did not take the site offline the group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned; but the website remained to function.\n\n\n=== Colonial Pipeline ransomware attack ===\n\nIn June 2021, the cyber attack took down the largest fuel pipeline in the U.S. and led to shortages across the East Coast.\n\n\n== Legal issues and global regulation ==\nInternational legal issues of cyber attacks are complicated in nature. There is no global base of common rules to judge, and eventually punish, cybercrimes and cybercriminals - and where security firms or agencies do locate the cybercriminal behind the creation of a particular piece of malware or form of cyber attack, often the local authorities cannot take action due to lack of laws under which to prosecute. Proving attribution for cybercrimes and cyberattacks is also a major problem for all law enforcement agencies. \"Computer viruses switch from one country to another, from one jurisdiction to another \u2013 moving around the world, using the fact that we don't have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world.\" The use of techniques such as dynamic DNS, fast flux and bullet proof servers add to the difficulty of investigation and enforcement.\n\n\n== Role of government ==\nThe role of the government is to make regulations to force companies and organizations to protect their systems, infrastructure and information from any cyberattacks, but also to protect its own national infrastructure such as the national power-grid.\nThe government's regulatory role in cyberspace is complicated. For some, cyberspace was seen as a virtual space that was to remain free of government intervention, as can be seen in many of today's libertarian blockchain and bitcoin discussions.\nMany government officials and experts think that the government should do more and that there is a crucial need for improved regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the \"industry only responds when you threaten regulation. If the industry doesn't respond (to the threat), you have to follow through.\" On the other hand, executives from the private sector agree that improvements are necessary, but think that government intervention would affect their ability to innovate efficiently. Daniel R. McCarthy analyzed this public-private partnership in cybersecurity and reflected on the role of cybersecurity in the broader constitution of political order.\nOn 22 May 2020, the UN Security Council held its second ever informal meeting on cybersecurity to focus on cyber challenges to international peace. According to UN Secretary-General Ant\u00f3nio Guterres, new technologies are too often used to violate rights.\n\n\n== International actions ==\nMany different teams and organizations exist, including:\n\nThe Forum of Incident Response and Security Teams (FIRST) is the global association of CSIRTs. The US-CERT, AT&T, Apple, Cisco, McAfee, Microsoft are all members of this international team.\nThe Council of Europe helps protect societies worldwide from the threat of cybercrime through the Convention on Cybercrime.\nThe purpose of the Messaging Anti-Abuse Working Group (MAAWG) is to bring the messaging industry together to work collaboratively and to successfully address the various forms of messaging abuse, such as spam, viruses, denial-of-service attacks and other messaging exploitations. France Telecom, Facebook, AT&T, Apple, Cisco, Sprint are some of the members of the MAAWG.\nENISA : The European Network and Information Security Agency (ENISA) is an agency of the European Union with the objective to improve network and information security in the European Union.\n\n\n=== Europe ===\nOn 14 April 2016, the European Parliament and the Council of the European Union adopted the General Data Protection Regulation (GDPR). The GDPR, which came into force on 25 May 2018, grants individuals within the European Union (EU) and the European Economic Area (EEA) the right to the protection of personal data. The regulation requires that any entity that processes personal data incorporate data protection by design and by default. It also requires that certain organizations appoint a Data Protection Officer (DPO).\n\n\n== National actions ==\n\n\n=== Computer emergency response teams ===\n\nMost countries have their own computer emergency response team to protect network security.\n\n\n==== Canada ====\nSince 2010, Canada has had a cybersecurity strategy. This functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure. The strategy has three main pillars: securing government systems, securing vital private cyber systems, and helping Canadians to be secure online. There is also a Cyber Incident Management Framework to provide a coordinated response in the event of a cyber incident.\nThe Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada's critical infrastructure and cyber systems. It provides support to mitigate cyber threats, technical support to respond & recover from targeted cyber attacks, and provides online tools for members of Canada's critical infrastructure sectors. It posts regular cybersecurity bulletins & operates an online reporting tool where individuals and organizations can report a cyber incident.\nTo inform the general public on how to protect themselves online, Public Safety Canada has partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations, and launched the Cyber Security Cooperation Program. They also run the GetCyberSafe portal for Canadian citizens, and Cyber Security Awareness Month during October.\nPublic Safety Canada aims to begin an evaluation of Canada's cybersecurity strategy in early 2015.\n\n\n==== Australia ====\nAustralian federal government announced an $18.2 million investment to fortify the cybersecurity resilience of small and medium enterprises (SMEs) and enhance their capabilities in responding to cyber threats. This financial backing is an integral component of the soon-to-be-unveiled 2023-2030 Australian Cyber Security Strategy, slated for release within the current week. A substantial allocation of $7.2 million is earmarked for the establishment of a voluntary cyber health check program, facilitating businesses in conducting a comprehensive and tailored self-assessment of their cybersecurity upskill.\nThis avant-garde health assessment serves as a diagnostic tool, enabling enterprises to ascertain the robustness of Australia's cyber security regulations. Furthermore, it affords them access to a repository of educational resources and materials, fostering the acquisition of skills necessary for an elevated cybersecurity posture. This groundbreaking initiative was jointly disclosed by Minister for Cyber Security Clare O'Neil and Minister for Small Business Julie Collins.\n\n\n==== India ====\nSome provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000.\nThe National Cyber Security Policy 2013 is a policy framework by the Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard \"information, such as personal information (of web users), financial and banking information and sovereign data\". CERT- In is the nodal agency which monitors the cyber threats in the country. The post of National Cyber Security Coordinator has also been created in the Prime Minister's Office (PMO).\nThe Indian Companies Act 2013 has also introduced cyber law and cybersecurity obligations on the part of Indian directors. Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000 Update in 2013.\n\n\n==== South Korea ====\nFollowing cyberattacks in the first half of 2013, when the government, news media, television stations, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011, and 2012, but Pyongyang denies the accusations.\n\n\n==== United States ====\n\n\n===== Cyber Plan =====\nThe United States has its first fully formed cyber plan in 15 years, as a result of the release of this National Cyber plan. In this policy, the US says it will: Protect the country by keeping networks, systems, functions, and data safe; Promote American wealth by building a strong digital economy and encouraging strong domestic innovation; Peace and safety should be kept by making it easier for the US to stop people from using computer tools for bad things, working with friends and partners to do this; and increase the United States' impact around the world to support the main ideas behind an open, safe, reliable, and compatible Internet. \nThe new U.S. cyber strategy seeks to allay some of those concerns by promoting responsible behavior in cyberspace, urging nations to adhere to a set of norms, both through international law and voluntary standards. It also calls for specific measures to harden U.S. government networks from attacks, like the June 2015 intrusion into the U.S. Office of Personnel Management (OPM), which compromised the records of about 4.2 million current and former government employees. And the strategy calls for the U.S. to continue to name and shame bad cyber actors, calling them out publicly for attacks when possible, along with the use of economic sanctions and diplomatic pressure.\n\n\n===== Legislation =====\nThe 1986 18 U.S.C. \u00a7 1030, the Computer Fraud and Abuse Act is the key legislation. It prohibits unauthorized access or damage of protected computers as defined in 18 U.S.C. \u00a7 1030(e)(2). Although various other measures have been proposed \u2013 none have succeeded.\nIn 2013, executive order 13636 Improving Critical Infrastructure Cybersecurity was signed, which prompted the creation of the NIST Cybersecurity Framework.\nIn response to the Colonial Pipeline ransomware attack President Joe Biden signed Executive Order 14028 on May 12, 2021, to increase software security standards for sales to the government, tighten detection and security on existing systems, improve information sharing and training, establish a Cyber Safety Review Board, and improve incident response.\n\n\n===== Standardized government testing services =====\nThe General Services Administration (GSA) has standardized the penetration test service as a pre-vetted support service, to rapidly address potential vulnerabilities, and stop adversaries before they impact US federal, state and local governments. These services are commonly referred to as Highly Adaptive Cybersecurity Services (HACS).\n\n\n===== Agencies =====\nThe Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division. The division is home to US-CERT operations and the National Cyber Alert System. The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.\nThe third priority of the FBI is to: \"Protect the United States against cyber-based attacks and high-technology crimes\", and they, along with the National White Collar Crime Center (NW3C), and the Bureau of Justice Assistance (BJA) are part of the multi-agency task force, The Internet Crime Complaint Center, also known as IC3.\nIn addition to its own specific duties, the FBI participates alongside non-profit organizations such as InfraGard.\nThe Computer Crime and Intellectual Property Section (CCIPS) operates in the United States Department of Justice Criminal Division. The CCIPS is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks. In 2017, CCIPS published A Framework for a Vulnerability Disclosure Program for Online Systems to help organizations \"clearly describe authorized vulnerability disclosure and discovery conduct, thereby substantially reducing the likelihood that such described activities will result in a civil or criminal violation of law under the Computer Fraud and Abuse Act (18 U.S.C. \u00a7 1030).\"\nThe United States Cyber Command, also known as USCYBERCOM, \"has the mission to direct, synchronize, and coordinate cyberspace planning and operations to defend and advance national interests in collaboration with domestic and international partners.\" It has no role in the protection of civilian networks.\nThe U.S. Federal Communications Commission's role in cybersecurity is to strengthen the protection of critical communications infrastructure, to assist in maintaining the reliability of networks during disasters, to aid in swift recovery after, and to ensure that first responders have access to effective communications services.\nThe Food and Drug Administration has issued guidance for medical devices, and the National Highway Traffic Safety Administration is concerned with automotive cybersecurity. After being criticized by the Government Accountability Office, and following successful attacks on airports and claimed attacks on airplanes, the Federal Aviation Administration has devoted funding to securing systems on board the planes of private manufacturers, and the Aircraft Communications Addressing and Reporting System. Concerns have also been raised about the future Next Generation Air Transportation System.\nThe US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.\n\n\n===== Computer emergency readiness team =====\nComputer emergency response team is a name given to expert groups that handle computer security incidents. In the US, two distinct organizations exist, although they do work closely together.\n\nUS-CERT: part of the National Cyber Security Division of the United States Department of Homeland Security.\nCERT/CC: created by the Defense Advanced Research Projects Agency (DARPA) and run by the Software Engineering Institute (SEI).\n\n\n== U.S. NRC, 10 CFR 73.54 Cybersecurity ==\nIn the context of U.S. nuclear power plants, the U.S. Nuclear Regulatory Commission (NRC) outlines cybersecurity requirements under 10 CFR Part 73, specifically in \u00a773.54.\n\n\n== NEI 08-09: Cybersecurity Plan for Nuclear Power Plants ==\nThe Nuclear Energy Institute's NEI 08-09 document, Cyber Security Plan for Nuclear Power Reactors,  outlines a comprehensive framework for cybersecurity in the nuclear power industry. Drafted with input from the U.S. NRC, this guideline is instrumental in aiding licensees to comply with the Code of Federal Regulations (CFR), which mandates robust protection of digital computers and equipment and communications systems at nuclear power plants against cyber threats.\n\n\n== Modern warfare ==\n\nThere is growing concern that cyberspace will become the next theater of warfare. As Mark Clayton from The Christian Science Monitor wrote in a 2015 article titled \"The New Cyber Arms Race\":\n\nIn the future, wars will not just be fought by soldiers with guns or with planes that drop bombs. They will also be fought with the click of a mouse a half a world away that unleashes carefully weaponized computer programs that disrupt or destroy critical industries like utilities, transportation, communications, and energy. Such attacks could also disable military networks that control the movement of troops, the path of jet fighters, the command and control of warships.\nThis has led to new terms such as cyberwarfare and cyberterrorism. The United States Cyber Command was created in 2009 and many other countries have similar forces.\nThere are a few critical voices that question whether cybersecurity is as significant a threat as it is made out to be.\n\n\n== Careers ==\nCybersecurity is a fast-growing field of IT concerned with reducing organizations' risk of hack or data breaches. According to research from the Enterprise Strategy Group, 46% of organizations say that they have a \"problematic shortage\" of cybersecurity skills in 2016, up from 28% in 2015. Commercial, government and non-governmental organizations all employ cybersecurity professionals. The fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance, health care, and retail. However, the use of the term cybersecurity is more prevalent in government job descriptions.\nTypical cybersecurity job titles and descriptions include:\n\n\n=== Security analyst ===\nAnalyzes and assesses vulnerabilities in the infrastructure (software, hardware, networks), investigates using available tools and countermeasures to remedy the detected vulnerabilities and recommends solutions and best practices. Analyzes and assesses damage to the data/infrastructure as a result of security incidents, examines available recovery tools and processes, and recommends solutions. Tests for compliance with security policies and procedures. May assist in the creation, implementation, or management of security solutions.\n\n\n=== Security engineer ===\nPerforms security monitoring, security and data/logs analysis, and forensic analysis, to detect security incidents, and mount the incident response. Investigates and utilizes new technologies and processes to enhance security capabilities and implement improvements. May also review code or perform other security engineering methodologies.\n\n\n=== Security architect ===\nDesigns a security system or major components of a security system, and may head a security design team building a new security system.\n\n\n=== Chief Information Security Officer (CISO) ===\nA high-level management position responsible for the entire information security division/staff. The position may include hands-on technical work.\n\n\n=== Chief Security Officer (CSO) ===\nA high-level management position responsible for the entire security division/staff. A newer position is now deemed needed as security risks grow.\n\n\n=== Data Protection Officer (DPO) ===\nA DPO is tasked with monitoring compliance with data protection laws (such as GDPR), data protection policies, awareness-raising, training, and audits.\n\n\n=== Security Consultant/Specialist/Intelligence ===\nBroad titles that encompass any one or all of the other roles or titles tasked with protecting computers, networks, software, data or information systems against viruses, worms, spyware, malware, intrusion detection, unauthorized access, denial-of-service attacks, and an ever-increasing list of attacks by hackers acting as individuals or as part of organized crime or foreign governments.\nStudent programs are also available for people interested in beginning a career in cybersecurity. Meanwhile, a flexible and effective option for information security professionals of all experience levels to keep studying is online security training, including webcasts. A wide range of certified courses are also available.\nIn the United Kingdom, a nationwide set of cybersecurity forums, known as the U.K Cyber Security Forum, were established supported by the Government's cybersecurity strategy in order to encourage start-ups and innovation and to address the skills gap identified by the U.K Government.\nIn Singapore, the Cyber Security Agency has issued a Singapore Operational Technology (OT) Cybersecurity Competency Framework (OTCCF). The framework defines emerging cybersecurity roles in Operational Technology. The OTCCF was endorsed by the Infocomm Media Development Authority (IMDA). It outlines the different OT cybersecurity job positions as well as the technical skills and core competencies necessary. It also depicts the many career paths available, including vertical and lateral advancement opportunities.\n\n\n== Terminology ==\nThe following terms used with regards to computer security are explained below:\n\nAccess authorization restricts access to a computer to a group of users through the use of authentication systems. These systems can protect either the whole computer, such as through an interactive login screen, or individual services, such as a FTP server. There are many methods for identifying and authenticating users, such as passwords, identification cards, smart cards, and biometric systems.\nAnti-virus software consists of computer programs that attempt to identify, thwart, and eliminate computer viruses and other malicious software (malware).\nApplications are executable code, so general corporate practice is to restrict or block users the power to install them; to install them only when there is a demonstrated need (e.g. software needed to perform assignments); to install only those which are known to be reputable (preferably with access to the computer code used to create the application,- and to reduce the attack surface by installing as few as possible. They are typically run with least privilege, with a robust process in place to identify, test and install any released security patches or updates for them.\nFor example, programs can be installed into an individual user's account, which limits the program's potential access, as well as being a means control which users have specific exceptions to policy.  In Linux, FreeBSD, OpenBSD, and other Unix-like operating systems there is an option to further restrict an application using chroot or other means of restricting the application to its own 'sandbox'.  For example. Linux provides namespaces, and Cgroups to further restrict the access of an application to system resources.\nGeneralized security frameworks such as SELinux or AppArmor help administrators control access.\nJava and other languages which compile to Java byte code and run in the Java virtual machine can have their access to other applications controlled at the virtual machine level.\nSome software can be run in software containers which can even provide their own set of system libraries, limiting the software's, or anyone controlling it, access to the server's versions of the libraries.\nAuthentication techniques can be used to ensure that communication end-points are who they say they are.\nAutomated theorem proving and other verification tools can be used to enable critical algorithms and code used in secure systems to be mathematically proven to meet their specifications.\nBackups are one or more copies kept of important computer files. Typically, multiple copies will be kept at different locations so that if a copy is stolen or damaged, other copies will still exist.\nCapability and access control list techniques can be used to ensure privilege separation and mandatory access control. Capabilities vs. ACLs discusses their use.\nChain of trust techniques can be used to attempt to ensure that all software loaded has been certified as authentic by the system's designers.\nConfidentiality is the nondisclosure of information except to another authorized person.\nCryptographic techniques can be used to defend data in transit between systems, reducing the probability that the data exchange between systems can be intercepted or modified.\nCyber attribution,  is an attribution of cybercrime, i.e., finding who perpetrated a cyberattack.\nCyberwarfare is an Internet-based conflict that involves politically motivated attacks on information and information systems. Such attacks can, for example, disable official websites and networks, disrupt or disable essential services, steal or alter classified data, and cripple financial systems.\nData integrity is the accuracy and consistency of stored data, indicated by an absence of any alteration in data between two updates of a data record.\n\nEncryption is used to protect the confidentiality of a message. Cryptographically secure ciphers are designed to make any practical attempt of breaking them infeasible. Symmetric-key ciphers are suitable for bulk encryption using shared keys, and public-key encryption using digital certificates can provide a practical solution for the problem of securely communicating when no key is shared in advance.\nEndpoint security software aids networks in preventing malware infection and data theft at network entry points made vulnerable by the prevalence of potentially infected devices such as laptops, mobile devices, and USB drives.\nFirewalls serve as a gatekeeper system between networks, allowing only traffic that matches defined rules. They often include detailed logging, and may include intrusion detection and intrusion prevention features. They are near-universal between company local area networks and the Internet, but can also be used internally to impose traffic rules between networks if network segmentation is configured.\nA hacker is someone who seeks to breach defenses and exploit weaknesses in a computer system or network.\nHoney pots are computers that are intentionally left vulnerable to attack by crackers. They can be used to catch crackers and to identify their techniques.\nIntrusion-detection systems are devices or software applications that monitor networks or systems for malicious activity or policy violations.\nA microkernel is an approach to operating system design which has only the near-minimum amount of code running at the most privileged level \u2013 and runs other elements of the operating system such as device drivers, protocol stacks and file systems, in the safer, less privileged user space.\nPinging. The standard ping application can be used to test if an IP address is in use. If it is, attackers may then try a port scan to detect which services are exposed.\nA port scan is used to probe an IP address for open ports to identify accessible network services and applications.\nA key logger is spyware that silently captures and stores each keystroke that a user types on the computer's keyboard.\nSocial engineering is the use of deception to manipulate individuals to breach security.\nLogic bombs is a type of malware added to a legitimate program that lies dormant until it is triggered by a specific event.\nA unikernel is a computer program that runs on a minimalistic operating system where a single application is allowed to run (as opposed to a general purpose operating system where many applications can run at the same time). This approach to minimizing the attack surface is adopted mostly in cloud environments where software is deployed in virtual machines.\nZero trust security means that no one is trusted by default from inside or outside the network, and verification is required from everyone trying to gain access to resources on the network.\n\n\n== History ==\nSince the Internet's arrival and with the digital transformation initiated in recent years, the notion of cybersecurity has become a familiar subject in both our professional and personal lives. Cybersecurity and cyber threats have been consistently present for the last 60 years of technological change. In the 1970s and 1980s, computer security was mainly limited to academia until the conception of the Internet, where, with increased connectivity, computer viruses and network intrusions began to take off. After the spread of viruses in the 1990s, the 2000s marked the institutionalization of organized attacks such as distributed denial of service. This led to the formalization of cybersecurity as a professional discipline.\nThe April 1967 session organized by Willis Ware at the Spring Joint Computer Conference, and the later publication of the Ware Report, were foundational moments in the history of the field of computer security. Ware's work straddled the intersection of material, cultural, political, and social concerns.\nA 1977 NIST publication introduced the CIA triad of confidentiality, integrity, and availability as a clear and simple way to describe key security goals. While still relevant, many more elaborate frameworks have since been proposed.\nHowever, in the 1970s and 1980s, there were no grave computer threats because computers and the internet were still developing, and security threats were easily identifiable. More often, threats came from malicious insiders who gained unauthorized access to sensitive documents and files. Although malware and network breaches existed during the early years, they did not use them for financial gain. By the second half of the 1970s, established computer firms like IBM started offering commercial access control systems and computer security software products.\nOne of the earliest examples of an attack on a computer network was the computer worm Creeper written by Bob Thomas at BBN, which propagated through the ARPANET in 1971. The program was purely experimental in nature and carried no malicious payload. A later program, Reaper, was created by Ray Tomlinson in 1972 and used to destroy Creeper.\nBetween September 1986 and June 1987, a group of German hackers performed the first documented case of cyber espionage. The group hacked into American defense contractors, universities, and military base networks and sold gathered information to the Soviet KGB. The group was led by Markus Hess, who was arrested on 29 June 1987. He was convicted of espionage (along with two co-conspirators) on 15 Feb 1990.\nIn 1988, one of the first computer worms, called the Morris worm, was distributed via the Internet. It gained significant mainstream media attention.\nIn 1993, Netscape started developing the protocol SSL, shortly after the National Center for Supercomputing Applications (NCSA) launched Mosaic 1.0, the first web browser, in 1993. Netscape had SSL version 1.0 ready in 1994, but it was never released to the public due to many serious security vulnerabilities. These weaknesses included replay attacks and a vulnerability that allowed hackers to alter unencrypted communications sent by users. However, in February 1995, Netscape launched Version 2.0.\nThe National Security Agency (NSA) is responsible for the protection of U.S. information systems and also for collecting foreign intelligence. The agency analyzes commonly used software and system configurations to find security flaws, which it can use for offensive purposes against competitors of the United States.\nNSA contractors created and sold click-and-shoot attack tools to US agencies and close allies, but eventually, the tools made their way to foreign adversaries. In 2016, NSAs own hacking tools were hacked, and they have been used by Russia and North Korea. NSA's employees and contractors have been recruited at high salaries by adversaries, anxious to compete in cyberwarfare. In 2007, the United States and Israel began exploiting security flaws in the Microsoft Windows operating system to attack and damage equipment used in Iran to refine nuclear materials. Iran responded by heavily investing in their own cyberwarfare capability, which it began using against the United States.\n\n\n== Notable scholars ==\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\nBranch, Jordan (24 September 2020). \"What's in a Name? Metaphors and Cybersecurity\". International Organization. 75 (1). Cambridge University Press (CUP): 39\u201370. doi:10.1017/s002081832000051x. ISSN 0020-8183. S2CID 224886794.\nCostigan, Sean; Hennessy, Michael (2016). Cybersecurity: A Generic Reference Curriculum (PDF). NATO. ISBN 978-9284501960. Archived (PDF) from the original on 10 March 2017.\nFuller, Christopher J (11 June 2018). \"The Roots of the United States' Cyber (In)Security\" (DOC). Diplomatic History. 43 (1). Oxford University Press (OUP): 157\u2013185. doi:10.1093/dh/dhy038. ISSN 0145-2096.\nBob, Yonah Jeremy (21 August 2021). \"Ex-IDF cyber intel. official reveals secrets behind cyber offense\". The Jerusalem Post.\nKim, Peter (2014). The Hacker Playbook: Practical Guide To Penetration Testing. Seattle: CreateSpace Independent Publishing Platform. ISBN 978-1494932633.\nLee, Newton (2015). Counterterrorism and Cybersecurity: Total Information Awareness (2nd ed.). Springer. ISBN 978-3319172439.\nMontagnani, Maria Lill\u00e0; Cavallo, Mirta Antonella (2018). \"Cybersecurity and Liability in a Big Data World\". Market and Competition Law Review. 2 (2). Elsevier BV: 71\u201398. doi:10.2139/ssrn.3220475. ISSN 1556-5068. S2CID 216704215. SSRN 3220475.\nShariati, Marzieh; Bahmani, Faezeh; Shams, Fereidoon (2011). \"Enterprise information security, a review of architectures and frameworks from interoperability perspective\". Procedia Computer Science. 3. Elsevier BV: 537\u2013543. doi:10.1016/j.procs.2010.12.089. ISSN 1877-0509.\nSinger, P. W.; Friedman, Allan (2014). Cybersecurity and Cyberwar: What Everyone Needs to Know. Oxford University Press. ISBN 978-0199918119.\nWu, Chwan-Hwa (John); Irwin, J. David (2013). Introduction to Computer Networks and Cybersecurity. Boca Raton: CRC Press. ISBN 978-1466572133.\nCybersecurity Best Practices | Cybersecurity and Infrastructure Security Agency CISA. (n.d.). Retrieved April 24, 2024, from https://www.cisa.gov/topics/cybersecurity-best-practices\nSztyber-Betley, A., Syfert, M., Ko\u015bcielny, J. M., & G\u00f3recka, Z. (2023). Controller Cyber-Attack Detection and Isolation \u2020: Sensors (14248220). Sensors (14248220), 23(5), 2778. doi:10.3390/s23052778", "link": "https://en.wikipedia.org/wiki/Computer_security"}, "NIST Cybersecurity Framework": {"title": "NIST Cybersecurity Framework", "content": "The NIST Cybersecurity Framework (CSF) is a set of voluntary guidelines designed to help organizations assess and improve their ability to prevent, detect, and respond to cybersecurity risks. Developed by the U.S. National Institute of Standards and Technology (NIST), the framework was initially published in 2014 for critical infrastructure sectors but has since been widely adopted across various industries, including government and private enterprises globally. The framework integrates existing standards, guidelines, and best practices to provide a structured approach to cybersecurity risk management.\nThe CSF is composed of three primary components: the Core, Implementation Tiers, and Profiles. The Core outlines five key cybersecurity functions\u2014Identify, Protect, Detect, Respond, and Recover\u2014each of which is further divided into specific categories and subcategories. These functions offer a high-level, outcome-driven approach to managing cybersecurity risks. The Implementation Tiers help organizations assess the sophistication of their cybersecurity practices, while the Profiles allow for customization based on an organization's unique risk profile and needs.\nSince its inception, the CSF has undergone several updates to reflect the evolving nature of cybersecurity. Version 1.1, released in 2018, introduced enhancements related to supply chain risk management and self-assessment processes. The most recent update, Version 2.0, was published in 2024, expanding the framework\u2019s applicability and adding new guidance on cybersecurity governance and continuous improvement practices.\nThe NIST Cybersecurity Framework is used internationally and has been translated into multiple languages. It serves as a benchmark for cybersecurity standards, helping organizations align their practices with recognized global standards, such as ISO/IEC 27001 and COBIT. While widely praised, the framework has been criticized for the cost and complexity involved in its implementation, particularly for small and medium-sized enterprises.\n\n\n== Overview ==\nThe NIST Cybersecurity Framework (CSF) is a set of guidelines developed by the U.S. National Institute of Standards and Technology (NIST) to help organizations manage and mitigate cybersecurity risks. It draws from existing standards, guidelines, and best practices to provide a flexible and scalable approach to cybersecurity. The framework provides a high-level taxonomy of cybersecurity outcomes and offers a methodology for assessing and managing those outcomes. Additionally, it addresses the protection of privacy and civil liberties in a cybersecurity context. \nThe CSF has been translated into multiple languages and is widely used by governments, businesses, and organizations across various sectors. According to a 2016 survey, 70% of organizations view the NIST Cybersecurity Framework as a best practice for computer security, though some have noted that implementation can require significant investment.\nThe framework is designed to be flexible and adaptable, providing high-level guidance that allows individual organizations to determine the specifics of implementation based on their unique needs and risk profiles.\nVersion 1.0 of the framework was published in 2014, primarily targeting operators of critical infrastructure. A public draft of Version 1.1 was released for comment in 2017, and the final version was published on April 16, 2018. Version 1.1 retained compatibility with the original framework while introducing additional guidance on areas such as supply chain risk management. Version 2.0, released in 2024, further expanded the framework's scope and introduced new guidelines on self-assessment and cybersecurity governance.\nThe framework consists of three main components: the \"Core,\" \"Profiles,\" and \"Tiers.\" The Core provides a comprehensive set of activities, outcomes, and references related to various aspects of cybersecurity. The Implementation Tiers help organizations assess their cybersecurity practices and sophistication, while the Profiles allow organizations to tailor the framework to their specific requirements and risk assessments.\nOrganizations typically start by developing a \"Current Profile\" to describe their existing cybersecurity practices and outcomes. From there, they can create a \"Target Profile\" to outline the desired future state and define the steps needed to achieve it. Alternatively, organizations can adopt a baseline profile based on their sector or specific industry needs. \nResearch indicates that the NIST Cybersecurity Framework has the potential to influence cybersecurity standards both within the United States and internationally, particularly in sectors where formal cybersecurity standards are still emerging. This influence could foster better international cybersecurity practices, benefiting businesses that operate across borders and contributing to global cybersecurity efforts.\n\n\n== Functions and categories of cybersecurity activities ==\n\nThe NIST Cybersecurity Framework organizes its \"core\" material into five \"functions\" which are subdivided into a total of 23 \"categories\". For each category, it defines a number of subcategories of cybersecurity outcomes and security controls, with 108 subcategories in all.\nFor each subcategory, it also provides \"Informative Resources\" referencing specific sections of a variety of other information security standards, including ISO 27001, COBIT, NIST SP 800-53, ANSI/ISA-62443, and the Council on CyberSecurity Critical Security Controls (CCS CSC, now managed by the Center for Internet Security).  Special Publications (SP) aside, most of the informative references requires a paid membership or purchase to access their respective guides.  The cost and complexity of the framework has resulted in bills from both houses of Congress that direct NIST to create Cybersecurity Framework guides that are more accessible to small and medium businesses.\nHere are the functions and categories, along with their unique identifiers and definitions, as stated in the framework document.\n\n\n=== Identify ===\n\"Develop the organizational understanding to manage cybersecurity risk to systems, assets, data, and capabilities.\"\n\nAsset Management (ID.AM): The data, personnel, devices, systems, and facilities that enable the organization to achieve business purposes are identified and managed consistent with their relative importance to business objectives and the organization's risk strategy.\nBusiness Environment (ID.BE): The organization's mission, objectives, stakeholders, and activities are understood and prioritized; this information is used to inform cybersecurity roles, responsibilities, and risk management decisions.\nGovernance (ID.GV):- The policies, procedures, and processes to manage and monitor the organization's regulatory, legal, risk, environmental, and operational requirements are understood and inform the management of cybersecurity risk.\nRisk Assessment (ID.RA): The organization understands the cybersecurity risk to organizational operations (including mission, functions, image, or reputation), organizational assets, and individuals.\nRisk Management Strategy (ID.RM): The organization's priorities, constraints, risk tolerances, and assumptions are established and used to support operational risk decisions.\nSupply Chain Risk Management (ID.SC): The organization's priorities, constraints, risk tolerances, and assumptions are established and used to support risk decisions associated with managing supply chain risk. The organization has in place the processes to identify, assess and manage supply chain risks.\n\n\n=== Protect ===\n\"Develop and implement the appropriate safeguards to ensure delivery of critical infrastructure services.\"\n\nAccess Control (PR.AC): Access to assets and associated facilities is limited to authorized users, processes, or devices, and to authorized activities and transactions.\nAwareness and Training (PR.AT): The organization's personnel and partners are provided cybersecurity awareness education and are adequately trained to perform their information security-related duties and responsibilities consistent with related policies, procedures, and agreements.\nData Security (PR.DS): Information and records (data) are managed consistent  with the organization's risk strategy to protect the confidentiality, integrity, and availability of information.\nInformation Protection Processes and Procedures (PR.IP): Security policies (that address purpose, scope, roles, responsibilities, management commitment, and coordination among organizational entities), processes, and procedures are maintained and used to manage protection of information systems and assets.\nMaintenance (PR.MA): Maintenance and repairs of industrial control and information system components is performed consistent with policies and procedures.\nProtective Technology (PR.PT): Technical security solutions are managed to ensure the security and resilience of systems and assets, consistent with related policies, procedures, and agreements.\n\n\n=== Detect ===\n\"Develop and implement the appropriate activities to identify the occurrence of a cybersecurity event.\"\n\nAnomalies and Events (DE.AE): Anomalous activity is detected in a timely manner and the potential impact of events is understood.\nSecurity Continuous Monitoring (DE.CM): The information system and assets are monitored at discrete intervals to identify cybersecurity events and verify the effectiveness of protective measures.\nDetection Processes (DE.DP): Detection processes and procedures are maintained and tested to ensure timely and adequate awareness of anomalous events.\n\n\n=== Respond ===\n\"Develop and implement the appropriate activities to take action regarding a detected cybersecurity incident.\"\n\nResponse Planning (RS.RP): Response processes and procedures are executed and maintained, to ensure timely response to detected cybersecurity events.\nCommunications (RS.CO): Response activities are coordinated with internal and external stakeholders, as appropriate, to include external support from law enforcement agencies.\nAnalysis (RS.AN): Analysis is conducted to ensure adequate response and support recovery activities.\nMitigation (RS.MI): Activities are performed to prevent expansion of an event, mitigate its effects, and eradicate the incident.\nImprovements (RS.IM): Organizational response activities are improved by incorporating lessons learned from current and previous detection/response activities.\n\n\n=== Recover ===\n\"Develop and implement the appropriate activities to maintain plans for resilience and to restore any capabilities or services that were impaired due to a cybersecurity incident.\"\n\nRecovery Planning (RC.RP): Recovery processes and procedures are executed and maintained to ensure timely restoration of systems or assets affected by cybersecurity events.\nImprovements (RC.IM): Recovery planning and processes are improved by incorporating lessons learned into future activities.\nCommunications (RC.CO): Restoration activities are coordinated with internal and external parties, such as coordinating centers, Internet Service Providers, owners of attacking systems, victims, other CSIRTs, and vendors.\n\n\n== Updates ==\nIn 2021 NIST released Security Measures for \"EO-Critical Software\" Use Under Executive Order (EO) 14028 to outline security measures intended to better protect the use of deployed EO-critical software in agencies\u2019 operational environments.\n\n\n=== Journey to CSF 2.0 ===\nThe NIST Cybersecurity Framework is meant to be a living document, meaning it will be updated and improved over time to keep up with changes in technology and cybersecurity threats, as well as to integrate best-practices and lessons learned. Since releasing version 1.1 in 2018, stakeholders have provided feedback that the CSF needed to be updated. In February 2022, NIST released a request for information on ways to improve the CSF, and released a subsequent concept paper in January of 2023 with proposed changes. Most recently, NIST released its Discussion Draft: The NIST Cybersecurity Framework 2.0 Core with Implementation Examples and has requested public comments be submitted by November 4, 2023. \n\n\n==== Main Changes ====\nThe following is a list of the major changes to the framework from version 1.1 to 2.0:\n\nThe title of the framework has changed from \"Framework for Improving Critical Infrastructure Cybersecurity\" to \"Cybersecurity Framework\". The scope of the framework has been updated to reflect the large population of organizations that use the framework.\nImplementation examples have been added to provide practical and action-oriented processes to help users achieve the CSF subcategories. Additionally, the framework Profiles have been revised and expanded to demonstrate the various purposes of the profiles.\nA new Function, Govern, has been added to provide organizational context and the roles and responsibilities associated with developing a cybersecurity governance model. There is also an additional category in this Function focused on cybersecurity supply chain risk management.\nThe latest update also provides greater information on cybersecurity assessments by placing greater importance on the continuous improvement of security through a new Improvement Category in the Identify Function.\n\n\n== See also ==\nCybersecurity\nCyber security standards\nPrivacy\nCritical infrastructure protection\nISO/IEC 27001:2013: an information security standard from the International Organization for Standardization\nCOBIT: Control Objectives for Information and Related Technologies - a related framework from ISACA\nNIST Special Publication 800-53: \"Security and Privacy Controls for Federal Information Systems and Organizations.\"\nRisk Management Framework - US-federally mandated risk framework (broader scope than cybersecurity)\n\n\n== References ==\n This article incorporates public domain material from NIST Cybersecurity Framework (PDF). National Institute of Standards and Technology.\n\n\n== External links ==\nOfficial website\nHarnessing the Power of the NIST Cybersecurity Framework\nNISTIR 8374 (Draft): Cybersecurity Framework Profile for Ransomware Risk Management (Preliminary Draft)\nInformative References Home\nDerived Relationship Mapping\nInformative Reference Catalog", "link": "https://en.wikipedia.org/wiki/NIST_Cybersecurity_Framework"}, "Cybersecurity and Infrastructure Security Agency": {"title": "Cybersecurity and Infrastructure Security Agency", "content": "The Cybersecurity and Infrastructure Security Agency (CISA) is a component of the United States Department of Homeland Security (DHS) responsible for cybersecurity and infrastructure protection across all levels of government, coordinating cybersecurity programs with U.S. states, and improving the government's cybersecurity protections against private and nation-state hackers. \nThe agency began in 2007 as the DHS National Protection and Programs Directorate. With the Cybersecurity and Infrastructure Security Agency Act of 2018, CISA's footprint grew to include roles in securing elections and the census, managing National Special Security Events, and the U.S. response to the COVID-19 Pandemic. It has also been involved in 5G network security and hardening the US grid against electromagnetic pulses (EMPs). The Office for Bombing Prevention leads the national counter-IED effort.\nCurrently headquartered in Arlington, Virginia, in 2025 CISA is planning to move its headquarters along with 6,500 employees to a new 10 story, 620,000 sq ft building on the consolidated DHS St. Elizabeths campus headquarters.\n\n\n== History ==\nThe National Protection and Programs Directorate (NPPD) was formed in 2007 as a component of the United States Department of Homeland Security. NPPD's goal was to advance the Department's national security mission by reducing and eliminating threats to U.S. critical physical and cyber infrastructure.\nOn November 16, 2018, President Trump signed into law the Cybersecurity and Infrastructure Security Agency Act of 2018, which elevated the mission of the former NPPD within DHS, establishing the Cybersecurity and Infrastructure Security Agency (CISA). CISA is a successor agency to NPPD, and assists both other government agencies and private sector organizations in addressing cybersecurity issues. Former NPPD Under-Secretary Christopher Krebs was CISA's first Director, and former Deputy Under-Secretary Matthew Travis was its first deputy director.\nOn January 22, 2019, CISA issued its first Emergency Directive (19-01: Mitigate DNS Infrastructure Tampering) warning that \"an active attacker is targeting government organizations\" using DNS spoofing techniques to perform man-in-the-middle attacks. Research group FireEye stated that \"initial research suggests the actor or actors responsible have a nexus to Iran.\"\nIn 2020, CISA created a website, titled Rumor Control, to rebut disinformation associated with the 2020 United States presidential election. On November 12, 2020, CISA issued a press release asserting, \"There is no evidence that any voting system deleted or lost votes, changed votes, or was in any way compromised.\" On the same day, Director Krebs indicated that he expected to be dismissed from his post by the Trump administration. Krebs was subsequently fired by President Trump on November 17, 2020 via tweet for his comments regarding the security of the election.\n\nOn July 12, 2021, the Senate confirmed Jen Easterly by a voice vote. Easterly's nomination had been reported favorably out of Senate Committee on Homeland Security and Governmental Affairs on June 16, but a floor vote had been reportedly held (delayed) by Senator Rick Scott over broader national security concerns, until the President or Vice President had visited the southern border with Mexico. Easterly hired new staff to monitor online disinformation to enhance what she called the nation's \"cognitive infrastructure\" and utilized the existing rumor control website during the 2021 elections.\nIn September 2022, CISA released their 2023\u20132025 CISA Strategic Plan, the first comprehensive strategy document since the agency was established in 2018.\n\n\n== Organization ==\n\nCISA divisions include the:\n\nCybersecurity Division\nNational Cybersecurity and Communications Integration Center\nCapacity Building\nJoint Cyber Defense Collaborative\nMission Engineering\nOffice of the Technical Director\nThreat Hunting\nVulnerability Management\nInfrastructure Security Division\nBombing Prevention\nChemical Security\nExercises\nInfrastructure Assessment & Analysis\nSchool Safety\nStrategy, Performance & Resources\nEmergency Communications Division\nNational Risk Management Center\nIntegrated Operations Division\nRegions 1 through 10\nStakeholder Engagement Division\nCouncil Management\nInternational\nSector Management\nStrategic Relations\n\n\n== Programs ==\nThe Continuous Diagnostics and Mitigations program provides cybersecurity tools and services to federal agencies.\nCISA issues \"binding operational directives\" that require federal government agencies to take action against specific cybersecurity risks. \nIn March 2021, CISA assumed control of the .gov top-level domain (TLD) from the General Services Administration. CISA manages the approval of domains and operates the TLD Domain Name System nameservers. In April 2021, CISA removed the fee for registering domains. In January 2023, Cloudflare received a $7.2M contract to provide DNS registry and hosting services for the TLD.\nCISA provides incident response services to the federal executive branch and US-based entities.  \nCISA manages the EINSTEIN intrusion detection system to detect malicious activity on federal government agency networks.  \nThe National Defense Authorization Act for Fiscal Year 2021 granted CISA the authority to issue administrative subpoenas in order to identify the owners of internet connected critical infrastructure related devices with specific vulnerabilities. In 2021, CISA issued 47 subpoenas.\nIn August 2021, Easterly stated \"One could argue we\u2019re in the business of critical infrastructure, and the most critical infrastructure is our cognitive infrastructure, so building that resilience to misinformation and disinformation, I think, is incredibly important.\"\nIn 2021, CISA released a report that provided guidance for how to navigate and prevent ransomware incidents. This was due to a significant jump in recent attacks related to ransomware.\n\n\n== Committees ==\n\n\n=== Cybersecurity Advisory Committee ===\nIn 2021, the Agency created the Cybersecurity Advisory Committee with the following members:\n\nSteve Adler, Mayor, City of Austin, Texas\nMarene Allison, Chief Information Security Officer, Johnson & Johnson\nLori Beer, Chief Information Officer, JPMorgan Chase\nRobert Chesney, James A. Baker III Chair in the Rule of Law and World Affairs, University of Texas School of Law\nThomas Fanning, Chairman, President and CEO, Southern Company\nVijaya Gadde\nPatrick D. Gallagher, Chancellor, University of Pittsburgh\nRonald Green, Executive Vice President and Chief Security Officer, Mastercard\nNiloofar Razi Howe, Board Member, Tenable\nKevin Mandia, Chief Executive Officer, Mandiant\nJeff Moss, President, DEF CON Communications\nNuala O\u2019Connor, Senior Vice President & Chief Counsel, Digital Citizenship, Walmart\nNicole Perlroth, Cybersecurity journalist\nMatthew Prince, Chief Executive Officer, Cloudflare\nTed Schlein, General Partner, Kleiner Perkins; and Caufield & Byers\nStephen Schmidt, Chief Information Security Officer, Amazon Web Services\nSuzanne Spaulding, Senior Advisor for Homeland Security, CSIS\nAlex Stamos, Partner, Krebs Stamos Group\nKate Starbird, Associate Professor, Human Centered Design & Engineering, University of Washington\nGeorge Stathakopoulos, Vice President of Corporate Information Security, Apple\nAlicia Tate-Nadeau (ARNG-Ret.), Director, Illinois Emergency Management Agency\nNicole Wong, Principal, NWong Strategies\nChris Young, Executive Vice President of Business Development, Strategy, and Ventures, Microsoft\n\n\n== Directors ==\n\n\n== See also ==\nList of federal agencies in the United States\nFlorida Digital Service\n\n\n== References ==\n\n\n== External links ==\nOfficial website", "link": "https://en.wikipedia.org/wiki/Cybersecurity_and_Infrastructure_Security_Agency"}, "Capture the flag (cybersecurity)": {"title": "Capture the flag (cybersecurity)", "content": "Capture the Flag (CTF) in computer security is an exercise in which participants attempt to find text strings, called \"flags\", which are secretly hidden in purposefully-vulnerable programs or websites. They can be used for both competitive or educational purposes. In two main variations of CTFs, participants either steal flags from other participants (attack/defense-style CTFs) or from organizers (jeopardy-style challenges). A mixed competition combines these two styles. Competitions can include hiding flags in hardware devices, they can be both online or in-person, and can be advanced or entry-level. The game is inspired by the traditional outdoor sport of the same name. CTFs are used as a tool for developing and refining cybersecurity skills, making them popular in both professional and academic settings\n\n\n== Overview ==\nCapture the Flag (CTF) is a cybersecurity competition that is used to test and develop computer security skills. It was first developed in 1996 at DEF CON, the largest cybersecurity conference in the United States which is hosted annually in Las Vegas, Nevada. The conference hosts a weekend of cybersecurity competitions, including their flagship CTF.\nTwo popular CTF formats are jeopardy and attack-defense. Both formats test participant\u2019s knowledge in cybersecurity, but differ in objective. In the Jeopardy format, participating teams must complete as many challenges of varying point values from a various categories such as cryptography, web exploitation, and reverse engineering. In the attack-defense format, competing teams must defend their vulnerable computer systems while attacking their opponent's systems.\nThe exercise involves a diverse array of tasks, including exploitation and cracking passwords, but there is little evidence showing how these tasks translate into cybersecurity knowledge held by security experts. Recent research has shown that the Capture the Flag tasks mainly covered technical knowledge but lacked social topics like social engineering and awareness on cybersecurity. \n\n\n== Educational applications ==\nCTFs have been shown to be an effective way to improve cybersecurity education through gamification. There are many examples of CTFs designed to teach cybersecurity skills to a wide variety of audiences, including PicoCTF, organized by the Carnegie Mellon CyLab, which is oriented towards high school students, and Arizona State University supported pwn.college. Beyond educational CTF events and resources, CTFs has been shown to be a highly effective way to instill cybersecurity concepts in the classroom. CTFs have been included in undergraduate computer science classes such as Introduction to Information Security at the National University of Singapore. CTFs are also popular in military academies. They are often included as part of the curriculum for cybersecurity courses, with the NSA organized Cyber Exercise culminating in a CTF competition between the US service academies and military colleges.\n\n\n== Competitions ==\nMany CTF organizers register their competition with the CTFtime platform. This allows the tracking of the position of teams over time and across competitions.. These include \"Plaid Parliament of Pwning\", \"More Smoked Leet Chicken\", \"Dragon Sector\", \"dcua\", \"Eat, Sleep, Pwn, Repeat\", \"perfect blue\", \"organizers\" and \"Blue Water\". Overall the \"Plaid Parliament of Pwning\" and \"Dragon Sector\" have both placed first worldwide the most with three times each.\n\n\n=== Community competitions ===\nEvery year there are dozens of CTFs organized in a variety of formats. Many CTFs are associated with cybersecurity conferences such as DEF CON, HITCON, and BSides. The DEF CON CTF, an attack-defence CTF, is notable for being one of the oldest CTF competitions to exist, and has been variously referred to as the \"World Series\", \"Superbowl\",  and \"Olympics\", of hacking by media outlets. The NYU Tandon hosted Cybersecurity Awareness Worldwide (CSAW) CTF is one of the largest open-entry competitions for students learning cybersecurity from around the world. In 2021, it hosted over 1200 teams during the qualification round.\nIn addition to conference organized CTFs, many CTF clubs and teams organize CTF competitions.  Many CTF clubs and teams are associated with universities, such as the CMU associated Plaid Parliament of Pwning, which hosts PlaidCTF, and the ASU associated Shellphish.\n\n\n=== Government-supported competitions ===\nGovernmentally supported CTF competitions include the DARPA Cyber Grand Challenge and ENISA European Cybersecurity Challenge. In 2023, the US Space Force-sponsored Hack-a-Sat CTF competition included, for the first time, a live orbital satellite for participants to exploit.\n\n\n=== Corporate-supported competitions ===\nCorporations and other organizations sometimes use CTFs as a training or evaluation exercise. The benefits of CTFs are similar to those of using CTFs in an educational environment. In addition to internal CTF exercises, some corporations such as Google and Tencent host publicly accessible CTF competitions.\n\n\n== In popular culture ==\nIn Mr. Robot, a qualification round for the DEF CON CTF competition is depicted in the season 3 opener \"eps3.0_power-saver-mode.h\"\nIn The Undeclared War, a CTF is depicted in the opening scene of the series as a recruitment exercise used by GCHQ\nGo Go Squid!, a Chinese television series, is based around training for and competing in highly stylized CTF competitions .\n\n\n== See also ==\nWargame (hacking)\nCyberwarfare preparedness\nHackathons\nCompetitive programming\nCybersecurity in popular culture\nPrivacy\n\n\n== References ==\n\n\n== External links ==\nctftime.org - an archive of historic, current, and future CTF competitions.", "link": "https://en.wikipedia.org/wiki/Capture_the_flag_(cybersecurity)"}, "Journal of Cybersecurity": {"title": "Journal of Cybersecurity", "content": "The Journal of Cybersecurity is an open access peer reviewed academic journal of cybersecurity. It is published by Oxford University Press. It was first issued in 2015.\nIts editors in chief are Tyler Moore and David Pym.\nThe journal is a member of the Committee on Publication Ethics (COPE). The journal concentrates on the belief that computer science approaches are critical, but are not enough to tackle cybersecurity threats. Moreover, the article maintains the belief that interdisciplinary academic contributions are needed to understand the different facets of cybersecurity.\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Journal_of_Cybersecurity"}, "Richard Forno": {"title": "Richard Forno", "content": "Richard Forno is a consultant, lecturer, and writer in the area of Washington, DC.\n\n\n== Education ==\nForno earned a Bachelor's degree in international relations from American University in 1994, a master's degree in international relations from Salve Regina University in 2002, and a PhD degree in Internet studies from Curtin University of Technology in 2010. He is also a graduate of Valley Forge Military Academy and College and the United States Naval War College.\n\n\n== Work ==\nDr. Forno is Graduate Program Director for University of Maryland's Cybersecurity Program, co-founder of the Maryland Cyber Challenge & Conference, and a visiting scientist at the Software Engineering Institute of Carnegie Mellon University. At one time, Forno was the Chief Information Security Officer for Network Solutions.\nForno writes and publishes on his web site articles on technology, computer security, and politics, roughly at the rate of one every two weeks. An example of his articles is \"The Joke of Federal Cybersecurity Oversight\".\n\n\n== The Joke of Federal Cybersecurity Oversight ==\nDated March 2004. Toward the beginning of the essay, Forno lists a series of news articles, mostly from CNET News.com, that describe inadequacies in the Federal Government's computer security. (He also notes his opinion that the mass media, in its drive to attract attention, tends to ignore news of positive developments in Federal computer security.)\nIn the rest of the essay, Forno criticizes the Government for insecure administration of information technology.\n\n\"Granted, popular enterprise technology is nowhere as secure as it should be, but today's federal cybersecurity woes result more from flawed technology management practices than flawed technology. To that end, we need to foster and reward innovative, effective management processes in the federal computer security arena and terminate the current technology management and oversight philosophy that tolerates and rewards idleness and mediocrity while doing little to actually eliminate them.\"\n\n\n== References ==\n\nOther", "link": "https://en.wikipedia.org/wiki/Richard_Forno"}, "Cyber-security regulation": {"title": "Cyber-security regulation", "content": "A cybersecurity regulation comprises directives that safeguard information technology and computer systems with the purpose of forcing companies and organizations to protect their systems and information from cyberattacks like viruses, worms, Trojan horses, phishing, denial of service (DOS) attacks, unauthorized access (stealing intellectual property or confidential information) and control system attacks.[1] While cybersecurity regulations aim to minimize cyber risks and enhance protection, the uncertainty arising from frequent changes or new regulations can significantly impact organizational response strategies. \nThere are numerous measures available to prevent cyberattacks. Cybersecurity measures include firewalls, anti-virus software, intrusion detection and prevention systems, encryption, and login passwords.[2] There have been attempts to improve cybersecurity through regulation and collaborative efforts between the government and the private sector to encourage voluntary improvements to cybersecurity. Industry regulators, including banking regulators, have taken notice of the risk from cybersecurity and have either begun or planned to begin to include cybersecurity as an aspect of regulatory examinations.\nRecent research suggests there is also a lack of cyber-security regulation and enforcement in maritime businesses, including the digital connectivity between ships and ports.\n\n\n== Background ==\nIn 2011 the US DoD released a guidance called the Department of Defense Strategy for Operating in Cyberspace which articulated five goals: to treat cyberspace as an operational domain, to employ new defensive concepts to protect DoD networks and systems, to partner with other agencies and the private sector in pursuit of a \"whole-of-government cybersecurity Strategy\", to work with international allies in support of collective cybersecurity and to support the development of a cyber workforce capable of rapid technological innovation. A March 2011 GAO report \"identified protecting the federal government's information systems and the nation's cyber critical infrastructure as a governmentwide high-risk area\" noting that federal information security had been designated a high-risk area since 1997. As of 2003 systems protecting critical infrastructure, called cyber critical infrastructure protection of cyber CIP has also been included.\nIn November 2013, the DoD put forward the new cybersecurity rule (78 Fed. Reg. 69373), which imposed certain requirements on contractors: compliance with certain NIST IT standards, mandatory reporting of cybersecurity incidents to the DoD, and a \"flow-down\" clause that applies the same requirements to subcontractors.\nA June 2013 Congressional report found there were over 50 statutes relevant to cybersecurity compliance. The Federal Information Security Management Act of 2002 (FISMA) is one of the key statutes governing federal cybersecurity regulations.\n\n\n== United States ==\n\n\n=== Federal government ===\nThere are few federal cybersecurity regulations and the ones that exist focus on specific industries. The three main cybersecurity regulations are the 1996 Health Insurance Portability and Accountability Act (HIPAA), the 1999 Gramm-Leach-Bliley Act, and the 2002 Homeland Security Act, which included the Federal Information Security Management Act (FISMA). The three regulations mandate that healthcare organizations, financial institutions, and federal agencies should protect their systems and information.[3] For example, FISMA, which applies to every government agency, \"requires the development and implementation of mandatory policies, principles, standards, and guidelines on information security.\" However, the regulations do not address numerous computer-related industries, such as Internet Service Providers (ISPs) and software companies.[4] Furthermore, the regulations do not specify what cybersecurity measures must be implemented and require only a \"reasonable\" level of security. The vague language of these regulations leaves much room for interpretation.  Bruce Schneier, the founder of Cupertino's Counterpane Internet Security, argues that companies will not make sufficient investments in cybersecurity unless the government forces them to do so.[5] He also states that successful cyberattacks on government systems still occur despite government efforts.[6]\nIt has been suggested that the Data Quality Act already provides the Office of Management and Budget the statutory authority to implement critical infrastructure protection regulations by the Administrative Procedure Act rulemaking process. The idea has not been fully vetted and would require additional legal analysis before a rulemaking could begin.\n\n\n=== State governments ===\nState governments have attempted to improve cybersecurity by increasing public visibility of firms with weak security. In 2003, California passed the Notice of Security Breach Act, which requires that any company that maintains personal information of California citizens and has a security breach must disclose the details of the event. Personal information includes name, social security number, driver's license number, credit card number or financial information.[7] Several other states have followed California's example and passed similar security breach notification regulations.[8] Such security breach notification regulations punish firms for their cybersecurity failures while giving them the freedom to choose how to secure their systems. Also, the regulation creates an incentive for companies to voluntarily invest in cybersecurity to avoid the potential loss of reputation and the resulting economic loss that can come from a successful cyber attack.\nIn 2004, the California State Legislature passed California Assembly Bill 1950, which also applies to businesses that own or maintain personal information for California residents. The regulation dictates for businesses to maintain a reasonable level of security and that they required security practices also extend to business partners.[9] The regulation is an improvement on the federal standard because it expands the number of firms required to maintain an acceptable standard of cybersecurity. However, like the federal legislation, it requires a \"reasonable\" level of cybersecurity, which leaves much room for interpretation until case law is established.[10]\n\n\n=== Proposed regulation ===\nThe US Congress has proposed numerous bills that expand upon cybersecurity regulation. The Consumer Data Security and Notification Act amends the Gramm-Leach-Bliley Act to require disclosure of security breaches by financial institutions. Congressmen have also proposed \"expanding Gramm-Leach-Bliley to all industries that touch consumer financial information, including any firm that accepts payment by a credit card.\"[11] Congress has proposed cybersecurity regulations similar to California's Notice of Security Breach Act for companies that maintain personal information. The Information Protection and Security Act requires that data brokers \"ensure data accuracy and confidentiality, authenticate and track users, detect and prevent unauthorized activity, and mitigate potential harm to individuals.\"[12]\nIn addition to requiring companies to improve cybersecurity, Congress is also considering bills that criminalize cyberattacks. The Securely Protect Yourself Against Cyber Trespass Act (SPY ACT) was a bill of this type. It focused on phishing and spyware bill and was passed on May 23, 2005, in the US House of Representatives but died in the US Senate. The bill \"makes unlawful the unauthorized usage of a computer to take control of it, modify its setting, collect or induce the owner to disclose personally identifiable information, install unsolicited software, and tamper with security, anti-spyware, or anti-virus software.\"[13]\nOn May 12, 2011, US president Barack Obama proposed a package of cybersecurity legislative reforms to improve the security of US persons, the federal government, and critical infrastructure. A year of public debate and Congress hearings followed, resulting in the House of Representative passing an information sharing bill and the Senate developing a compromise bill seeking to balance national security, privacy, and business interests.\nIn July 2012, the Cybersecurity Act of 2012 was proposed by Senators Joseph Lieberman and Susan Collins.[14] The bill would have required creating voluntary \"best practice standards\" for protection of key infrastructure from cyber attacks, which businesses would be encouraged to adopt through incentives such as liability protection.[15] The bill was put to a vote in the Senate but failed to pass.[16] Obama had voiced his support for the Act in a Wall Street Journal op-ed[17], and it also received support from officials in the military and national security including John O. Brennan, the chief counterterrorism adviser to the White House.[18][19] According to The Washington Post, experts said that the failure to pass the act may leave the United States \"vulnerable to widespread hacking or a serious cyberattack.\" [20] The act was opposed by Republican senators like John McCain who was concerned that the act would introduce regulations that would not be effective and could be a \"burden\" for businesses.[21] After the Senate vote, Republican Senator Kay Bailey Hutchison stated that the opposition to the bill was not a partisan issue but it not take the right approach to cybersecurity.[22]The senate vote was not strictly along partisan lines, as six Democrats voted against it, and five Republicans voted for it.[23] Critics of the bill included the US Chamber of Commerce,[24] advocacy groups like the American Civil Liberties Union and the Electronic Frontier Foundation,[25] cybersecurity expert Jody Westby, and The Heritage Foundation, both of whom argued that although the government must act on cybersecurity, the bill was flawed in its approach and represented \"too intrusive a federal role.\"[26]\nIn February 2013, Obama proposed the Executive Order Improving Critical Infrastructure Cybersecurity.  It represents the latest iteration of policy but is not considered to be law as it has not been addressed by Congress yet. It seeks to improve existing public-private partnerships by enhancing timeliness of information flow between DHS and critical infrastructure companies. It directs federal agencies to share cyber threat intelligence warnings to any private sector entity identified as a target. It also tasks DHS with improving the process to expedite security clearance processes for applicable public and private sector entities to enable the federal government to share this information at the appropriate sensitive and classified levels.  It directs the development of a framework to reduce cyber risks, incorporating current industry best practices and voluntary standards. Lastly, it tasks the federal agencies involved with incorporating privacy and civil liberties protections in line with Fair Information Practice Principles.\nIn January 2015, Obama announced a new cybersecurity legislative proposal. The proposal was made in an effort to prepare the US from the expanding number of cyber crimes. In the proposal, Obama outlined three main efforts to work towards a more secure cyberspace for the US. The first main effort emphasized the importance of enabling cybersecurity information sharing. By enabling that, the proposal encouraged information sharing between the government and the private sector. That would allow the government to know what main cyber threats private firms are facing and would then allow the government to provide liability protection to those firms that shared their information. Furthermore, that would give the government a better idea of what the US needs to be protected against. Another main effort that was emphasized in this proposal was to modernize the law enforcement authorities to make them more equipped to properly deal with cyber crimes by giving them the tools they need in order to do so. It would also update classifications of cyber crimes and consequences. One way this would be done would be by making it a crime for overseas selling of financial information. Another goal of the effort is to place cyber crimes prosecutable. The last major effort of the legislative proposal was to require businesses to report data breaching to consumers if their personal information had been sacrificed. By requiring companies to do so, consumers are aware of when they are in danger of identity theft.\nIn February 2016, Obama developed a Cybersecurity National Security Action Plan (CNAP). The plan was made to create long-term actions and strategies in an effort to protect the US against cyber threats. The focus of the plan was to inform the public about the growing threat of cyber crimes, improve cybersecurity protections, protects personal information of Americans, and to inform Americans on how to control digital security. One of the highlights of this plan include creating a \"Commission on Enhancing National Cybersecurity.\" The goal of this is to create a Commission that consists of a diverse group of thinkers with perspectives that can contribute to make recommendations on how to create a stronger cybersecurity for the public and private sector. The second highlight of the plan is to change Government IT. The new Government IT will make it so that a more secure IT can be put in place. The third highlight of the plan is to give Americans knowledge on how they can secure their online accounts and avoid theft of their personal information through multi-factor authentication. The fourth highlight of the plan is to invest 35% more money that was invested in 2016 into cybersecurity.\n\n\n=== Other government efforts ===\nIn addition to regulation, the federal government has tried to improve cybersecurity by allocating more resources to research and collaborating with the private sector to write standards. In 2003, the President's National Strategy to Secure Cyberspace made the Department of Homeland Security (DHS) responsible for security recommendations and researching national solutions. The plan calls for cooperative efforts between government and industry \"to create an emergency response system to cyber-attacks and to reduce the nation's vulnerability to such threats\n\"[27] In 2004, the US Congress allocated $4.7 billion toward cybersecurity and achieving many of the goals stated in the President's National Strategy to Secure Cyberspace.[28] Some industry security experts state that the President's National Strategy to Secure Cyberspace is a good first step but is insufficient.[29] Bruce Schneier stated, \"The National Strategy to Secure Cyberspace hasn't secured anything yet.\" [30] However, the President's National Strategy clearly states that the purpose is to provide a framework for the owners of computer systems to improve their security rather than the government taking over and solving the problem.[31] However, companies that participate in the collaborative efforts outlined in the strategy are not required to adopt the discovered security solutions.\nIn the United States, the US Congress is trying to make information more transparent after the Cyber Security Act of 2012, which would have created voluntary standards for protecting vital infrastructure, failed to pass through the Senate. In February 2013, the White House issued an executive order, titled \"Improving Critical Infrastructure Cybersecurity,\" which allows the executive branch to share information about threats with more companies and individuals. In April 2013, the House of Representatives passed the Cyber Intelligence Sharing and Protection Act (CISPA), which calls for protecting against lawsuits aimed at companies that disclose breach information. The Obama administration said that it might veto the bill.\n\n\n== India ==\nIn the light of the hacking of the website of the Indian Space Agency's commercial arm in 2015, Antrix Corporation and government's Digital India programme, a cyberlaw expert and advocate at the Supreme Court of India, Pavan Duggal, stated that \"a dedicated cyber security legislation as a key requirement for India. It is not sufficient to merely put cyber security as a part of the IT Act. We have to see cyber security not only from the sectoral perspective, but also from the national perspective.\"\n\n\n== European Union ==\nCybersecurity standards have been of great prominence in today's technology driven businesses. To maximize their profits, corporations leverage technology by running most of their operations by the internet. Since there are a large number of risks that entail internetwork operations, such operations must be protected by comprehensive and extensive regulations. Existing cybersecurity regulations all cover different aspects of business operations and often vary by region or country in which a business operates. Because of the differences in a country's society, infrastructure, and values, one overarching cyber security standard is not optimal for decreasing risks. While US standards provide a basis for operations, the European Union has created a more tailored regulation for businesses operating specifically within the EU. Also, in light of Brexit, it is important to consider how the UK has chosen to adhere to such security regulations.\nThree major regulations within the EU include the ENISA, the NIS Directive and the EU GDPR. They are part of the Digital Single Market strategy.\nRegarding standards, the Cybersecurity Act / ENISA Regulation does not refer directly to standards. Nevertheless, ENISA recognises on its website that \"EU\u2019s cybersecurity strategy underscores support for greater standardisation via the European standardisation organisations (CEN, CENELEC and ETSI) as well as ISO.\"\nISO/IEC Standards, as well as European Standards from CEN, CENELEC and ETSI can be used on a voluntary way to support the requirements in the EU legislation. An updated list of ISO/IEC and CEN/CENELEC standards on the topic of Cybersecurity can be followed up via the free and publicly available information website Genorma.com.\n\n\n=== ENISA ===\nThe European Union Agency for Cybersecurity (ENISA) is a governing agency that was originally set up by the Regulation (EC) No 460/2004 of the European Parliament and of the Council of 10 March 2004 for the Purpose of Raising Network and Information Security (NIS) for all internetwork operations in the EU. ENISA currently runs under Regulation (EU) No 526/2013, which has replaced the original regulation in 2013. ENISA works actively with all member states of the EU to provide a range of services. The focus of their operations are on three factors:\n\nRecommendations to member states on the course of action for security breaches\nPolicy making and implementation support for all members states of the EU\nDirect support with ENISA taking a hands-on approach to working with operational teams in the EU\nENISA is made up of a management board that relies on the support of the executive director and the Permanent Stakeholders Group. Most operations, however, are run by the heads of various departments.\nENISA has released various publications that cover all major issues on cybersecurity. ENISA's past and current initiatives include the EU Cloud Strategy, Open Standards in Information Communications Technology, a Cyber Security Strategy of the EU and a Cyber Security Coordination Group. ENISA also works in collaboration with existing international standard organizations like the ISO and the ITU.\n\n\n=== NIS Directive ===\nOn July 6, 2016, the European Parliament set into policy the Directive on Security of Network and Information Systems (the NIS Directive).\nThe directive went into effect in August 2016, and all member states of the European Union were given 21 months to incorporate the directive's regulations into their own national laws. The aim of the NIS Directive is to create an overall higher level of cybersecurity in the EU. The directive significantly affects digital service providers (DSPs) and operators of essential services (OESs). Operators of essential services include any organizations whose operations would be greatly affected in the case of a security breach if they engage in critical societal or economic activities. Both DSPs and OES are now held accountable for reporting major security incidents to Computer Security Incident Response Teams (CSIRT). While DSPs are not held to as stringent regulations as operators of essential services, DSPs that are not set up in the EU but still operate in the EU still face regulations. Even if DSPs and OES outsource the maintenance of their information systems to third parties, the NIS Directive still holds them accountable for any security incidents.\nThe member states of the EU are required to create a NIS directive strategy, which includes the CSIRTs, in addition to National Competent Authorities (NCAs) and Single Points of Contact (SPOCs). Such resources are given the responsibility of handling cybersecurity breaches in a way that minimizes impact. In addition, all member states of the EU are encouraged to share cyber security information.\nSecurity requirements include technical measures that manage the risks of cybersecurity breaches in a preventative manner. Both DSP and OES must provide information that allows for an in depth assessment of their information systems and security policies. All significant incidents must be notified to the CSIRTs. Significant cybersecurity incidents are determined by the number of users affected by the security breach as well as the longevity of the incident and the geographical reach of the incident. A NIS 2 is in the making.\nOnly 23 Member States have fully implemented the measures contained with the NIS Directive. Infringement proceedings against them to enforce the Directive have not taken place, and they are not expected to take place in the near future. This failed implementation has led to the fragmentation of cybersecurity capabilities across the EU, with differing standards, incident reporting requirements and enforcement requirements being implemented in different Member States. \n\n\n=== EU Cybersecurity Act ===\nThe EU Cybersecurity Act establishes an EU-wide cybersecurity certification framework for digital products, services and processes. It complements the NIS Directive. ENISA will have a key role in setting up and maintaining the European cybersecurity certification framework.\n\n\n=== EU GDPR ===\n\nThe EU General Data Protection Regulation (GDPR) was set into place on 14 April 2016, but the current date of enforcement is set to be on 25 May 2018. The GDPR aims to bring a single standard for data protection among all member states in the EU. Changes include the redefining of geographical borders. It applies to entities that operate in the EU or deal with the data of any resident of the EU. Regardless of where the data is processed, if an EU citizen's data is being processed, the entity is now subject to the GDPR.\nFines are also much more stringent under the GDPR and can total \u20ac20 million or 4% of an entity's annual turnover, whichever is higher. In addition, like in previous regulations, all data breaches that effect the rights and freedoms of individuals residing in the EU must be disclosed within 72 hours.\nThe overarching board, the EU Data Protection Board, EDP, is in charge of all oversight set by the GDPR.\nConsent plays a major role in the GDPR. Companies that hold data in regards to EU citizens must now also offer to them the right to back out of sharing data just as easily as when they consented to sharing data.\nIn addition, citizens can also restrict processing of the data stored on them and can choose to allow companies to store their data but not process it, which creates a clear differentiation. Unlike previous regulations, the GDPR also restricts the transfer of a citizen's data outside of the EU or to a third party without a citizen's prior consent.\n\n\n=== The NIS 2 Directive ===\nOn the 16 January 2023, the EU Parliament and Council adopted the 2022/2555 of the European Parliament and of the Council of 14 December 2022 on measures for a high common level of cybersecurity across the Union, amending Regulation (EU) No 910/2014 and Directive (EU) 2018/1972, and repealing Directive (EU) 2016/1148 (NIS Directive). This new Directive aims to extend the scope of obligations on entities required to take measures to increase their cybersecurity capabilities. The Directive also aims to harmonise the EU approach to incident notifications, security requirements, supervisory measures and information sharing. \n\n\n=== The Digital Operational Resilience Act (DORA) ===\n\nDORA creates a regulatory framework on digital operational resilience whereby all firms need to make sure they can withstand, respond to and recover from all types of ICT-related disruptions and threats. These requirements are homogenous across all EU member states.\nThe regulation will apply from 17 January 2025 for relevant financial entities and ICT third-party service providers.\n\n\n=== Cyber Resilience Act ===\n\nThe Cyber Resilience Act (CRA) is a regulation proposed on 15 September 2022 by the European Commission which outlines common cybersecurity standards for hardware and software products in the EU.\n\n\n=== Individual EU Countries ===\n\n\n==== Republic of Ireland ====\n\nThe Criminal Justice (Offences Relating to Information Systems) Act 2017 was introduced in May 2017 to consolidate laws on computer crime.\n\n\n== Reactions ==\nWhile experts agree that cybersecurity improvements are necessary, there is disagreement about whether the solution is more government regulation or more private-sector innovation.\n\n\n=== Support ===\nMany government officials and cybersecurity experts believe that the private sector has failed to solve the cybersecurity problem and that regulation is needed. Richard Clarke states that \"industry only responds when you threaten regulation. If industry does not respond [to the threat], you have to follow through.\"[32] He believes that software companies must be forced to produce more secure programs.[33] Bruce Schneier also supports regulation that encourages software companies to write more secure code through economic incentives.[34]  US Representative Rick Boucher (D\u2013VA) proposes improving cybersecurity by making software companies liable for security flaws in their code.[35] In addition, to improving software security, Clarke believes that certain industries, such as utilities and ISPs, require regulation.[36]\n\n\n=== Opposition ===\nOn the other hand, many private-sector executives and lobbyists believe that more regulation will restrict their ability to improve cybersecurity. Harris Miller, a lobbyist and president of the Information Technology Association of America, believes that regulation inhibits innovation.[37] Rick White, former corporate attorney and president and CEO of the lobby group TechNet, also opposes more regulation. He states that \"the private-sector must continue to be able to innovate and adapt in response to new attack methods in cyber space, and toward that end, we commend President Bush and the Congress for exercising regulatory restraint.\"[38]\nAnother reason many private-sector executives oppose regulation is that it is costly and involves government oversight in private enterprise. Firms are just as concerned about regulation reducing profits as they are about regulation limiting their flexibility to solve the cybersecurity problem efficiently.\nSpecifically around the CRA, concern is expressed over the breadth of impact by prominent free and Open source software organizations: Eclipse Foundation, Internet Society, and Python Software Foundation. These organizations raise several consequences which are unstated by the regulation, that they conclude fundamentally damage the Open source movement. They offer changes that would allow Open source to be used in the EU without being regulated in the same manner as would be on commercial software developers.      \n\n\n== See also ==\nCERT Coordination Center\nCyber security standards\nCybersecurity Information Sharing Act\nCyber Security and Resilience Bill - proposed UK regulation.\nDefault password\nInformation assurance\nList of data breaches\nMedical device hijack\nNational Cyber Security Division\nNational Strategy to Secure Cyberspace\nPresidential directive\nProactive cyber defence\nUnited States Computer Emergency Readiness Team\nUnited States Department of Homeland Security\n\n\n== Notes ==\n^   \"A chronology of data breaches reported since the ChoicePoint incident.\" (2005). Retrieved October 13, 2005.\n^  \"Electronic privacy information center bill track: Tracking privacy, speech and civil liberties in the 109th congress.\" (2005). Retrieved October 23, 2005.\n^  \"How computer viruses work.\" (2005). Retrieved October 10, 2005.\n^  \"The National Strategy to Secure Cyberspace Archived 2012-02-27 at the Wayback Machine.\"  (2003).  Retrieved December 14, 2005.\n^  \"Notice of security breach \u2013 civil code sections 1798.29 and 1798.82 \u2013 1798.84.\" 2003). Retrieved October 23, 2005.\n^  \"Richard Clarke interview.\" (2003). Retrieved December 4, 2005.\n^  Gordon, L. A., Loeb, M. P., Lucyshyn, W. & Richardson, R. (2005). \"2005 CSI/FBI computer crime and security survey.\" Retrieved October 10, 2005.\n^  Heiman, B. J. (2003). Cybersecurity regulation is here. RSA security conference, Washington, D.C.  Retrieved October 17, 2005.\n^  Kirby, C. (2003, December 4, 2003). \"Forum focuses on cybersecurity\". San Francisco Chronicle.\n^  Lemos, R. (2003). \"Bush unveils final cybersecurity plan.\" Retrieved December 4, 2005.\n^  Menn, J. (2002, January 14, 2002). \"Security flaws may be pitfall for Microsoft\". Los Angeles Times, pp. C1.\n^  Rasmussen, M., & Brown, A. (2004). \"California Law Establishes Duty of Care for Information Security.\" Retrieved October 31, 2005.\n^   Schmitt, E., Charron, C., Anderson, E., & Joseph, J. (2004). \"What Proposed Data Laws Will Mean for Marketers.\" Retrieved October 31, 2005.\n^  Jennifer Rizzo. (August 2, 2012) \"Cybersecurity bill fails in Senate.\" Accessed August 29, 2012.\n^  Paul Rosenzweig. (July 23, 2012) \"Cybersecurity Act of 2012: Revised Cyber Bill Still Has Problems.\" The Heritage Foundation. Accessed August 20, 2012.\n^  Ed O'Keefe & Ellen Nakashima. (August 2, 2012 ) \"Cybersecurity bill fails in Senate.\" The Washington Post. Accessed August 20, 2012.\n^  Alex Fitzpatrick. (July 20, 2012) \"Obama Gives Thumbs-Up to New Cybersecurity Bill.\"  Mashable. Accessed August 29, 2012.\n^  Brendan Sasso. (August 4, 2012) \"After defeat of Senate cybersecurity bill, Obama weighs executive-order option\". The Hill. Accessed August 20, 2012.\n^  Jaikumar Vijayan. (August 16, 2012) \"No partisan fight over cybersecurity bill, GOP senator says\". Computerworld. Accessed August 29, 2012.\n^  Carl Franzen. (August 2, 2012)  \"As Cybersecurity Bill Fails In Senate, Privacy Advocates Rejoice\". TPM. August 29, 2012.\n^  Alex Fitzpatrick. (August 2, 2012) \"Cybersecurity Bill Stalls in the Senate\". Mashable. Accessed August 29, 2012.\n^  Jody Westby (August 13, 2012) \"Congress Needs to Go Back To School on Cyber Legislation\". Forbes. Accessed August 20, 2012.\n\n\n== References ==", "link": "https://en.wikipedia.org/wiki/Cyber-security_regulation"}, "Bitdefender": {"title": "Bitdefender", "content": "Bitdefender is a multinational cybersecurity technology company dual-headquartered in Bucharest, Romania and Santa Clara, California, with offices in the United States, Europe, Australia and the Middle East.\nThe company was founded in 2001 by the current CEO and main shareholder, Florin Talpe\u0219. Bitdefender develops and delivers cybersecurity products and services, including endpoint protection, cloud and managed security, antivirus software, and security for the Internet of things. Bitdefender products are distributed through partners in over 150 countries and the US market is estimated to generate more than 40% of its revenues.  As of 2023, the company employed more than 1,800 people worldwide.\n\n\n== History ==\nBitdefender software was originally developed by SOFTWIN, a company founded in 1990 in post-communist Romania. It was originally sold as AVX (Antivirus Expert) from 1996 until 2001, when the Bitdefender subsidiary was created, and AVX was rebranded under the Bitdefender name.\nIn 2007, Bitdefender became a separate business entity with external capital entry, with Axxess Capital Investment Fund as a key shareholder.\nFrom 2004 to 2015, the company expanded with offices in the United States, Germany, United Kingdom, Denmark, and the Middle East.\nIn 2017, the company acquired French partner Profil Technology. British fund Virtuvian Partners would then buy a 30% stake in the company, putting its valuation over $600 million.\nIn 2018, Bitdefender opened a new subsidiary in Australia, through acquisition of assets from SMS eTech. That year, the company also acquired behavioral and network security analysis company RedSocks. \nBitdefender opened its own Security Operations Center in San Antonio, Texas in 2019. Bitdefender signed a multi-year partnership deal with Scuderia Ferrari on September 28, 2022.\nIn April 2024, Bitdefender announced the launch of Bitdefender Voyager Ventures, a corporate venture capital unit.\n\n\n== Products and services ==\nBitdefender's original product was sold as Antivirus Expert until 2001, when it was rebranded under the Bitdefender name.\nSince 2011, Bitdefender has expanded to include consumer and enterprise security products, and in 2021, an extended detection and response (XDR) platform. Like most security suites, Bitdefender\u2019s consumer services are managed through an online portal, called Bitdefender Central.\nIn December 2023, Bitdefender launched Scamio, a free AI-powered scam detector.\n\n\n=== Cybersecurity ===\nThe company joined the No More Ransom initiative in 2016, releasing free decryption tools for victims of ransom attacks to decrypt their devices without having to pay to do so.In 2018, Bitdefender joined the Cybersecurity Tech Accord.\nBitdefender\u2019s 2020 research into the Interplanetary Storm botnet proxy network was provided to law enforcement ahead of the FBI dismantling the network in November 2023.\n\n\n== Controversies and incidents ==\n\n\n=== Trojan.FakeAlert.5 ===\nOn March 20, 2010, computers running Bitdefender under 64-bit versions of Windows were affected by a malfunctioning update that classified every executable program as well as DLL files as infected. These files were all marked as 'Trojan.FakeAlert.5' and were moved into quarantine. This action led to software and systems malfunctions that affected users around the world.  Bitdefender representatives announced the removal of the faulty update and a workaround for the users affected, except for those using the 2008 version.\n\n\n=== DarkSide ransomware ===\nIn 2021, Bitdefender was accused of self-promotion when releasing and publicly announcing a decryptor to the detriment of victims of DarkSide, a hacking group.  In 2020, DarkSide switched their main encryption ransomware product over to an \"affiliate\" model wherein other attackers could download and use their software in exchange for a portion of the profits.  However, they introduced a bug in the process where affiliate hackers would all use the same private RSA key - meaning that a decryption package for a single target who paid the ransom would work on any target that had the ransomware installed.  Security researchers noticed and were quietly already helping victims of the software, but with no public notice, making it so that the attackers would only see an inexplicable decrease in ransom payments that could be written off as chance.  \nMonths later, Bitdefender publicly released a decryptor of their own development and issued a blog post describing the flaw.  This was criticized in an article in the MIT Technology Review.  The article wrote that Bitdefender's program wasn't even safe - it was flawed and would \"damage\" files decrypted with it due to bugs within it, unlike the earlier decryptors that had been quietly used.  Second, the blog post tipped off DarkSide as to the nature of the flaw, leading to the group sarcastically thanking Bitdefender and patching the bug.\nA notable incident that took place after Bitdefender's public disclosure was the Colonial Pipeline cyberattack in May 2021.  While the security researchers who had been using the flaw before acknowledge that it's probable DarkSide would eventually have noticed and fixed the issue, they still criticized Bitdefender for using the bug merely for a brief burst of publicity, rather than in the way that would most help victims of the scheme.  Bitdefender has defended their actions on their blog, saying that they only wished to make as many organizations as possible aware of the existence of the bug in DarkSide's ransom attacks.  The article and blog post triggered a discussion among cybersecurity professionals about the pros and cons of publicly disclosing such vulnerabilities in malware.\n\n\n== Awards ==\n2023 - AV-Comparatives Outstanding Security Award\n\n\n== See also ==\nComparison of antivirus software\nComparison of firewalls\nComparison of computer viruses\nMultiscanning\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nA Q&A discussion in IT World Canada on whether Bitdefender ought to have released the decryptor software w/r/t DarkSide", "link": "https://en.wikipedia.org/wiki/Bitdefender"}, "Comodo Cybersecurity": {"title": "Comodo Cybersecurity", "content": "Xcitium, formerly known as Comodo Security Solutions, Inc., is a cybersecurity company headquartered in Bloomfield, New Jersey. Under the brand Sectigo, the company acts as a web Certificate authority (CA) and issues SSL/TLS certificates.\n\n\n== History ==\nThe company was founded in 1998 in the United Kingdom by Melih Abdulhayo\u011flu. The company relocated to the United States in 2004. Its products are focused on computer and internet security. The firm operates a certificate authority that issues SSL certificates. The company also helped on setting standards by contributing to the IETF (Internet Engineering Task Force) DNS Certification Authority Authorization (CAA) Resource Record.\nIn October 2017, Francisco Partners acquired Comodo Certification Authority (Comodo CA) from Comodo Security Solutions, Inc. Francisco Partners rebranded Comodo CA in November 2018 to Sectigo.\nOn June 28, 2018, the new organization announced that it was expanding from TLS/SSL certificates into IoT security with the announcement of its IoT device security platform. The company announced its new headquarters in Roseland, New Jersey on July 3, 2018 and its acquisition of CodeGuard, a website maintenance and disaster recovery company, on August 16, 2018.\nOn June 29, 2020, Comodo announced their strategic partnership with the company CyberSecOp. The firm has partnered with Comodo in the past, and seeks to provide a range of cybersecurity products and consulting services.\n\n\n== Companies ==\nComodo CA Limited (Sectigo): Based in the City of Salford, Greater Manchester, UK, is a digital certificate authority that issues SSL and other digital certificates. In November 2018, Francisco Partners announced that Comodo Certificate Authority (Comodo CA) is rebranding as Sectigo.\nComodo Security Solutions, Inc: Based in Clifton, New Jersey, US, develops security software for commercial and consumer use.\nDNS.com: Based in Louisville, Kentucky, US, the company provides managed DNS services.\n\n\n== Industry affiliations ==\nComodo is a member of the following industry organizations:\n\nCertificate Authority Security Council (CASC): In February 2013, Comodo became a founding member of this industry advocacy organization dedicated to addressing industry issues and educating the public on internet security.\nCommon Computing Security Standards Forum (CCSF): In 2009 Comodo was a founding member of the CCSF, an industry organization that promotes industry standards that protect end users.  Comodo CEO Melih Abdulhayo\u011flu is considered the founder of the CCSF.\nCA/Browser Forum: In 2005, Comodo was a founding member of a new consortium of certificate authorities and web browser vendors dedicated to promoting industry standards and baseline requirements for internet security. Melih Abdulhayo\u011flu invited top browser providers and certification authorities to a round table to discuss the creation of a central authority responsible for delivering digital certificate issuance best practice guidelines.\n\n\n== Products ==\nComodo Dragon (web browser)\nComodo Ice Dragon (web browser)\nComodo Internet Security\nComodo System Utilities\nComodo Mobile Security\nComodo Endpoint Protection\nXcitium EDR\n\n\n== Controversies ==\n\n\n=== Symantec ===\nIn response to Symantec's comment asserting paid antivirus is superior to free antivirus, the CEO of Comodo Group, Melih Abdulhayo\u011flu had challenged Symantec on 18 September 2010 to see whether paid or free products can better defend the consumer against malware. GCN'S John Breeden understood Comodo's stance on free Antivirus software and challenging Symantec: \"This is actually a pretty smart move based on previous reviews of AV performance we've done in the GCN Lab. Our most recent AV review this year showed no functional difference between free and paid programs in terms of stopping viruses, and it's been that way for many years. In fact you have to go all the way back to 2006 to find an AV roundup where viruses were missed by some companies.\"\nSymantec responded saying that if Comodo is interested they should have their product included in tests by independent reviewers.\nComodo volunteered to a Symantec vs. Comodo independent review.  Though this showdown did not take place, Comodo has since been included in multiple independent reviews with AV-Test, PC World, Best Antivirus Reviews, AV-Comparatives, and PC Mag.\n\n\n=== Certificate hacking ===\nOn 23 March 2011, Comodo posted a report that 8 days earlier, on 15 March 2011, a user account with an affiliate registration authority had been compromised and was used to create a new user account that issued nine certificate signing requests. Nine certificates for seven domains were issued. The attack was traced to IP address 212.95.136.18, which originates in Tehran, Iran. Moxie Marlinspike analyzed the IP address on his website the next day and found it to have English localization and Windows operating system. Though the firm initially reported that the breach was the result of a \"state-driven attack\", it subsequently stated that the origin of the attack may be the \"result of an attacker attempting to lay a false trail.\".\nComodo revoked all of the bogus certificates shortly after the breach was discovered. Comodo also stated that it was actively looking into ways to improve the security of its affiliates.\nIn an update on 31 March 2011, Comodo stated that it detected and thwarted an intrusion into a reseller user account on 26 March 2011. The new controls implemented by Comodo following the incident on 15 March 2011, removed any risk of the fraudulent issue of certificates. Comodo believed the attack was from the same perpetrator as the incident on 15 March 2011.\nIn regards to this second incident, Comodo stated, \"Our CA infrastructure was not compromised. Our keys in our HSMs were not compromised. No certificates have been fraudulently issued. The attempt to fraudulently access the certificate ordering platform to issue a certificate failed.\"\n\nOn 26 March 2011, a person under the username \"ComodoHacker\" verified that they were the attacker by posting the private keys online and posted a series of messages detailing how poor Comodo's security is and bragging about his abilities:I hacked Comodo from InstantSSL.it, their CEO's e-mail address mfpenco@mfpenco.com\nTheir Comodo username/password was: user: gtadmin password: globaltrust\nTheir DB name was: globaltrust and instantsslcms\nEnough said, huh? Yes, enough said, someone who should know already knows...\nAnyway, at first I should mention we have no relation to Iranian Cyber Army, we don't change DNSes, we\njust hack and own.\nI see Comodo CEO and other wrote that it was a managed attack, it was a planned attack, a group of\ncyber criminals did it, etc.\nLet me explain:\na) I'm not a group, I'm single hacker with experience of 1000 hacker, I'm single programmer with\nexperience of 1000 programmer, I'm single planner/project manager with experience of 1000 project\nmanagers, so you are right, it's managed by 1000 hackers, but it was only I with experience of 1000\n\nhackers.Such issues have been widely reported, and have led to criticism of how certificates are issued and revoked. As of 2016, all of the certificates remain revoked. Microsoft issued a security advisory and update to address the issue at the time of the event.\nFor Comodo's lacking response on the issue computer security researcher Moxie Marlinspike called the whole event extremely embarrassing for Comodo and rethinking SSL security. It was also implied that the attacker followed an online video tutorial and searched for basic opsec\nSuch attacks are not unique to Comodo \u2013 the specifics will vary from CA to CA, RA to RA, but there are so many of these entities, all of them trusted by default, that further holes are deemed to be inevitable.\n\n\n=== Association with PrivDog ===\nIn February 2015, Comodo was associated with a man-in-the-middle enabling tool known as PrivDog, which claims to protect users against malicious advertising.\nPrivDog issued a statement on 23 February 2015, saying, \"A minor intermittent defect has been detected in a third party library used by the PrivDog standalone application which potentially affects a very small number of users. This potential issue is only present in PrivDog versions, 3.0.96.0 and 3.0.97.0. The potential issue is not present in the PrivDog plug-in that is distributed with Comodo Browsers, and Comodo has not distributed this version to its users. there are potentially a maximum of 6,294 users in the USA and 57,568 users globally that this could potentially impact. The third party library used by PrivDog is not the same third party library used by Superfish....The potential issue has already been corrected. There will be an update tomorrow which will automatically update all 57,568 users of these specific PrivDog versions.\"\n\n\n=== Certificates issued to known malware distributors ===\nIn 2009 Microsoft MVP Michael Burgess accused Comodo of issuing digital certificates to known malware distributors. Comodo responded when notified and revoked the certificates in question, which were used to sign the known malware.\n\n\n=== Chromodo browser, ACL, no ASLR, VNC weak authentication ===\nIn January 2016, Tavis Ormandy reported that Comodo's Chromodo browser exhibited a number of vulnerabilities, including disabling of the same-origin policy.\nThe vulnerability wasn't in the browser itself. Rather, the issue was with an add-on. As soon as Comodo became aware of the issue in early February 2016, the company released a statement and a fix: \"As an industry, software in general is always being updated, patched, fixed, addressed, improved \u2013 it goes hand in hand with any development cycle...What is critical in software development is how companies address an issue if a certain vulnerability is found \u2013 ensuring it never puts the customer at risk.\" Those using Chromodo immediately received an update. The Chromodo browser was subsequently discontinued by Comodo.\nOrmandy noted that Comodo received a \"Excellence in Information Security Testing\" award from Verizon despite the vulnerability in its browser, despite having its VNC delivered with a default of weak authentication, despite not enabling address space layout randomization (ASLR), and despite using access control lists (ACLs) throughout its product. Ormandy has the opinion that Verizon's certification methodology is at fault here.\n\n\n=== Let's Encrypt trademark registration application ===\nIn October 2015, Comodo applied for \"Let's Encrypt\", \"Comodo Let's Encrypt\", and \"Let's Encrypt with Comodo\" trademarks. These trademark applications were filed almost a year after the Internet Security Research Group, parent organization of Let's Encrypt, started using the name Let's Encrypt publicly in November 2014, and despite the fact Comodo's \"intent to use\" trademark filings acknowledge that it has never used \"Let's Encrypt\" as a brand.\nOn 24 June 2016, Comodo publicly posted in its forum that it had filed for \"express abandonment\" of their trademark applications.\nComodo's Chief Technical Officer Robin Alden said, \"Comodo has filed for express abandonment of the trademark applications at this time instead of waiting and allowing them to lapse. Following collaboration between Let's Encrypt and Comodo, the trademark issue is now resolved and behind us, and we'd like to thank the Let's Encrypt team for helping to bring it to a resolution.\"\n\n\n=== Dangling markup injection vulnerability ===\nOn 25 July 2016, Matthew Bryant showed that Comodo's website is vulnerable to dangling markup injection attacks and can send emails to system administrators from Comodo's servers to approve a wildcard certificate issue request which can be used to issue arbitrary wildcard certificates via Comodo's 30-Day PositiveSSL product.\nBryant reached out in June 2016, and on 25 July 2016, Comodo's Chief Technical Officer Robin Alden confirmed a fix was put in place, within the responsible disclosure date per industry standards.\n\n\n== See also ==\nComparison of antivirus software\nComparison of computer viruses\nInternet Security\nComparison of firewalls\n\n\n== References ==\n\n\n== External links ==\nOfficial website", "link": "https://en.wikipedia.org/wiki/Comodo_Cybersecurity"}}}